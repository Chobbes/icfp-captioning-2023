>>, Testing, our first speakers will-- be 
>>  Oh my gosh I love your poster what is it about? 
>> These are posters on free generators are a tool that use property based testing I was working on. Allows us to you view a generator of random things as a poster. You can also thinking about it as parsing those values and that is what free generators formalize. If you have some property that you want to be true about a programme for example, the inserting into a binary search tree remains validity. And to quick check that testing the property always goes. And what free generators does is allows us to decompose a generator, instead of just pulling a value out of thin air, we pull some random choices out of thin air and we make those choices. They are formally based on a for your monad which is a abstract syntax tree of the monad programme. There is one operation called. Which is either interpreted as a choice. As a parsing for the first choice in a sequence. And when you write the free generator it looks like a normal man attic generator. But now you can do symmetrical things with it. For example you can manipulate the random choices after you have made them. 
>>  While, that is really cool  thanks. And the bidirectional programmes revolve around bidirectional transformations. This is one piece of code that can be run into directions and is normally expected to satisfy some sort of round tripping property. For example my two directions, be transforming to and from different representations of the same thing. And my round tripping could say going there and back again is the identity function. This is really useful because it saves time, intense effort and it saves the issue we have two pieces of code and they become out of sync. So the classic example here is the by parser, this is one piece of code that we run forwards as a parser, or backwards as a printer. I suppose these are actually bit like you are free generators except this time the randomness being parsed as a string and what is being generated as an ASC. Anyway these are normally written using coordinators. So you think of lenses but the mannitol makes it more expressive by allowing them to be written medically. And this is achieved a partial monadic pro functors. And pro functors are a standard construct from category theory and generalizing ordinary functions or structure preserving maps. And they support both Co. And contrary operations. So in Haskell they realise that this type of class where the operations are called L map or are Matt. And you might actually recognise the L map under the better name of [Laughter]. The example here is the function arrow words better contrary and in letter B and covariant in A. And here I'll map, our map are pre and post composition. Respectively. So a man attic Pro function is one that is meant attic in its second parameter. Bear with me but I will get more technical it's worth getting to grips with this. So I wanted to make my bypass like this, it would be something like this, this is the pro functor in letter B and A. A man attic in letter A. It works with combining this and it can be parsing a string into letter A or, run backward into print a letter B. I suppose the scheme makes most sense when the parser is aligned. That is when to be an A are the same type. That's on the parsing imprinting of the same thing and hear the printing function is just the identity function which also produces a string representation.
 The structure is important because we can compose this medically. For example if I have a are parser for electors and one for numbers and I wanted to make one for the lectures or numbers it might look something like this. But this is if you new things to it so let's roll it back to something more familiar. 
>>  Hang on, wait how did you animate your poster? 
>> So this type is a by part dash my part by parser for a letter or number and it's written into halves and each half is combined together using alternative are constructed so it's a little bar thing. If we focus on each part individually it's built upon a letter and number sub by parser's. For example number is going to produce a number N. And that gets passed along and then wrapped up in the number constructor.
 For those familiar with monadic and alternative programming this seems like the natural solution. Remember when no longer programming with just monads, we are programming with partial monadic pro functors. So we can get away with just the monad interface. We also need a partial pro functor operations. So as X or Y we need the partial pro functor operations starting with the pro functor ones and going on with the partial ones. The pro functor operations are motivated by the fact that this term, this expression doesn't type check. That is because bind only operates in the second type of the parameter of the pro functor. So in the number case, binders expecting its first argument to have the by parser Lauren and tight. That is not the type have number. This means that we have some function that it will change the first parameter of the pro functor. Locally, that is exactly what the LMAP Pro functor operation does. LMAP so can fix this type of air if given a function. And when programming with partial monadic pro functors these will appear on the so-called or personal call it will be always some sort of projection function from the bigger structure in this case LORN to a smaller contained structure, in this case int or char. And now let's motivate why we need the partiality when considering what happens if we need the L annotation the a number or our R annotation a letter. So we currently can't quite type check because that's where we need partial monadic pro functors. Because partial nomadic pro functors can have partially injected first type parameter of the pro functor using this function called proven and our annotation functions can be total. Because they expect to have the type learned to maybe int.
 We also have this generator example that might be of interest to you. So bigenerators, can be ran forwards at a generator like normal but also run backwards. When they run backwards, they check whether a generator could have made that value. This is particularly useful for generators like BST one that bakes into what is to be a binary search tree. Now the by generator can be used as a checker for that value. 
>> Yes, that is really neat, actually think that if we combine partial monadic pro functors with three years we can solve a problem that I've been thinking about. So you know how in property based testing sometimes you to shrink the values. Sometimes the values that you have are too big and it is hard to debug and you don't know what part of the value causes the problem. Often this is a manual process of figure out how to shrink the values. There is some automation, typeface. But ultimately when you have really complex situations it's hard to shrink values. And there are some tools for example that are used in the Python's Hypothesis from work that use the generator to help in the shrinking process. Let me draw some stuff to give you example of what I'm thing about. 
>> SAMANTHA: Oh while you are really good at drawing that is like a professional! 
 
                >> HARRISON: Thank you. You have these JSON files that define what packages and note package in JavaScript is. And we will analyse and define that stuff. And say a JavaScript user comes to us and say hey this JSON file I have breaks your code. Your code doesn't work and more. And the problem it is really big. The JSON file as we began you don't know what part of it is actually causing your code to fail. What you would like is to be able to shrink that file down, blow away a bunch of the fields that don't matter, keeping the important parts of the structure and isolate the part that is actually causing the bug. In this case rogue dependency. But you don't want to have to write this manual. You don't want to have to figure out how to safely train a JSON file. So instead you wanted to be automatic. And if you think about you know, the whole free generator thing. Since generators are parsers, what if instead of shrinking the value, we would shrink the choices that we produce that value? This is a technique developed by McCalver and Donaldson and no I said is a hypothesis. It supercool, it really works except there is a problem here. It doesn't affect work for us. Because this problem came from the user.
                 We don't have the choices that produce the value and / and so we can't do this internal shrinking process. If we have some way of going backward, in the choices that would produce a value. And we can do the streaking process the way that we want. And a generator running and reverses is only what we need here. 
>> I thank you are right. So remember when I said the generators, check whether a value was generated by generator? I bet we could also get them to return the choices that led to a value. Have you got your free generator codes? 
>> Oh yes, I can pull it up. 
>> Nice! Oh! Go away, so the main operations at the generators need are the partial magnetic pro functor ones. It is bind, all of those guys. A 
>> On my gosh, you're so fast at typing. 
>> So, well the freer monad, that means the free generators are already monadic. So suppose we don't need binder return but we will need an extra type parameter. And while we are adding that let's get a little bit more intuition about these type of parameters. You can think of the first one is sort of our overall of the result type that we want and the second one is a intermediary value that we need to change so now I've added that type parameter and I can add the partial pro functors operations into this type. And so like free Jenna name, it doesn't really working more, it doesn't quite make sense. What about reflective's and R, because these generators reflect upon the choices that they've made. 
>> Oh that is very clever. 
>> It's almost as if you came up with it. So let's make that change, so we see if it works? So let's do the BST example. See if we can convert that to one of these reflective's. So first things first. We are going to change the type. Because we are now working with our reflective things. And you know what, I reckon it is worth sticking to this interface. Because you know, we know in the quick check interface. So let's make our own versions of frequency and choose. Next we will add our backwards annotations. So let's focus firstly on the ones on return. Let's put my bi- generator one hat on. What I want to function that gives LMAP the to do is to make sure that only a leaf could have been produced. So let's introduce failure prone, and give the function that does that to LMAP, are probably going to do this like every time instead of return. Celeste Teddy the up into a company to. Now this thing of the other annotations, ones in the node case. The ones on the subtrees and the ones that live in the number binary search tree. So I reckon that we want them to look something like this. So the function that will be given to LMAP will be focusing in on the value that the sub generator will be creating. So in that case it is the node and its extracting out number that lives in the node.
 It's like focusing on the value that is generating. It is a projection function isn't it so I think we can tidy this up a bit more, what will use a lens? If we use a lens to focus and because the lens people are really clever, that means they can be automatically generated. So that focusing function _ node can just be automatically generated. It is really cool. Oh, oh we are using for your, this means all we have done so far as buildup syntax tree. So we better right and interpretation for it. Let's start easy, but start with a generation interpretation. Because we definitely want this to still generate things. So what about something like this? For the interpretations of bind and return, why don't we interpret them as you turn and bind for gen? And then for pik, what did you do for pik, shall we see if it works, it does those are both binary search trees! But we didn't really use the backward direction. So I reckon we also want to get a check instance because that is the other half of the five pastors --by parsers. Let me quickly do that now we can check it using these BST's but I will break the second one to make sure that it says no, okay awesome! Does that give you what you need? 
>> Yes I think so, what we can do is while we are checking the value we can keep track of the choices we are making pics we can go back and get the choices from the value. And concretely what this is going to look like, the word backward is a little bit of a misnomer here. We are actually doing is running the generator forward, but keeping track of what would happen, what choices we would make if we were generating the value we are following along with. And those focusing annotations allow us to focus in on parts of the value so that we can analyse the choices. So we will focus in on the first part, here are some choices that we need to make. The next part, some more choices, and so on. Until we get a set of choices that would produce the value. This may not be a unique set of choices by the way. It could be that multiple sets of choices map to the same value but we really only need one for the tracking that we will do. Because once we have the set of choices we shrink the choices which happens once and for all. We can write a routine to shrink the choices without knowing anything what the underlying values and that we can do this internal shrinking procedure.
 And actually, now that we have got all of this, but we can do even more with this. Reflective generators are still generators. And so we can use this for generation all rhythms as well. There's this thing called inputs from hell which is an example-based tuning technique. Basically you provide a bunch of examples you think are common examples are application may see. And then you try to tune the generator to produce more examples like that. And I think we can use reflective generators to extend that algorithm to a broader set of generators. Basically we think back to the striking example. Instead of the user giving us one thing. It will give us a bunch of things. And instead avoid backward cut just excerpting a set of choices. Now we are going to extract weights on choices. We will determine the frequencies of choices that need to be made. And then we can run the generator forward again. With these updated frequencies to produce more values there were like the ones that we started with. And this is more expressive than the stuff that the inputs from hell folks to because they were working with grammar based generators. And grammar based generators are really cool but they can't express things like for example, binary search trees. And so since reflective generators are remote addict, they can capture a broader class of preconditions that we might want to write generators for. 
>> That is really cool, I wonder what else? I bet it could also complete values with holes in them. 
>>  That is a good it could also parse things right it's a freely freed generator. 
>>  And also do the opposite, it can generate the on parsing things. 
>>  And also do that we did generation thing that we were talking out. 
>>  What about that enumeration? 
>> Wait a minute hang on we are getting herself. What does, how do we know these things are correct? How does it mean any of this to be correct? Well first there is a corrective criteria for the interpretations. We need them to be partial, monadic pro functors homo- morph-isms. And so we have these soundness and completeness properties. Sound this music can generate a thing that we better be able to reelect on the choices and it. And completeness says that if we can reflect on choices in something, better be able to generate it. And since we are doing property based testing, why don't we make sure that we can actually test these properties. And it turns out we can. There are properties that you can test and apply to your reflective generators to make sure that they are both sound and complete. And we should also think about what reflective generators can do. Because they do require a little bit more work. The first thing that they definitely can't do is generate a thing and then throw it away. Because we need to be able to go backward at every step. We need to be able to get the value out of the thing of the continuation. And a little bit more interestingly, recovering information has to be decidable. Because this is a generator here for say, system F terms. You generate type and then you generate an expression of that type. In a case where they type references undecidable you can always go backward. And there's a few more limitations. But in practise, we can write all the generators that we actually want to write with reflective generators. There is just a little to get to the way that you have to frame it. 
>>  Gosh, it sounds like this has a lot of potential. I bet in the future work like automating the conversion from normal generators to reflect one's is currently we have to do that by hand and we have to write those backward annotations. But those backwards annotations were just projection functions. And that means we'll have a really specific type and normally only if you inhabitants of that type. And normally because we actually wanted to project, this one inhabitant of that type so but we could like synthesize them got maybe using a tool like Google +. And even though I have just met you I can tell that you are also passionate about making sure that the languages we make in this field, are actually usable. And even though I have just met you in the maybe because I stopped you online, I know you've been working with the HCI group of Penn. Maybe we should do some user testing to make sure they are just as usable as normal generators. Gosh, maybe we should write a paper? 
>> Sam, we've already written a paper. Thank you all for listening.
>> It was you know given a bunch of programmes that all the property or some display bug. You can learn weights. The decisions that led to those programmes, some more like weights on which decisions made. You also so that you can shrink to existing programme. How are these two concepts related? Can you use some here is six or those weights on the shrinking part instead of the generating similar programme? These seem to be two related techniques that are both useful, but I wonder are they related? 
>>  That is really good question. So concretely these things are pretty separate, right? They are different interpretations of the reflective generator. And the shrieking happens purely on a structure of choices. Whereas the example base, they both, you know what? Potentially. A lot of this now that we are in the domain of just manipulating choices. You can apply arbitrary functions to the choices and then go back forward. So yes, I'm not sure. The could finally be something there. 
>> Yes it seems like something all bit more principled then say see reduced does and there's a lot more heuristics that like a very useful in practise. Very interesting. Thank you. 
>> Hello there. So one of the things that is useful to do sometimes when you generating datatypes between variance is to find a function that restores an invariant and a broken example. For example by generating a tree labeled with integers and then to prune it, that could be useful for both the generation time, doing the shrinking or in your case considering aim example that has come from the field. It might not be something you can generate but perhaps you can fix it. So what it is, fixing of values so that they are generated will something that can be done using effective generators? 
>> Is an awesome question. I am not, so, so the tricky thing is that if you try to reflect on a value that does not satisfy the invariant, the generator contains, you don't get choices. What you kind of need is to generators that are in some way compatible. One that can reflect on choices and the invalid value and when I can go forward to fix up the choices and make better ones. I want to think about it. I'm not 100% sure but I think it is a great idea. 
>> When you feel about it, telling what you come up with. 
>> Hi Jeremy Gibbons from Oxford. Great talk, a nice double act. But I'm a bit spooked about the partial, partiality and the part monadic things, you got letters and numbers and you do a case analysis and you read the partial thing for each case and put them together and happen to know that covers all the cases. That feels to me like you are trained to squeeze into the lens format something that is not a lens. And input trailers something that is better done as a prison. Which is lenses for some types rather than the product types. Did you think about that? 
>> So it actually does use prisms we do the lens side of like what we do, the prisms for the backwards annotations. 
>> So I need to go read the paper. 
>>  Yes, yes I just said lenses because. 
>>  Hello I wondered if you looked into using this. You know writing journey is quite painful. So then you know going back was here it's great, I wonder if, you know, I go through the pain of writing my PDF generator. And then I just wrap a bunch of PDFs. They all go backwards. And then I can understand the distribution of the data. Heavy looking to like if you can leverage this into understanding the distribution of structured data by just running the generator in a backwards way? 
>> We have not done any analyses on the choices unlike choices from the set of things. But there is no reason you can't. There is a lot of flexibility because of on interpreted structures, either in the backward direction or as the postprocessing after to do analysis with that. 
>> Okay so I can have a generator that's I don't know the probably for the choices, and then run it backwards anyway? 
>>  Sorry, say that again? 
>>  S want to run the generator, if I want to understand the distribute in of the data, I cannot show the same abilities. Because you know, I don't know that. Two mac right. You just rate the generator without any rates. The backward direction ignores the way. So you can figure out the weights after that. 
>> Thank you, to the speaker again. That's all we have time for questions. 
