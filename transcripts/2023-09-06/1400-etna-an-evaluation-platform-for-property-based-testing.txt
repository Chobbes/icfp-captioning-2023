>> So hello everyone! Welcome to my talk on 1017, Etna: An Evaluation Platform for Property-Based Testing (Experience Report). Before going into the details of Etna, let's talk weekly about what property based testing is. So let's assume that you want to write the binary search tree because will love binary search trees and we read them all time. First you define your type and you define your invariant and then you start by writing your functions. Such as insert. And that this point you start the questions, okay how can I make sure that my call is correct and you want to test it. And the option that you can use here is property based testing, and property based testing what you do is you write these executable specifications that define the correctness of the programme. With respect to some properties. And here, what we have is if you have a binary source tree, if you answer any key to that. The resulting tree should also still be a binary search tree. And once you have these types of properties. The next thing that you are supposed to do is you want to generate inputs to them so that you can test those properties. And make sure that you are code is correct. So how do you generate the user input? You can generate something but the better option is to use a framework or a library by quick check, quick check is most popular option around. It has been around for more than 20 years. But is it the only option? Of course not. We can have small check like we can have frameworks like small check and lazy search that does iteration based input generation. Or we have methods such as nomination testing and that generates inputs and smart and fancy ways. And you are limited to Haskell. His property based testing on those only exist Haskell? Of course not. If you're going to-- for example you can have a quick check or fast-track. So you have plenty of options that are not even shown in this graphic and what do you want to do at this point? You want to make some choice got you want to choose one of these things. We are all scientist years we want to make informed decisions based on data. Right? And aren't these frameworks quantitatively related, don't have data on them? Of course we do. We have plenty. So for this graph here, what is again a subset of all the things available. But the colored blobs that you see in the middle are some case studies that these methods on the outer edge, our relational. And what is the problem here? The problem here is when you have two different methods that share one common case study, one common quantitative analysis. Think and talk about them in terms of their performance and that common benchmark. And the frameworks or methods such as quick check and small check in this graphic and it is possible to talk to them in terms of that depend on data. So we wouldn't be making informed changes on. And is a problem we are talking about today. And within the property based testing space, with all the abundance of methods in case studies. The problem is there ad hoc nature. The fact that they are on standardized. The fact they are not always comparable, the fact they are not always compatible.
 So to overcome this problem, we have worked on Etna. It is a multi-language extensible evolution and analysis platform that we have populated with a set of footnotes programmes they can test your testing tools on. That come from areas that the programming is heavily used. And that existed in the literature before. And also added some baseline strategies. The if you are a copy based testing tool developer. You can evaluate your own new fancy generation strategies against those baselines. We have added some example experiments that we have done on Etna in the process of guiding it, and show you how you can use Etna. And also to demonstrate the usefulness of Etna and a number different of cases for a different number of job objectives.
 And at last we have added some building case analysis presentation methods into Etna. So once you've done your relations you can write your papers easily.
 So for the rest of the stock I have two main agendas. In the first one is, introducing Etna to you got with the details that you will find useful if you ever want to use Etna. The second part of the talk, I will talk about our experiments with Etna. And try to demonstrate how transom team can be useful to you in a different number of cases if you 're interested in the property based testing. So what is going to the details of Etna? The first part of the details is about evolution. So how do we evaluate testing strategies within Etna? And here we use the mutation testing. So you might be asking what mutation testing is. Let me briefly explain to you. That's go back to the beginning and here is the body of the insert function. So the certain function consecutively compares the piece of the item with the keys of the note in the tree and it finds the right place of the item within the tree. And if we have an item and if you're going to add a new item with the same key, which is going to have a good testing methodology if you have a good tester, you would want to find the bug like this right? This is what the mutation testing relies on. Mutation testing relies on the fact that if you mutate a part of a programme to incur a buck and then you can run your test on it, you can run your testing methodology on it. And looking at the performance, if you are mutates are realistic enough, if they are close to the, like if the testers like testing methodologies performance on that mutant reflect its performance in real low-- real world and you can run realistic realizations in your testing methodology. And that is what we do in Etna. In Etna we rely on the handcrafted mutates that come from different studies in the past and we add them to our workloads using this comment orientated syntax. And I had talked about the close so let's go into what they are. So it in Etna is a programme. So some set of properties like the binary search tree and the insert valid property we had. And some set of missions as the one I have shown before like the insert GT. And we have three categories like today. We have three categories of workloads that come from areas where the functional programming is heavily used. In the property based testing tools have been evaluated in the past. And the financial trees and many properties that come from AI as I said. I have to specify, the second language we have the simple type lambda calculus and system. That have the progress and preservation as the properties. In the last one is coming from the security domain, the information flow control that is coming from information not inference property. So with all of these properties, what happens when you actually want Etna?
 Let's say that you decided to use that and we have some new fancy generation strategy and you're running on one of these workloads that mention. What you see at the end? So here is our pocket chart. Here is a lot of numbers here. Let's mystify it. So here, we see that we have 36 task that are mentioned like this bucket in the first two. So let's pause for a second. And talk about what the task is. So when you have some properties, of some of the mutants, and all the mutants are going to be triggering all of the, like not all mutants are going to be incurring bugs in all the properties. So what I mean by this, is if you think back about the binary search tree example at the beginning. And the insert valid property. If you mutate some part of the delete function, that property is not going to be invalidated. Right. So if you have X number of missions and the Y number of properties. The total number of X times y. And which one of these pairs are actually bugs and so each of them are task, so here we have an imaginary workload, we have some 36 task, so your test is supposed to find all of them if it is good enough. And what one does like the dark to like the Black to white gradient mean. And it was evaluated on this and it was able to find 14 of these backs for the four of the task with the 60 second timeout. That's what we have. So we talked about what workloads are in the Etna and how you can see results. So what can you should do with that now? What are the use cases that the plot form provides you with? In the first test you can run the existing experiments in Etna today. Instantaneously without any effort so that you can replicate all of our results from the paper. And you can also add new experience on top of using the existing testing strategies and workloads. So if you are curious about any question about property based testing. And you think that, you can answer those questions, using the existing thing on top right evaluating existing charges on existing workloads, you can use that. Can also add new strategies. So if you have a new fancy generation technique. Then you can add that. For Etna, by complying with some simple language that will have conventions. And run your own new generation strategy against the baseline strategies that we have divided it. So you can benchmark it. You can add new workloads, if you think that you have a programme that is going to help Etna be more representative. You can add those workloads so that the rest of the committee also benefits from them. And at last, if your favorite which is not Haskell or Coq so you want to do property based testing any differently which what you can add any different language driver that will ask for other people to add new strategies over close or also run experiments.
 So let's go into the second part of the code, were a talk about the experiment that we have done with that one, Etna. So we have experiments that we have ported today in our experience report. And I would like to talk about these experiment in 2 phases. And the first one is, the first to experience about comparing frameworks and Haskell and comparing generation strategies in check and coq.
 The reason I have these to talk about in one group to talk about is that the existence of these experiments already filled some of the blanks that we have talked about at the beginning of the talk. The circuit diagram. And the second phase is we have three more experiments. And these experiments are, they originated from our questions that we are curious to answer. In terms of property based testing. So these experiments both showcase that Etna is not only useful for comparisons between tools but it can also be used for a variety of experiment take and's. For different aims or objectives. And they also guide, like they are also, aim at guiding you in creating your own experiments. Within such different settings.
 So what is going to be experiments? The first experiment that we have is comparing frameworks in Haskell. So here we have three frameworks and also 4 generators and the two of the generators come from quick check, what is the naïve type based generator that is unlikely driven derived from the type information in Haskell. And the other one is the bespoke quick check generator that we have written by hand. And the bespoke quick check generator is aimed at being a top line, that generated automated methods are aspiring to achieve. And the other two, are the lean check and Smalltalk generators that are both integration-based generation techniques. So that see how these different generators fail in the workloads.
 And when you look at the binary search tree, and how the methods fair, we see that the old quick check and lien check every close to our bespoke generator, the top line. And small check falls behind them. And when we go into the ropelike tree, we see semi trend with respect to ordering, going and going in the same way. What we see a drop in all three type waste methods. So let's pause for a brief second. And you may talk about why that is.
 So going back to the property and looking at the condition of the property. Where we have, if the original tree is a binary search tree. Then the results should be a binary search tree. So if our testing strategy is not able to generate a valid binary search tree. That we really cannot test this property. And in case that is for such as reflection as the invariance of the data structure, gets more and more complex. So it becomes more separate from the type definition itself. And it becomes harder for the type based generator to satisfy. The preconditions. So we see a drop in the all three of the type based generators but we still see the same order in between them. And then we look at the hold picture. We see the same trend continuing and all four workloads in Haskell. And one major key take away that one could have here is that if you are today using small check, it is definitely worth checking that lead check as far as our experiment go.
 And so now let's look at our second experiment. That is comparing different strategies within quick check and coq.
 So here we have four different generators all written in the quick check. And the specification based generator that is automatically driven from the inductive specification in Coq that are similar to the invariance that we have seen such as BST. The last one is a bespoke generator that again acts as a top line. And we see that all of them are very performance in buying a binary search tree example, like my binary search tree workload. And we see this in a drop in the performances in the red like tree. Here we can see that the specification based methods, specification based generator is rather better than the type based ones.
 And again looking at the whole picture, although we see a similar trend, we have 2 two is here and they are both in IFC. So the first thing we see is that there is no specification based generator for IFC. And the reason for that is IFC doesn't have this one inductive specification that defines its validity as a precondition.
 And the second is although the type based closer is less performance than the typeface generator and all the other rituals, it sold seven task in the IFC were as a generator selves and the conclusion that we can draw here is that even though fuzzing may not be working in all of them. I can all of the cases. In cases where the generator is not able to satisfy the preconditions. And the fact that fuzzing has some feedback, may allow it to solve some of the problems. And our third extreme is about exploring the effective size in the bug finding. So there's this knowledge that if you have a larger structure, and the existence of an explanation number of substructures in that is going to mean that it can act as if it is as of multiple inputs are being tested. At the same time. So is this always correct you wonder? And we decided to answer that by designing this trial. And running the Etna. And here different site and like generators configured with generate different sizes of tree and we look at its performance in finding bugs. And what we see here is that even though it is both correct, in cases where you have dependencies between different input. It may become harder if you have larger structures. And thing about the structure and what it means. Here when you have the two keys and the bug exists only when the two keys are the same. Right. So if you are something a very large space, it might become harder for the like for the two keys to be the same.
 But if it is a smaller space, then the probability of collision is higher. So in cases such as this, where there are dependencies and we do multiple inputs. The fact that the input space is greater or bigger. The bug finding performance drops. As we were running the comparisons in the quick check, we have realized that the type phase buzzer is performing worse than we would ask Becky to. So we started investing and what the reason for that could be. And we have realized that two points for fosec the with the new version and have seen dramatic improvements. So with this experiment, I have completed my talk. And if you are interested in Etna in what it can do and what is about you can definitely check out the paper. But I would strongly suggest for you to just come use it. With us. Just talk to us, reach out to us, so that we can work on boarding you. And can be more helpful to the community. Because we will have built that for you. Thank you. 
>> Okay time for some questions. Actually before I asked any particular questions, I just want to say thanks I have done some work in the past on property-based testing for OCaml, and one of the problems with it is very hard to value whether you are winning. You do something and you find some bugs and trying to define whether is better or worse than the last thing trying to find bugs it is very hard with no standard benchmark solution. So thank you for causing this to exist.
 But the question I want to ask though as you mentioned the possibility of adding support for new languages. How much work is that? Should we implement all the benchmarks or what is it? 
>> For adding a new workload, the base work that you are supposed to do is when your programme, you need to be able to bill that. You need be to be able to run trials on it and apply returns. So if you emblem of these methods that are required for running experiments. Then you can just start adding like workload strategies into the incremental workload. And it is some effort, it would not happen overnight it is not possible. 
>> Okay thank you. 
>> Hello, I want to ask first of all you included small check in your evaluation. Is that the strict for the lazy version? 
>> So I have, I think it is, yes. 
>>  Okay so very often the lazy version outperforms the strict version by orders of negative. And 
>> [Speaker away from microphone] 
>> Okay I was going to say that there is the lien check is strict, it's possible to make that and things as well. I think one of the ripples that you have there is a lazy version that uses the same strategies as lien check. Would be interesting to include that as well to see how well that works.
 It is called lazy tiered small check I think or something like that, or tiered lazy small check. I mean is, not an independent tool. Just a module that pinpoints the idea. 
>> I had another observation as well which was that while you are talking about effective size on the time to failure, the problem you had was to do with the size of the space that you draw keys from. 
>> Yes. 
>> And certainly drying keys from very large space, especially if you draw the uniformly, it's going to be very bad at falsifying many properties. 
>>  Yes. 
>> But that's different measure from the size of the tree? 
>>  So at that like the actual extrema is like the sizes of the tree but it will be harder to make the point in a slide, so I moved from the tree to like a toy example that didn't really exist. So the keys is a different example. And the actual experience are like about the tree and the sides. And the button and the performance is about also like the tree size. 
>> Okay I would have done some similar, some more experience without to specify properties and their. You probably want the probably of the randomly chosen key being in a randomly chosen tree to be about 50%. So there is a relationship that you want between the sizes of trees. And the size and the space you draw keys from. 
>>  Okay I'm not really sure if I understand that. I'm going to ask you. 
