>> The third and final talk of the session is just getting set up. 
>> Hello, can you hear me? 
>>  Yes I can hear you but not see you at the moment. 
                >> MAHSA: Okay I am sharing my screen. Can you see my screen? 
>> [Speaker away from microphone] 
>> Can you hold on for two minutes, they are still sitting at the video here. 
                >> MAHSA: Yes. 
>> Thank you.
[NEXT SESSION]

                >> STEPHEN DOLAN: Okay thank you we are ready, we are having a speaker on Formal Specification and Testing for Reinforcement Learning. 
                >> MAHSA: Thank you, in general, reinforcement learning is concerned with learning control policies, or any gene see, agent, in order to actually recall while interacting with an environment. Will see an example of an agent that is a moving car that moves towards an obstacle. And the goal is to learn and optimal policies such that the car would break before the obstacle and also avoid sharp breaks. So in other words, what we are interested in and this reinforcement learning is to learn a policy that would tell us given the speed and position of the car, what is the required acceleration in order to achieve that course. So in general, the learning usually involves running a set of experiments. And the car, selects some de acceleration value in a specific environment. And then observes a report and by accommodating this rewards throughout different experiment, at the end, we know what is the expected reward for taking an action in a specific state of the environment. And this is values that we can obtain a policy that will tell us what is the best action that we can take in the state of the environment. So usually the enforcement research focuses on evaluation the quality of the obtain policies throughout this process. But the policies, something go wrong we learn the policy and with the policy might be wrong.
                 And the because of getting the wrong policy can be either because of getting a bad design or it can also be because of box happening in the implementation of different components that are involved in the process of reinforcement. But finding this box in the components for the implementation of the components involved in the reinforcement learning requires systematic testing. But at the moment there is a lack of testing or techniques and tools that can be applied for systematic testing of reinforcement learning setups. And in general, so there is a lack of correctness with the specifications for the enforcement set up. So this work that I am presenting today. We developed a partial correctness specification for reinforcement learning. And also we developed a method to test the temporal difference algorithms which are the class of the reinforcement learning. The algorithms and also long-term is a continuation of this work, a development test. Enforcement learning applications more systematically. And aside from our work is a well tested configurable implantation of the TD of reinforcement learning algorithms and also a test harness that includes a test, quick test that I went to excellent throughout this presentation. And before this first step we have formalized that it reinforcement learning set up. As a topic like this. And this is a very common for monetization for the reinforcement learning. That top company would find the reinforcement learning literature picks we find a infinite set of the states that this from the state of the environment and the agent of the site of initialization. And a set of observable estate that were present how the agent observes its surroundings. And after they are a set of functions. For example we have an observation function that maps the status of the environment and agent to the set of observable essays. The transition function that is a probabilistic transition function that turns out given data state of action where we are going to end up in which state we are going to end up and. But that probably. And also a report function that tells us given the state and action what is the revert that we are going to observe and then the function that will tell us whether we are in the final state of the environment.
                 But the more interesting part of this formalization of this topic is about how we can describe or specify reinforcement learning. And so let's have a look at the existing description of the RN algorithms in the reinforcement literature. So this is the standard impure representation of the algorithm learning. And here I will not go into the details because of the time constraints. But rather just, on a high level, we will tell you that in general, there are, involves starting in the estate and taking action and observing the reward. And observing what is the next action. So for this assembly. And also samples the next estate and the target estimate and the important part is the update that happens. Updating the value of the accident in the estimate that we have started from based on the new observation that we have. So based on the reward that we have observed and also based on the value of the next estimate --estimate and action. We are updating the value of specific action in the estimate that we have started from. And this is the core of the site that algorithm. But this is the typical representation that we would find in the literature. But let me just highlight what are the problems with this imperative discussion that we have here. So because of the presentation that we see here, there are many details and interfaces that remain within, because of the type of presentation of the algorithm that they have that are crucial for obtaining and then the value of the reward would depend on the action that has been sampled and also that they have started from or the absolutely none media policy depends on how the values of the queue are calculated. Also the sampling of the next estate remains in this description. So not knowing this detailed explicitly can cause errors once we start from this disruption in moving towards implementation.
                 Furthermore, there is also a lot of commonalities between different TD algorithms that remain hidden because of this description that cannot be appreciated why we are implanting these algorithms. For example here you see 2 visual representations of the steps of the two BDL rhythms and their backup diagrams. And they represent the many steps are taken in the algorithms. And you can see the commonalities are more visible in the backup diagrams rather than the in the texture representation or disruption of the diagram. -- algorithm. So these commonalities will be also appreciated or we are implementing that these items based on the current descriptions. Additionally, TD algorithms as they grow, or the TD items that not only take one step ahead into the account. And their updated step, they are more complicated in the algorithm. And so as the algorithms grow. The description becomes more complex. Which can cause even more errors-- and implementation. This considering the destruction that we saw.
                 So our solution to address the current challenges that we see in the imperative disruptions of the TD algorithms in the algorithms as we propose to move towards a more declarative discussion description of the algorithms. So we pose BDL which is the domain specifically which the can explain the backlog diagrams that you saw that visually represent and so we use this and propose this specific language that can describe the main elements that are included or that are contained in different backup diagrams of the TDI for instance. So this is the same impact BDL of linkers that we are proposing. So the BDL works first in the estimation terms, that can be of their sample or exploitation. And then also BDL term, can have a float chain of estimations which are the upper part of the back of diagram followed by the last of which is on the update estimate and followed by the sample or expectations.
                 So here for example SARSA this is the backup diagram that we saw for SARSA and the BDL disruption of SARSA something like this. The first term in this description corresponds to this part or the first element which is a sampling element in the backup diagram and the second part corresponds to an updated system in the cup. So in other words, BDL is capturing the semantics of the basic blocks of the backup diagrams. Which are the heart of the temporal difference.
                 So this is the BDL description for different backup diagrams that are representing different temporal difference algorithms. So this is the texture of the presentation using BDL. And now, we describe the semantics of the different terms in the BDL. We describe the semantics and a-- the annotation, a toy which gives us a interpreter for the video terms. And which is generated using this VPN interpreter which uses the four semantics of a backup diagram that can turn be considered as a specification for the temporal difference. So let's have a look at the semantics of video. So I just explain the semantics of the sampling terms for BDL. In this, so you can see here that the semantics of a sampling term in the BDL is something that is described like this. And the semantics is executed in the probability monad and here you can see a monad bind. And what is ascribed here is it is starting in the state and action and giving the current related reward that we have. Here, this is the discount factor that is required in the updated steps or in the value as well. Which we can perform the transitions, so simple the environment and then we can observe the next update. And here that the only operation in the monad. So the deterministic or the direct distribution. But the important point about the steps that we see are these compositions that you see here in the semantics of the sample term is that we clearly know which and we are each of these omens for example, where the next action will come from or how they are calculated.
                 And this helps us to pull from this semantic store and implementation directly go to an implantation of the algorithm in a functional programming. So this is just an example of how to define the semantics of the terms in BDL. And also defined the semantics of the chaining decimation which applies the composition of the estimations. That can be chained together. And also again we defined the semantics for the updated steps. In a similar manner.
                 And at the end what we have is the interpreter for the BDL that as a result of having this interpreter, we can get the semantics of the entire backup diagram and which can be considered as the specification for the TD so we can have a different composition types of terms that can give us disruption of different types of terms that can give us the destruction of different algorithms. TV-- TD algorithms and are meekly using the BDL interpreter as we can get the specification of those algorithms based on the semantics that we have. So we use the specifications that we have now for the property-based testing. So we read our test in the check so I will just explain that what type of properties we test in our framework. So we write the first two categories of properties. The properties that are concerned with RL problems and the properties are concerned with RL algorithm and then we separate these into  generic properties and specific properties. So the generic properties are the puppies that can be applied for any or and algorithms or are problem. And the specific proper these are the ones that can be applied to RL to a system in order problems or specific or algorithm. So this is how we categories the test that we rate. So example of the test that we write is so now using the BDL this-- interpreter, we can get suspicions of the TD algorithm based on the disruption I get. So given that we have the specification we can now test easily the limitation of this algorithm. So we have the test that compares the output of learning from the for example the implementation which is an example of algorithm, and the TD algorithm and that specific use and that was generated using the interpreter and compare their output and that which are just regions are probably just visions and we compare that these distributions are equivalent with each other using the statistical test. Another example of the generic test for the RL problems is the transition function which tells us that starting in any state we always end up with the state that is observable. This is an example, the another example of a property that should hold for any transition relation that we define in our RL for all problem. And implementation is also very close to this logical description that it gives. As you can see here the description of the tester the implementation of the test is very close to the logical description.
                 Another example of the generic problems is after any number of iterations of learning that we have, the values that will be observed, the values of the actions in each of the state that we observe should be within the floating-. Range and a test like this, generate-- generic test like this can help us define for example problems in the implementation of their reward function and that's his specific portion in their enforcement learning network. Another example of this, an example of a problem a specific test is that for example the court example. We can test that the forward moving car cannot move backward by breaking and the moving car cannot move backward by breaking. And this is a lot of --logical disruption starts with a positive velocity and taking the transition. We always end up in the position that is greater than the original position. And as you can see, this test requires writing his own generators. For the example of the position and velocity of the car. And our framework, we provide templates for the age writing agents and it enforces writing generators from the beginning, which facilitates writing such property test, for such test for the agents that are implemented in our framework. So are implementation using the colour tree. We have many reasonable test. And you can have templates for writing agents for example. Or writing environments in this template. And also the framework makes it easier to implement the new similar algorithm six we have the implantation of several agents that are either you know, like the smaller scale or some of them are medium scale that are inspired by industrial cases. And all are of able in the data repository that is in our paper. An order to validate the test or the test partners that we have. We also use mutation testing, evaluating how good the test that have been written are. And those that result also showed that there is mutation testing. So that in 75 percent of the cases the mutations was above 90%. So it shows that we have a test that can define the bugs that are injected into mutation. But in order --another interesting result was that odd cases, the generic test, it's only using the generic test, is either able to find 40% of the box. And this tells us that having the generated test that can be applied for any reinforcement learning problem or algorithm can find almost half of the bugs that we are injected in the implementation. So this tells us that using these tests, developer can have a head start on testing.
                 So I presented is a Formosa certification of the key elements of the backup diagrams for the BDL diagrams and a translation of these into a test harness. And our aim is to extend our framework for other reinforcement learning algorithms. In other classes of pre reinforcement learning algorithms and also classes up reinforcement learning algorithms considering richer, value function and also consider currently tabular representations. So yes, thank you.  
                >> STEPHEN DOLAN: We have some time for questions? So one question I was wondering in other applications of property-based testing, there are systems like for instance, small check that is the notion of exhaustive coverage that checks all things up to all test cases to a small size in the discrete domain. Or things that measure code coverage after running quick check. So if you thought about running, is there any way of quantifying coverage for the sort of continuous spaces that you have here? Is there any way of judging how much of a state space that has been a sword by this testing. 
                >> MAHSA: This is a very good question, so far no, we have not considered measures of coverage at the moment. But because we are also for example in our Corporation week mostly considered the estate, which discrete the continuous data space so that's a discrete set of estates, so it shrinks the data space that we have. But yes I mean we could consider that so far. But we have not. Is an interesting point that we could consider that maybe covers to see a much of the space is covered and why we are testing or the test that we have written, how much they cover this part of their space that they cover. 
>> Okay thank you. 
>> Hello-- epic games. I have two questions for you. One is that I really like the way that you try to formalize what a reinforcement learning algorithm is. Because I have read quite a few papers about reinforcement learning and always frustrates me how informal it is. My question is, have you found any ticket from the reinforcement and community? Have they fallen upon this formalism with glad cry saying, at last! We can write algorithms that we can understand! Or have they just ignored you completely? 
                >> MAHSA: Do you mean the impact of our board on the committee? 
>>  I wasn't so much meaning the testing that, I was just meaning your language, BDL, the formalize it, I mean like the hope that the reinforcement learning community itself, which in case it is big these days! Would say oh, fantastic we would love to use this! 
                >> MAHSA: I would be very happy! So far, this is very new work and we are covering TD learning algorithms and the tabular presentations maybe this is smaller. Even how vast the reinforcement learning failed is and how much for example the deep learning algorithms are popular. And this is a soft class of algorithms. I mean if they were to present the work with the community, but yes I think, if we move the learning community tours, my hope is that we move it towards more formalizing more deep reinforcement learning algorithms. Then we can really try some, that attention of the community as well. But once we have started implementing the agar them we have realized that there are problems in the presentation of the algorithms. And this is what the feedback that we have got is from our community, is the same. So want to start implementing you would realise that there is a lot of programmes in the presentation, the current presentation. 
>> Good for you, they need you! They need you a lot! Do have time for one more? 
>> Inc. You for a great talk, I was when to ask a question about the specification of the solutions. And I mean, we do a lot of work in the deep neural network. Specifications deep neural network which are used for enforcement learning algorithms. And then the community that is been with various degrees of futility trying to actually warmly verify specifications of neural network's. I was wondering if there's any equivalent work in the tabular-based approaches to the reinforcement learning of actually taking a specification, like you can write down your language, and checking formally whether the neural network --sorry, whether the tabular representation actually is guaranteed to obey the specification? And if not what would be the difficulties of extending your framework? 
                >> MAHSA: So I think that a specification can be used for support testing and verification. So we use testing because the testing is usually like given the set up that we have. Was more looked more appropriate at least for finding bugs for us. During the implementation. But is not so, their specific user not necessarily for testing. It can be also used for verification. We have not done the verification so far. But at this point the current specifications can be used for verification as well. 
>> Okay great, are you aware of any work that  [ Overlapping Speakers ] tabular presentations? 
                >> MAHSA: No, no, at least for now. Not so certification like this. Not like this. I can tell you that. 
>> Okay thank you very much. 
>> So it's me again, I've got another, another question about what success looks like? I guess and he reinforcement learning algorithm got you described you know just bugs in the implementation as being something you would like to discover. Up quite often a bug in the implementation might simply mean it does not learn as well as it could. I mean there is not a Christian criteria for it doesn't succeed. Is just you know the car doesn't break as smoothly as it does. Or the little figure doesn't wind its way through the maze as effectively. So it seems hard to test. Whether there are bugs in the implementation of the reinforcement algorithms. Unless there's some sort of catastrophic bugs that might simply fall over dead and has segment vaults or something. How can you get it any of those kind of just the more insidious bugs? 
                >> MAHSA: I mean I totally agree that it is difficult to test this type of algorithms. And so far, I mean, we've come up with some properties. Like for example, the generic properties that we show for testing a specific component. In the reinforcement learning over those. But looking at the like the whole process for example. If you look at the often policy and you see that something is wrong. It is fickle to detect maybe that something is wrong in the policy. But there is a body of work that actually works on the evaluation of the policies. But I think ours would be on the side of those works. So it may need, assuming that something is like the existing work discovered that there is something wrong in the policy that has been obtained. Then it can be on the side to define some of the work of the box of the properties that have been defined so far. So we know should set the satisfied for any type of let's say the reward function or any type of implementation of the transition function. And then that can help. I would say it would just be like a work on the side that can help be discovering at least a part of the parts and that can cause the policy being the wrong policy. I would say. 
>> Okay thank you. 
>> Thank you speaker again. 
                >> MAHSA: Thanks. 
>> Thank you.
