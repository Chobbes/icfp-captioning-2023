>>  Okay, so let me introduce the next speaker...
Yiyun Liu, talking about: Dependently—Typed Programming with Logical Equality Reflection.  Check, check.  Okay.  I guess I'll start the talk first with a very brief introduction. My name is Yiyun Liu, and Ph.D.  student at University of Pennsylvania and advisor is professor Stephanie Weirich, also co- author of the paper.  
First, what is this talk about?
In general purpose program language dependent types help us write safer programs.  But unfortunately, evaluating proofs is both costly, and sometimes hard to avoid.  .
In erasing proofs encoded as programs is dangerous when the programs are allowed to be partial, and this is what motivates us to design this core calculus, called system DE, dependently typed.  And it supports both partial programs, and erasable proofs, and most importantly, it is based on the design of GHC score intermediate language, which makes it suitable for future extension of the Haskell programming language with dependent types for the upcoming examples.  I'm going to adopt syntax from the dependent Haskell proposal, which has not yet been implemented in GHC.As an alternative, I could have used the singletons library to present all the examples,but nice to assume dependent types, to get cleaner syntax to formulate the problem.  
So first why do we need did not types for general purpose program language?


Here is an example that most people are familiar with, the less index factor type, it's the same as list type, but additionally indexed by uninatural, which tells us about the size or the number of elements stored inside a vector, the length index is very useful because it allows us to specify more precise type signatures for functions.  For example, we can define function, "vhead", takes the first element from the list And by constraining the inputs to be a vector of a nonzero size, and make sure never apply vhead to empty vector, I'm pretty sure you have seen it already, but independent Haskell, for all is more general and introduces dependent function type, and also means the argument is erasable.  
And in this case, we're allowed to use the keyword, to quantify over the index N and type variable A, because neither is used in the body.  
And we can also use dependent types reasoning about program equivalence, through the equality type, I present here as generalized algebraic data type.  So, one program that we might want to reason about This property is not immediately obvious to the type checker because this is defined by a pattern matching over its first argument, and in this case, because as a variable, the computations lot to prove this ...mathematically, we would need some sort of inductive reasoning, and this with cursor is can be encoded as a program that is recurvive, and has pretty self explanatory type signature, encodes the exact property we want to prove the for each keyword serves a similar purpose as far all keyword, and has dependent function type, and unlike for all key wore, for each, means the argument is not erased at runtime.  
In this case, the argument N is not erasable, because it uses by case analysis over it.  
To show you how this recursive function corresponds to an inductive mathematical proof I listed the proof obligations on the rights in the base case.  We need to show zero plus zero is equal to zero, which is trivial, reflexivity proof in the inductive case, we start off with our initial goal, to a few steps or reasoning through reduction.  Congruence, and we end up with a goal that n plus (I is equal to n.  And that is exactly our inductive hypothesis.  And this corresponds to the function recursive call, NZ of N, And in order to sort of make this hypothesis available to the type checker with you a case and analysis over at NZ of N within the body of the pattern match.  The only thing we need to do is write them, type checker, automatically pick up the inductive hypothesis, and apply reduction and come and apply the proof, and it's very concise, and might be reminiscent of SMT based theorem provers, and those familiar with the tools, underlying mechanism is different, we usually don't thing NZ as function, but witness appropriate above the program, but worth keeping in mind, if ever to evaluate the function, it's going to take time to this license year to input, because of the recursive call, we made in the successor case, and we'll come back to this later.  So, so far it seems that we have digressed a little by talking about proving instead of programming.  It turns out if you want to leverage dependent types, inevitably you have to write proofs. Here is an example, so consider...  vappend.  What it does is take two vectors and concatenate them together, and function we want to write is this appendNil, and want to append to empty vector, it's rather round about way to define identity function for the vector type, because if you append a vector to something empty, you end up with the same vector, so you would expect the return type to be the same as the input type of vector of N but unfortunately this append nil function written the way it is, won't type check, because if we follow the type signature exactly, the body of function is vector of size, N + Z, and unfortunately, as same reason we mentioned earlier, type checker doesn't know the vector size, and same problem.   We can reuse the proof at NZ, which we have written earlier by doing a case analysis over it, we can he type checker that this program has the right type.  And in order to do so, we first explicitly quantify a lens index, and each keyword, so we can refer to it explicitly in the body of the function definition, and through a case analysis over at NZFN now it knows, same as 0 and that program type checks.  And so one thing that really sets programming apart from theorem proving is that we really care about the computational cost, and so how does the previous append function perform.  Here is the program we wanted to write initially, and in order to convince the type checker that our program is correct, we need to add a case analysis at the front, and every time we apply the function before we get down to the real action, appending the vector, we need to first evaluate the proof, and for the reason we discussed earlier, the pattern match is going to runtime cost that is linear to the size of the vector, and that's not ideal.  
And so, just to be fair, append nil is not a useful function, and personally would never be used in practice.  But can still draw comparison, with its counterpart, and the same function for...  and last index.  So dependentent types, we have safer programs, and have this unnecessary performance cost, and in fact, there is more practical programs to write in practice, and you will run into the same issue of proof evaluation, and observed in the development of crucible, symbolic execution framework, that is implemented using Haskell with dependently typed features -- I guess fancy type features.  If proofs get in the way of performance, why don't we just erase them, if we stare at the definition long enough, we might feel very entitled to do so, because in both the case, we end up returning, reflectivity regardless, and if N is natural number, and not diverging computation, then NZ...  should...  pointless to some CPU cycles just for sake of the computation with results.  We already know beforehand, but it's worth keeping in mind that we work in programming languages, and means your equality proof could look something like this.  And loop as witness, to unload related types A and B, and doing case analysis of the proof, bogus proof, that takes input, type A, and cast to something type B, just from the signature alone, it's pretty scary, but turns out to be rather harmless, but if you tried to run it, it gets stuck in infinite loop, it doesn't do anything useful but didn't crash, However, if they try to erase the pattern match from our program, we end up with this actually scary definition of course, which semantically the same as the identity function, but it's able to cast a term to another arbitrary type, and if you don't use this function without caution you can easily write the program that crashes.  Here is the recap of that problem we're really trying to solve.  
Features such as general recursion are essential for programming.  
But, now that we can have a partial programs, witness body types, the equality proof can no longer be safely erased because they can could very well be bogus, and might be possible to impose termination checking our programs, but it's not very practical, because it takes away from the useful programs features that we really need.  Here is an important observation, if our program language is able to differentiate between partial and total computations, then maybe we can at least erase the proof terms, written in sort of the sublanguage that is known to terminate.  
To capture the essence of the problem, we designed this calculus called System DE, which is dependently typed system that tracks termination and relevance, from modalities terms in this language split into two fragments, and term could be programmatic, it's allowed to use arbitrary features like general recursion, and they may diverge. at the term could also be a logical, and it uses subset of program features and subject to termination checking, and worth pointing out that modalities here don't implement termination checking.  
They merely tell us which fragment of the language termination checking should be applied, our goal here is to allow programmers to write nontrivial proofs, in the logical fragment, and we can use the proofs in either one of the fragments and introduce no runtime cost, the typing judgment, being the new model type system is index by modality, which tells us which fragments term belongs to, when it's L, the term is logical, and P program is problematic and my diverge, function types are now index with modality as well, which tells us whether the function takes logical input or programmatic input.   So, using modalities to distinguish between total and partial computations is not something new and our work is more or less, inspired by the Zombie Trellys programming language, however, point out that being able to tell partial and total computations apart, also exist, and found in existing language, such as atreus and Agda, and can use, to specify each individual function and either total or partial, and through modalities it's slightly more fine grained, that we kind of discussed in the details in the paper.  
Here is very important convention, when nodes dropped from an notation, we assume, term or input argument, for example, a arrow B means it's a function from a programmatic takes programmatic input produces a programmatic or potentially diverging out put, here is an example of termination tracking with modalities works in our language, First let's look at the familiar definition of the factorial function.  Without modalities, it's apparently a function from that to net modalities.  We can assign fact to be a function that takes a programmatic net as input and produces a programmatic net as an output.  And this corresponds to not imposing any termination checking at all.  We don't enforce termination tracking on the call site of the function either, and in fact, all existing comes from nat...  can be assigned to the type signature.  And assuming our underlying mechanism for checking, knows for structural recurse, we can assign check, to function that takes a terminating logical inputs and returns a logical number as an output, however.  Regardless of the underlying mechanism we use for termination tracking we can never type...  assumption that takes programmatic natural number as an input and returns to number as an output, because in general, whenever matching over a potentially diverging computation, the entire computation would also diverge.  And our type system with modalities prevents the information flow from the programatic fragment to logical fragment, and we don't have accidental leakage of nontermation in the logical fragment, and system DE, equality type is primitive type, like the quality type we seen earlier, defined as data type, insist on the as equality  primitive, we use the reflectivity keyword for consturcition equality proof, and use reflect keyword to consume it.  And there is some similarity in terms of syntax, compared to the quality type we seen earlier, the constructor becomes the reflextivety keyword, and case analysis corresponds to the reflect construct.  The similarity is good, it doesn't demand more from the inference, similar to what we already have.  
With modalities added to system, reflect, additionally requires, and quality to be logical in the definition of signature, the input equality, annotated with modality L, and terms of semantics behaves more like type annotation, and makes different than case analysis, because the reflective equality proof is never evaluated.  This new primitive quality type works really well, with runtime erasure and by revisiting the example from earlier, we see how it helps address the performance issues.  
And so...  the proof at NZ from earlier, with modalities we can assign with more precise type signature, and takes logical natural number as input and returns the terminating proof, here is example of appendNil with case analysis, and new version that uses reflect.  The most interesting difference can be observed by looking at signature signatures.  Or apendnil.   And is quantified by the for each keyword.  And just as a reminder for each keyword indicates the argument is now the reraceure runtime, and each N needs to be presented as runtime, in order to evaluate proof of NZFn in scrutiny case analysis.  However, with the reflect keyword.  This reflect the proof is never evaluated.  We don't actually need the lens index N and means it's erasable, and take keyword quantify over it, and which tells us the L-index N is irrelevant at runtime, so this version on the right, after erasure gets rid of the index argument, and the construct completely.  And this gives version of append Nil that has the same cost, as the append Nil with the list counterpart, where the index is completely erased, and that solves the problem we want to adjust at the beginning.  
And so far, I have presented the examples using barrier declarative index, but in reality the system is core calculus, and annotated and inspired by system D, dependently typed extension of GHC's Core .  And so introduction how GHC works, you take the syntax and elaborate into the intermediate core language, which has properties such as cycle type checking.  The rule, requires an explicit proof witness called coercions, which I‘m going to explain shortly.  And we mechanized, the type soundness proof of system DE, Coq proof progress.  So the construct we seeing earlier, suggests the quality is really extensionally flavored.   Which means we need to first show the week normalization of the logical fragment.  So we can prove that definitional equality in this language, is conflict, and we need to prove that before we can show progress.  The proof technique is pretty conventional, and the main difficult part is really the definition.  Because when specifying the semantics of the language, we need to be explicit.  And also when we define the logical relation itself, we need to be very careful what it means for something, termed to be of Val, when you can't have the coercions floating around.  
And here is how the coercion proofs appear in the conversion rule system DE, and I'll omit the modalities for now because it doesn't help with the discussion. If you have seen the conversion rule for other type systems, like... you will notice the equality for system DE is indexed by the gamma that stands for coercion proof, and coercion proof gamma also has to appear in the term syntax every time we apply the conversion rule. As an explicit record of a reduction that needs to be applied in order to equate a and b. If we're in such a gamma, it is generally to equate a and b. If we're in such a gamma, it is generally undecidable, but the validity of the correctness of the gamma is indeed decidable. And Coercion proofs only exist so type checking is decidable. They are completely erasable from the language, which is justified by the lack of all the collision objects, a serious step to be using the untyped reduction relation, and this is lock-step simulation, and so prior to erasure, and after erasure, and so to extend the language with coercions with the Logical Equality Reflection feature we have seen earlier, we added two new constructs for the language. First, we have this construct that takes a coercion object and embeds it as a logical equality proof term. Reflect goes in the opposite direction, takes a logical term witnesses equality, and gives back coercion. So, just to see how the surface syntax from earlier corresponds to the actual syntax, from zero plus zero to zero. And in the successful case, flexibility gets expanded, you don't need to know the details, but gamma 1 does the reduction, and gamma 2 applies the coercion rule for application. The reflect keyword simply makes here at NZN proof available as a dilemma. When we are constructing the coercion proofs, gamma 1, and gamma 2. The final example we see how "append Nil" gets elaborated into system DE, the most important part of the elaboration process I'm showing here, reflected at NZ proof, appears inside coercion object. Gamma one is used to the right type vector size N + C to vector size N, however, as mentioned earlier, the object can be completely erased. So as I said earlier, even though it looks like case analysis, it's really like type annotation because we never actually have to evaluate the proof object when we are running our program.
 
And so, just to be clear, this elaboration process is not mechanized or formalized either, but only here just to show you intuitively, that in terms of the elaboration process, we don't...  aren't really demand much from the type checker, it's a very similar process, how you elaborate case analysis, and originally type of R and case analysis.  
And so I guess that concludes the talk, System DE is a dependently typed system that tracks relevance and termination with modalities.  In System DE, we can write erasable inductive proofs in the logical fragment write programs without any restrictions on features that may cause divergence The design is based on GHC's Core and can be used for future extension of th Haskell programming language with dependent types.  .
And happy to take questions.  


>>  Yeah, you can use the logical proofs, the reason about both fragments.  
And so there is assumption property, that says if you have a logical program, you can use it in a context that expect something problematic.  
So in terms of usage, you can use the logical program anywhere.  Including executing them as runtime.  
>>  I have a question about data types...  and languages like Haskell could define rustle paradox in data types, and so does it catch this, do you have to declare data types in the logical fragment and subject them to more rules or how does that work.  
>>  And just to clarify, we don't actually have data type definitions.  
The details is your system uses type and type axiom to add divergence to the programmatic fragments and to make the logical fragment, terminate, which is exclude the axiom.  So, I imagine.  So, I said earlier, termination checking mechanism, not part of you can use of existing approaches.  So yeah, I think if you were to actually make it into GHC, you need to have some restriction on data type definition, and another interesting thing I want to add is also matter of whether data type is conductive or coinductive, which we don't have a solution yet.  But I imagine we will be asking too much if the user has to write two copies of the the list data type.  
So I think that would also be an interesting future research topic.  
>>  Cool, thank you.  
>>  Yep.  
>>  Really interesting talk, thank you.  And maybe you partially answered the question already, I was surprised to see the reflexivity, and equality primitive in the system, and what happens if I want to define my own, if I want to do to heterogeneous equality for having inductively defined proposition to these things get erased as well.  .  
>>  Yeah, I guess similar to the previous question, when defining new data types, can you specify whether actually inductive, or if can encode for example, rustle paradox, I imagine if you have the primitive equality type you can build on top of it.  And have other inductivive types, that makes use of it, and you can erase those as well.  But then pure speculation at this point, because we don't have inductive types in our language.  
>>  Thank you.  
>>  Could also be other types...  [inaudible] 
For example if you remove proofs, and data structures evaluated and processing...  language...  [inaudible] 


>>  So I guess here the assumption is that you only erase something when it's actually pure, and pure in the sense that it doesn't even include divergence.  
And so I guess...  yeah, I'm not sure if the case you are talking about...  if it's pure, then it doesn't really matter the order of evaluation, you will end up with the same thing, regardless of evaluation order.  Are you worried more about fine grained performance issues.  
>>  That's one thing...  [inaudible].  


>>  I guess in that sense eraising the proof your program becomes "more defined" which is not a bad thing, I would say.  
>>  Got time for short question and short answer.  
>>  Okay.  So I think you mentioned...  [inaudible] 
Does that mean collaboration would also be undecidable, or more restrictive?  
>>  The short answer is...  no, it's not decidable.  Because your equality...  equality proof can witness programs that can potentially diverge, and generally not decidable process.  
And you can already do that without the event types, with the undecidable instances.  


>>  So if you didn't get a chance to ask your question, encourage you to find the speaker at lunchtime.  
>>  We can talk after.  
>>  Thank you.  
 
