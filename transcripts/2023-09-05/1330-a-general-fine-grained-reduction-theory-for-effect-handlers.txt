                >> NINGNING: Hello everyone! I am Ningning Xie and I work for the University of Toronto and will come to the morning session of SMP on effect. One of my favorite research topics. We have three talks in the session. And we have Q&A at the end of each topic for people from the audience you can ask questions at one of the microphones and you can ask questions through this court, we have paper specific talks on this court. For each of the talks, our first speaker is Filip Sieczkowski and he will talk about a fine-grained reduction theory for effect handlers. 
                >> FILIP: Yes thank you this is joint work with my colleagues Dariusz Biernacki and also Mateusz Pyzik the University Wroclaw. Of there might be some words in the title that are more clear than others for different people. Presumably. I would like to talk about a little bit effect handlers, what they are and how they work.
                 So what they are is this new increasingly popular way of structuring computations with the effects, mutations. So I have here an example a snippet of a programme. And they have several operations here and read as you can tell. And the ideas that these are the Dino effects that a programme can make or operations that can appear in the programme. And the for idea, I suppose from operational point of view, of how these operations work is that they have a meaning associate with them at this point where they appear in the programme. So what they say is okay I will pass the control somewhere else. And that part of the programme where I passed the control, might not know where it goes to just yet. It will tell me how to interpret the operation at that point. So the other part, these are the effects, the operations. The other part is the handlers.
                 And the handlers are what gives some semantics to mention operations. So we have the ask operation in the matching handler for the operation. And we have a Pell operation which snippet out of the code is not no worries, it is not interpreted. Right and then the idea is it looks like a little bit exceptions that generalizes exceptions because the control can return to the call site. So handler for us here is it says well I will get asked to provide some value. And I will gain access to the resumption. Which represents the part of code between the ask and the handler. And then I can pass the value back. So this is what will happen. So the idea is it a simple. We have some iteration through the list. 1, 2, 3. With a function the first ask for some value in the idea is it tells another value, computes another value and tells what the result should be. So we can easily see that you know, this programme can compute at least part of its output. No one of the things I mentioned already is that the tail operation is not interpreted in this snippet of code.
                 So the idea is if we start to just simply try to reduce part of the context and Passover to the handler and continue evaluation. We will eventually reach a programme like that. And this programme and the simple view, is stock. Because will have a partial programme and this can happen when I don't know, try to optimize programming partially evaluated. Train to find like fully reduce the programme and find a normal form. This sort of stuff.
                 In the idea is well we hang on in operation, we have a tail, we don't know what to do. And the sort of most of the popular semantics, operation of semantics for handlers at this point they would get stuck. So they are very nice for actually running completely evaluating fully closed programmes. Were all the operations will be interpreted. And then there will be no free variables. Nothing like that. But then in the open context, the sort of get stuck in a place that is not necessarily brilliant. Because that like I don't know about you. But this is not the sort of normal form that I would like for this mutation. There is still something that could be hopefully evaluated. All sorts of iteration over the second list, a right. The smaller list and then the idea is then well, do better? Because you know this is not great for programme optimization purposes. For instance. And effect we can. And so in the paper we build on some of our earlier work and you know other control operated context. Where we found out that we can give semantics to the limited control, by looking at how the delimiter, in this case the handler, interacts with the computation formats.
                 So the idea that is and this is slightly different syntax here. So how does the interaction of the handler with various capitation formers behave? So what we have? Well, we can have a capitation that is just a value. We can have a competition that is some sort of an operation here called to do. We can have a competition is formed by sequencing. Through expressions. And we can have a computation that is some other operation that the handler does not know about. So the tail in the example before and this is expressed through the list. And I will not go into too much detail about the do and list. There is some interesting stuff going on here you have to see the paper done. Okay so how do the interactions behave?
                 Well, the interaction of the handler with a value, it behaves in the same way that most affecting literature does. So you basically say that I have my handler here at the underscore the and I have some capitation I'm supposed to do after I finish the return value. Okay so if simply pass the value of the return value to the return clause.
                 Now the second part of the interaction, this can be viewed as a sort of optimized version of the usual reduction role for the effect handlers. Particularly trivial context between the handler. Now this is nice, it is short and you know, you don't really capture very much of the context got you don't need to reunify is a value. That is all nice but not all context or like that. And the idea then is well, we will get to the point where all the context look-alike lab. By successfully eating the intervening context. So successfully looking at how to handle interacts with the let expressions, so with the other capitation forms. And in the rule for the handling interacting with the let's, what we see is that the handler gets duplicated. So those of you who know about these things. This is what makes the reduction, the handlers to behave as deep handlers.
                 Because the idea is no matter whether the due appears, the operation appears in the E1 or E2 it will be handled by the same handler. See here, the age, the handler that the body of the handle what it says, the operation will be interpreted. Is repeated. So both will be interpreted in the same way.
                 And then finally, if the operation does not match the handler. Then we simply seek sequence a capitation and nothing in particular interesting happens. And then obviously for some picture there is some boring it reduction rules like data reduction, who needs that, right. Okay so let's take a look at this set of rules. For now it is just a set of rules, arbitrary. And look how it behaves if we start from the point where we got stuck last time.
                 So this is the same capitation we saw in the slide before. And this we see that we have an operation that does not match the handler. But it is not really in the evaluation position. We start with the sequencing operation. So there is a let a somewhere hidden by the semicolon and then what happens here is the handle first interacts with the left and gets duplicated. And see here it gets past to like the continuation of the tail operation gets passed to the return because of the handler and gets wrapped inside handler. And what we can see here is that this part, this duplication is what will allow us to reduce further and further and further going forward. Right?
                 So this we can sort of believe that this will possibly eat the entire list, iterate through the entire list. But first what happens on the outside is that the handler doesn't match the tail operation. So what happens is we simply sequence the return clause, the body of the handle with tell 6. The we get a nice tail operation just hanging there just waiting for the handler because we still don't know the handler for. And so in the end we get the sort of normal form, what we see below. And to my mind at least this is much better than what we had before. But you know. Obviously. This means we are all set, this is all fine. It worked on the one simple example. Chosen by me. So obviously everything is fine, right? Well, not necessarily. Now it just a set of reduction rules. Who knows what sorts of reduction. They get to encode. So the question that we would like to ask is what makes a reduction. A good reduction area? And to my mind, there's two angles here, on one hand but the properties that the reduction system has on the other system is applications.
                 Now as it happens, a set of reduction rules, recently to this one, slightly different setting, but very similar in spirit to this one. Was already considered, I think I see a year or two ago as a means of programme optimization. Okay this can increase our confidence a little bit. Because okay that is two sets of people coming from different directions looking a very similar set of rules. Right? But another part of what makes the reduction theory interesting or good. Is what sorts of properties it satisfies. And the properties that I would say are the ones that would make this reduction theory interesting and good for the perfect handlers is firstly confluence. So if we reduced to two different self expressions by reducing in different parts of our expression. We can always go to the same answer.
                 And the second one, so this is the proof is mostly standard. There's a lot of critical payers. But other than that there is nothing much going on. So you will have to look at the paper if you want to see the details. But the second part is, standardization. Which is quite tricky. And I will try to tell you a little bit about how this works.
                 And then obviously you know right now we had some sort of reduction theory. Maybe we will have some sort of evaluation relation based on that reduction theory. That is fine but how does it relate to the sort of traditional reduction that we started? And if I have some time I will be able to show you maybe a little bit about that. And then terms of applications what we also did in the paper is based on this reduction theory and the nice normal forms that it produces. We managed to build a semi- all rhythm, and for full normalization. And for the style over them. Okay so what is standardization even? So the idea is that this is one of the big open problems in lambda in calculus, right? So if I start reducing, if I have a sequence of reduction sort of happening anywhere in the term, so the sequence here in Black. The idea is that for any such sequence I can sort of split it into two sequences. First the evaluation sequence that will proceed to some point. Not necessarily to a value. You know, this is any sequence that there might be no reductions at all. And then a sequence that follows that of internal reductions. Now so far, so simple. You know, we can take the evaluation to the identity and every thing is fine, right?
                 But in order for it to make some sense. We want to impose some conditions on both the evaluation condition and the internal reduction which is what happens in the second stage. And to my mind the sort of defining properties for these are, the evaluation relation does not go under binders. Which is probably what most of us would process evaluation. And that it is deterministic not necessarily in the contraction itself. But in the location of the redux.
                 And the internal reduction like an contrast it always preserves the top level constructor and generally speaking it only allows internal reductions itself and the evaluation position of every construct. So is sort of very constrained in terms of how the shape of the term can evolve. All right so it doesn't do too much. We are forced to do the things that we can do in the evaluation in the evaluation stage.
                 Okay this has been solved for lambda calculus some 50 years ago and in the last 20 or 30 years ago there was a lot of interesting work and making the proof much nicer. Because this used to be a very tricky problem and the early proofs of this are pretty ghastly to look at. Okay so there is good methodology got well-developed. Just roll it out, everything will be fine. Right? Not quite, there is these equal payers that I talked about in one of these is around the one that we see here. So we look at the handle of let beta red X a right we can reduce this term here in two ways. We can either reduce the handle with the let by the interaction role for handle and let that we saw before, right? The one double heats the handler. Or we can simply substitute V in E for X, right, so this is a reduction that may appear, it's certainly can appear in the sequence here. How should we treat? Is an evaluation reduction? Is internal? Well it shouldn't be in evaluation. A reduction because clearly the other reduction for handle with let has to be evaluation reduction. And so probably has to be internal. On the other hand, the reduction in stealth here, cannot be internal. So it seems that we are violating this star drawdown here. It happens in the valuation position of the handler but it is not internal itself.
                 So the idea here, how do we square the circle, is we relax the rule about internal reductions. But only for handle. So the idea is that for most constructors, the internal reduction is defined in this way. Okay so any sequence of reductions is internal for a let expression if we have the sequence of internal reductions on the left and the sequence of evaluation reductions followed by internal reductions on the right. And this is allowed on the right because this is not the evaluation position. Right. This is the continuation. Now for the handle, these are internal because these are values. The handlers and the return values got nothing, you know they can't evaluate. So this is fine. This is simple.
                 And now here, and the evaluation position of the handler, the idea is that if we allow some evaluation steps, everything will work nice. All right so what sort of evaluation steps do we allow for? Precisely the ones that involve top-level left and lived. So precisely the ones like this one.
                 And then the idea is that we can follow our methodology, show transitivity of the internal internal reduction standardization is a nice consequence. Okay so that's great. Seems like a ad hoc hack. Or is it? And it seems that this is not just an ad hoc hack. It seems that there is a deeper insight into why this problem appears for a particular semantics of effect handlers. And this is because the evaluation strategy that we get for the effect handlers as you know part of the first part of standardization is hybrid. And that means that there is multiple kinds of evaluation context. And they treat the kind of evaluation context in which we are different determines how we treat certain connectives. In this case, lef--let and lift, okay so let and left can be treated either as evaluation context form is if we are outside any handlers. Or as a sort of psuedo values which we are not allowed to reduce if we are within the handle. So you know these hybrid semantics are used mostly for like normalization type evaluations. But here one comes out naturally. And you know this is the evaluation that this reduction gives us. This reduction system gives us. Okay. Can we relate that to?
                 And I'm sort of running out of time so I won't go into too much detail of how much --of how we do it. With the idea is that we can define several relations. And then we will relate that context of capturing reductions to the fine-grained reductions that we've defined. One direction a simpler cut the square on the autumn left is pretty scented. And the other direction a slightly trickier. And the two main points here is on the one hand, the fact that we need to allow some sort of context flattening to happen on the fine-grained side, this is the sort of normalization of the handler interacting with the computation formers. And then on the other hand because of how the captured continuations look. There is some possibility for the a tax expansions that appear here, it causes major trouble in the proofs.
                 But since my time is running out. I would like to just walk over the sort of what is there in the paper, what is the main ideas, what is the main points? In the idea is that yes provide a production theory for the deep effect handlers. With nice normal forms. The idea is that the semantics is defined by how the delimited controller, the delimiter of the delimited control ask another computation formers. We use the approach valve to buy Asako Takahasi and refined by James McKenna to prove the first standardization result for the first explicit standardization result at least for the effect handlers. And show how it scales to hybrid strategies for evaluation. And we show that the evaluation in our reduction semantics etches the classical approach extension now it. And the proof quite involved. And that has some nice allegations in terms of programme optimizations, this is prior work. And in terms of nice more forms allow us to define an VE and in the future we would like to have connections to pure calculus via CPS. A lot of the line work from which our insights that went into this work was derived and into other context and possibly we like to check whether this allows us to find a nice formal justification of sophisticated optimizations. Just a sort of still in open problem as far as I'm aware, impact of handlers. Yes thank you very much and if there's any questions I would like to take them. 
                >> NINGNING: This also and people are encouraged to ask questions online through discord. If there is any questions please, feel free to go to one of the microphones in the room. 
>> Hello. 
                >> FILIP: Hello Sam. 
>> SAM: Can use a bit more about the challenges of accenting this too shall handlers? 
                >> FILIP: Oh, so usually the difference between a deep and shallow handlers as expressed by different rule for handling and operation. And the sort of allows you to choose whether you want to duplicate the handler or not. And here because of the fact that this is, you know, the deep handler nature is already embedded in the role of the handler interacting with let. The it seems much more difficult to you know, determine how shallow handlers would behave. Because we don't know, like in shallow handlers, the idea would be at the handler age should handle the first operation and country. And in this point, we don't know whether is E1 or E2. E one could be a value in which case the handler should be used for the continuation of the let. Lab and or, and operation just in the E one in that case it should be allowed. I don't think I have a clear idea how to do this. I think that there are some possibilities that are sort of similar but not necessarily quite like that. But nothing set in stone. 
>> I would like to ask one question myself. So you start this talk with a motivation where you have this programme with the ask and tail. And then you know which statistically which handler is when to handle this aspect I wonder would happen in practise? Because in practise programmer often separate the programme where the operations performed from the programmer where the handler is defined? 
                >> FILIP: I think this can appear through partial optimization where you know the definition of the function, you know, sorry partial evaluation where you know the definition of the function which is, you know, in the module you still know it. And then you have some call and if you in-line the call, suddenly you have the ask and the handler. But yes it is quite motivated and well practise by the programme optimization work by Haeus and others. I think ICFP last year if I'm not mistaken, are they really want to have. And so what they do is a proper optimization not simply just rewriting like that. Of recursive function definitions, effect will, yes you have a recursive effect function. And you wrap that in the handler. And you really like to get rid of the handler if possible and you really need to figure out the really smart optimization. Not just, the reduction theory. But yes. The idea is hopefully with the sort of semantic with the sort of reduction theory. This might help justify formally a sort of optimizations. 
>> It sounds like there is some interesting future work may be by combining a partial evaluation and effect. I would hope so yes. 
>> Okay that is it.
>> FILIP: Thank you very much.
