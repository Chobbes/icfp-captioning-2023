Okay, welcome to the dependent time session.  We have 3 very interesting talks coming up.  The first one is a JFP presentation.  I think they all have something in common.  They're all about improving the type systems of dependent type theories that underpin today's and tomorrow's dependently typed languages by adding more information to the typing judgments.  This is to add further guarantees and tracking or to allow more good programs to be typed out and rule out more programs.  The talks are set for 30-minute slots, with 25 minutes for the presentation and five minutes for questions.  We have two microphones.  So, if you have questions, please save them for the end and queue up to ask the question.  I'll introduce the first speaker, Jonathan Chan, who is going to talk to you about sized typing for Coq practical.  
>>  And this is joint work done at UBC with Michael Li, now at University of Cambridge, and William Bowman at UBC.
I'll be starting with an overview of what size types are, and what they are for, and outline the key contributions in contrast to existing work on size-dependent types that are designed towards being used for, and spend the remainder of the talk going over one of the key contributions, which is the implementation of size types in Coq.  To explain why we think this design for sized typing for Coq is not practical, our setting here is the proof assistant Coq and core calculus, which is some form of CIC.  So for CIC to be consistent as a logic and for type checking to be viable, all programs must terminate, and the current method of checking termination of recursive functions...  So this minus example, you see, passes the check, which is a subargument of N, and this example on the right does not pass the guardedness check.  Because minus N prime M is not a subargument.  Unfolding the definition of minus to get it to pass.  And this is sometimes difficult to reason about and doesn't always work, and in fact, this code as it stands does not check in Coq.  And if we make a small modification like this, then it will check.  
But the change that was made wasn't even in the definition of "div", but in the base case of minus, and so returning the first argument "n" rather than "0".  
So this strategy isn't very modular between the top-level definitions and one more disadvantage to unfolding definitions it can also be bad for performance if the definitions are very large.  
But the real reason, we know that div will terminate because minus N prime M will never return an argument bigger than first argument N prime, because it's subtraction, and N prime is a subargument of N, So this is where sized typing is coming, and the idea is to mark type of minus with size annotations to indicate this.  And so when type checking div as place guardedness check, as a more expressive and more modular alternative to termination checking.  So these annotations will occur on inductive types and conductive types, and focus on that.  And annotations either size variable, the successor of size, and infinite size.  Inductive measures, at most, how large and element is, or how many lacer of constructor it has, and infinite size is used for inductive proofs.  
And so conductors increase the size of rotation, and successor of actual size that is one greater than that natural.  
And correspondingly, deconstructing a natural, gives, successful branch in subargument, whose size is one smaller.  
And so we're constructing N here, and successive branches, M has size S.  
Now, the idea behind ensuring the function terminates it can only recursively call that function on argument with strictly smaller size, so given a natural size, plus 1.  Recursive call must be on natural size E.  
The key here, is that minus doesn't increase the size of what passes through the first argument, so body of div when subargument N prime size V, is passed furs argument, of minus, we can still use result of minus, because size V, and first argument of div and satisfy the condition, only recursively called on argument of size V.  
And so far everything I described is from pass work on size types.  And the totality of literature dates back more than 25 years, and focus on these 3 in the middle designed specifically for adding size types to CIC, and CC, with could inductors, and so vision of these types of systems was to start from plain CIC code, and with size inference algorithm end up with completely size annotated code, and these systems also feature nested inductives and coinductive, and type universes including predictive prop.  
However, there is still a gap between the past type systems and fully supporting size types in Coq, and therefore designed new system CIC star on these systems, narrowing the gap closer to Coq.  There is number of changes that this entails, one of them is that past systems require you to always annotate the decreasing argument of recursive function, but Coq can sometimes guess which one that is.  So therefore augment size inferences to also include inferring those annotations.  
These past systems also need polarity annotations on parameters to really support nested size inductive and conductives and these are more complicated to infer, so far now we don't support them.  
Coqâ€™s kernel calculus also includes cumulative it, and global and local definitions were missing from those past systems CIC ^* has as well.  And biggest contribution, is prototype implementation of CIC ^ * size a notation algorithm, inference algorithm in the kernel of fork Coq 8.12.  
There are a few more technical differences between CIC ^ *, and past work.  
But, ultimately, this is the goal we were aiming for, want completely backward compatible size typing for Coq, so expressive, modular and efficient, especially compared to guardedness checks.  
And turns out trying to keep as much backward compatibility as possible, that also the direction of all pass work was headed towards, leads to challenges making type checking performant enough to be practically usable.  And to see what goes wrong, we'll dive into the implementation of size type.  


To summarize what the implementation touches, here is a pipeline of how toplevel definition passes through the Coq kernel, and goes through type checking and size inference in the same algorithm, yielding CIC ^ * term, along with a set of constraints those variables must satisfy.  
And we will then solve the constraints, and unify the size variables, to yield size expressions substituted in the term.  And this produces CIC ^ * termics that will then lead the kernel.  
So formally, for the first half of the pipeline, we have this judgment that takes the current set of constraints and type of context, and term and type, and left to this squiggly arrow.  And returns a new set of constraints with size entry, term and its type.  


In the second half of the pipeline, given the term, type and set of constraints on the size variables, in that term and type, these...  get unified with left, size and term, and type.  
And so once, this term leaves type-checking, it becomes part of global definition that can be used in other terms, these definitions will range over the size variables in the terms and their types...  and each use of definition can instantiate those variables different, in the case of Div function that uses custom definition, 3 times in it's type, with two different sizes...  V and W.  
And so on to the pipeline, and start with first half of pipeline, and focus on the most important rule, case of typing fixed points, fixed point F of type Tao, and body E, if body hype Tao with size E + 1, only allow to recursively call F size E, and to ensure, this is truly smaller size, we have constraint that V can not be infinite.  This step of the inference algorithm can run mulitple times for size and an notation inference -- the asterisks of tao.  And runs as many times as needed to obtain two pieces of information, one is to figure out which argument has the decreasing size, and two to determine whether the output will also preserve that size.  
So different combinations of annotations are tried until one that doesn't violate constraints or until we run out.  So here in the top right, we have an example of a fixed point type that takes two naturals and return the natural, and different possible combinations of annotations.  
That's the fixed point rule, and another important rule is the cumulative rule, which allows terms to typed as super type, and checking that subtyping relation, also generates a set of constraints.  
So if we look at part of the subtyping judgment, if we're given two inductives of size S1 and S2 we get output constraint the S1 must be equal to S2.  And intuitively, this is because the size of inductive, and represents at most how large it is.  In this example, natural of size, also has size...  if less than equal to S2.  Coinductives this constraint is contravariant.  With respect to subtyping now representing at least how productive, it is.  In general, constraint is an inequality relation between successors of size variables.  So E1, and N...  so on.  N represents how many successors there are on, on top of that size variable.  These are also known as "difference constraints".  
So the most important rule stating whether constraint is satisfiable, is that size is smaller than or equal to successor and all sizes smaller than or equal to infinite size, and also rules for reflexivity, transitivity, conquerance and so components.  So that was the first half of the pipeline.  And so far, those components.  ...  so that is all adapted from past work, and just augments to for the features, the new features that we have for CIC ^ *.  
In the second half of the pipeline, however, there is new addition, and handles global definitions.  
From checking the satisfiability of the given set of constraints we can obtain size substitution from size variables to size expressions, so given annotated term, type, along with set of constraints and either first half, we can apply the substitution we obtain from checking the constraints to that term end type.  
So because of the size algebra of this, and most past work are simple enough, the constraints are all different constraints, and these can be represented as weighted directed graphs, and checking satisfiability can then with implemented as negative cycle detection such as using the Bellman Ford algorithm.  And this is specifically how we implement it.  But there are other possibilities such as increment cycle detection, whose complexity can be a little lower, but still greater than linear than V and C.  
And so this satisfiability check is where the problems arise; because the number of variables and constraints involved can easily get very, very large.  
And so here is a sort of artificial example using the 6 definitions of natsl and so on.  
And just to illustrate roughly what the result of these size definitions might look like.  Here are size variables that might get generated for nats definitions for each inductive, nats and pair types, and along with the sample of constraints that need to be checked and solved, at the very bottom, exact variables aren't crucial, but what is important to note; is that each use of nats one in the body of nats2 needs its own fresh size variables for all of its sizes.  And so this is how we can easily produce definitions that handle exponentially many sites variables with respect to the size of the terms..
Can you see on the table on the right, the time it takes to check definition increases exponentially as well, to the point nats6 will take more than 2 minutes to check.  
And so these 6 examples are sort of contrived examples...  but this isn't just a theoretical problem, that only applies to pathological examples but does affect real Coq code, especially when compiling the standard compiling standard library with size typing.  
We looked at one file in particular, MSet list, which is an implementation of finite sets using lists, and can see that there is a 5.5x increase in compilation time with sized typing versus without and around 77% Of that total time spent compiling is operations related to checking satisfiability of those constraints and again this it due to the very large number of variables, and constraints involved during each satisfiability check, this graph here shows the log distribution over the entire file, with buckets of the number of variables X number of constraints along the X-axis.  And logarithm of the number of operations in each bucket along the Y-axis.  
And so, although most of the time there are few variables, and constraints, and so the graph is mostly grouped towards the left.  Yeah.  There is still a nontrivial number of instances near the right, where V X C can reach up to 10 million, and that's 4,000 variables, and 2.5 hundred constraints in the largest case.  


And so, to summarize: 
Compilation of Coq libraries with size typing has really bad performance due to constraints checking and solving operations during type-checking, whose runtime increases with the number of variables and constraints.  
And the sheer number of size variables that gets generated is due to having to generate fresh variables for every single inductive, and every single use of definitions multiplied by size variables are in that definition.  
So our conclusion is that size typing will not be practical if you follow the footsteps of all the past work on size types, CIC, and insist on full backward compatibility to infer all these size constraints.  If we allow ourselves to abandon full backward compatibility, so to speak, one possibility could be to manually annotate the types of top-level definitions, while still taking advantage of size inference to gather and solve constraints between those manually annotated size variables and the inferred size meta variables in the body.  In this example, again, such as minus Ndiv where the Vs and infinite sizes are written by the user and question mark U and question mark W are inferred ones.  In general, specifying exactly the size definitions we want to range over and how, this could avoid needless size variables and reduce the overall number, and could improve performance and make size typing for Coq practical.  But we leave all this for future work.


You can check out the paper for more details and discussion, including a full specification for the size inference algorithm and proof of soundness, and metatheoretical results and consistency proof attempt, and more detailed steps on the performance measurements.  We also have the now-closed draft requests that contain the implementation as well as some more discussion with members of the team.  Thanks for listening and happy to take questions.


>>  So slide where you proposed annotating, do you have an N-set example, how many times spent solving things specifically, and if you could speculate if you had something like this, how much time you could possibly save with that.  
>>I can't say for sure, but I know that file has a number of fixed point definitions.  
So I think this sort of thing would help, but I wouldn't know exactly how it would help.  
>>  Cool, thank you.  
>>  Hi, so when you showed that, you could use the bellman-ford algorithm, linear, and variable constraints.  
Do you know that the kind of constraints you are generating from Coq code...  still have this lower bound, but maybe special case that could be solved by a logarithmic algorithm or something like that.  
>>  I'm not sure.  I haven't looked at the specific constraints generated that closely.  


I think the performance was one of the big ones.  
>>  Yeah, smaller impact, would probably be more likely.  


>>  Between manual and automatic, did you explore having something akin to declared directive, and file record state types, that should be size...  don't need a standard library.  Finish.  
>>  So the way size typing works, there is flag turn on, to make some definitions if you want the definitions to be size.  The issue with not having the standard library checked with size typing, for instance if you have the minus definition in the standard library, and your own div function, and wanted to use the standard library, you wouldn't have the library, and wouldn't be size and wouldn't have the benefit.  
>>  This may have been answered with the previous question.  But is there a way to only run the sizing analysis when the regular termination checking fails?


>>  So...  yeah, but still the previous problem of you might want it to sort of annotate that size preserving-ness from in the standard library, if you want to use that property later on.  
