>>  Test, test my audible? All right. Okay. All right is my pleasure to welcome you to the session, the SRC, stands for the student research Competition thank you all for the five minutes. We have six students this year giving talks. And a poster session afterwards at the reception. We will start with the talks here. A five-minute talks, lightning talks and sort of introduction to the research area. There will be no Q&A. The Q&A we afterwards during the reception. It will be an extended 1.5 hour Q&A believe. So please ask your questions them. Pleased to make sure to visit the posters when you're out there and talk to the students. And I think I don't want to keep talking, I think we should just get started here. In the first one will Kai Pischke be from the University of Oxford, if you come here, yes thank you. To tell us about what is this higher order hardware disruption, semantics of synthesis time computation. 
                >> KAI: Yes okay thank you everyone I am Kai Pischke talking about the higher order hardware description. This is a little silly circuit that as three bits together. Full and are, and the lead expression that is defines a half matter which is like a sub circuit of the circuit. And we can see that when you use the sub circuit two times, we have like you do find it once. It is kind of an example of trying to make our hardware programmes more concise. And is something that we can actually do in real hardware disruption languages. And many times very log as well we can use kind of synthesis time loops and elaboration time constants. It is quite a lot of competition that happens in between us writing hardware disruption in actually synthesizing into something that goes onto would if PGA or ASIC. So I'm interested in this type of thing. In other linkages like clash which particularly exciting and all kinds of things before we do fully to the synthesized hardware. So this is the process that I'm interested in. And I'm particularly wondering how we look at the semantics of a language that features the synthesis time capitation as well as the description of hardware?
                 So going to look at just a very simple setting that features kind of combinational circuits as well as the simply type lambda calculus. And that will allow some functions between the higher order functions. So that we can look at this type of thing. So this is just a very silly little example of the same half adder everything but written as a function. Sweet taken implementation of the half adder as input and then we have the hardware that we have the input. And formally we define the syntax that we kind of split circuits into the hardware. Which consists of kinds of wires that are either pears or projections of applications and functions as well as kind of circuits that feature this lambda oculus that lets us do synthesis time computation. And what I'm interested in is how we do this semantically and make sure that it sounds like in the sense that we look at the meaning of one programme and we evaluated we get the kind of intended meaning that we were aiming for. And physically when I talk about the semantics of hardware we are really talking about the physical hardware. This we want to distinguished between the two equivalent circuits that maybe have different numbers of dates. We want to distinguish between copying and sharing circuitry. And this is a very interesting dissection because it comes up in other settings as well. So probabilistic programming has the same exact distention. Because it is also kind of a communitive of effect. And that is very different from generating into different random numbers. These connection between these two different areas is very interesting from semantic point of view. Because we can use a lot of the kind of existing semantics for probably stick programming for the hardware because it has a very similar structure. So I use the Markov categories for the wires and nominal sets for the semantic interposition for circuits. Because the circuits kind of contain this lambda calculus and they need to have a -- close category. And then for this we kind of did some theorems, we have normalization theory him that says if we type everything properly in the higher order types them end up with kind of the circuit that we wanted. And once we evaluate everything, also have a soundness through, that says all along the way we stay with the hardware that we intended. And I also give a kind of implementation that shows that you can actually use this. So you can kind of rate a little hardware disruption programme. And a slightly kind of extended version of this calculus with some syntactic sugar. And then type inference happens. And some various simplifications happen. And we do this kind of reduction. And we end up with a very log programme that we can either put in the FPGA were simulate or turn into the actual hardware. So yes that is my project. That as my research, thank you Very much for listening. 
>> Thank you, okay let's have the next speaker here. Who is next? Is Andrew. You know like in the form that one, when they change the tires. All right. All right, please welcome Andrew and Andrew will tell us about serving FPGA technology. 
                >>  ANDREW: Okay yes awesome, hello everyone my name is Andrew and I am a master student at the University of Washington with a pulse group. And today I will talk about my research assessing hardware compilers and see how well they are taking advantage of the hardware that they are actually compiling to. As a quick refresher, hardware, PARylation works a lot like suffer compilers and. Essentially this diagram here you have some high-level design fragment that some higher level of abstraction says with the hardware to do. You feed that into your favorite hardware compiler which of course is Yosis and you get some low-level implantation in terms of smaller at PGA primitives. Now these FPGA primitives, and the being quite complex. As you can see in the diagram this is the DSP for eight E two. And it is the ultra scale + FPGA architecture. It is quite complicated and implements very many things. But some of those things are the exception that you see on the site here which is the four data inputs that have these logical or arithmetic operators flowing Through 1 thing to know is that this complexity makes FPGA be primitives very powerful. One of these complicated primitives, you can efficiently implemen several micro designs. What this power is super useful to the end-user if only the hardware compilers are actually fully utilizing them. So I guess natural question for that is what does actually look like? So let's look at this, I am a hardware designer, that is me, that I have a design that I know will go into exactly one of the specialized DSP units. After the reading of the documentation. I come up with this conclusion. I write up one of the micro designs and I feed that into one of the favorite hardware compilers. And as the output is not just the DSP. But multiple of the primitives. Not just one DSP that there's next or one added to the site and I also have this extra lookup table. These extra primitives after I know reading the documentation are a necessary. I could just use one of the primitives to efficiently implement the design. Instead of the extra use of them, and it making the design overall less performative and they give a higher area. So the end result is that I'm a very, very grumpy hardware designer. Now, is this something that actually happens? Is another natural question. And answer to that is yes! All it takes is a simple Google's search to go to your favorite hardware form and see the post just like the three that I've screen shot here and sort of saying that they are completing about this exact keys. I have a design that you going to the specialized primitive. Instead of going on to many.
                 From these forum posts we can infer that the support for these primitives is incomplete. But the question is how incomplete are these primitives? Existing work has been done that rigorously evaluate these hardware primitives on the dimensions of correctness and also dimensions of performance, and no such work exist to evaluate these compilers and how well they are taking a vantage of the power of the individual primitives. So to answer that question, I built what I like to call cookie, it is a tool that evaluates Harvard compiler completeness, but quickly enumerating over a design space. Now the flow for cookie ends up being relatively straightforward. We start with some suite of initial designs that we know should go on to exactly one specialized primitive. We then tweak each one of those designs and several dimensions including say the signs of the inputs or the number of pipeline stages. This quickly helps us enumerate and build a test port foil that we can plug into the several hardware compilers. And then from there and something case-by-case. Either we have a case where the compiler sixties and we have a single primitive plantation that's efficient or we get the interesting case. Or get the hardware compiler has this completeness error. And we get this multi primitive them from mentation. And part of my research has discovered that across every compiler that we have seen so far that we've seen that sort of these complete. I'm happy about chatting with you later. Thank you. 
>> All right next up is Cynthia. And Cynthia's going to tell us about what is a good rule set? 
                >>  CYNTHIA: Hi so I'm Cynthia and this is my talk on the equality saturation and how we can figure if something is a good rule set. And I'm going to talk about equality saturation which may be you have heard of but I'm going to assume you haven't. So equality saturation is a technique that uses data structure called and E graph which is to perform nondestructive term rewriting. Which means we never lose information. And equality saturation and real applications, these are the ones that have logos. Which depend on rewrite rules to determine equivalence. And this is like an example of a rewrite right here.
                 So we have an application, we might at some point want to compare one rule set to another. To see which is more complete or useful. But that is kind of hard. And certainly is hard to do just by looking or maybe we don't have a benchmarked are the benchmarks take a long time to run. Also we don't want to just throw every rule that we can think of your application. Because too many rules that are not useful will add lots of information to the eager. Leading to the performance issues. And so to combat this we use this notion of derive ability. And we say that the rule is derivable by a rule set. If it is left and right hand sides are determined to be equivalent following the application of rules and the other rule set. But that is a little vague so more concretely we have come up with variance of derive ability. Say we want to see if we can drive rule X rewrites to Y to rewrite two X. One choice that we might make is that initializing of the graph with just left-hand side of the expression. Or we could choose to initialize it with both sides of the expression.
                 And what we see is the different variance of drivability actually result in different behaviour. So here, the rule Y it rewrites to the X, emerges X and Y in the eager graphic so we report this will as derivable work on the left-hand side here we can see that no rule fires so we can never report that will derivable.
                 And we also find that there are performance differences. In the terms of derive ability. So in this case we have the rule X rewrites Z. And the rule set containing these three rules. And here we do same thing. We initialize one side with just left-hand side and the other side with both sides. And we see when we initialize with the left-hand side only, we need to take two steps. Where as on the right, we'll have to take one step. And this is like a very simple example. Like in this case, we probably would not run into performance issues. But in practise you can imagine a rule set where there is a theoretically derived rule. But it takes a lot of steps. And practise the equality saturation tend to blow up within a relatively small number of iterations.
                 And so we may interest a situation where something should be derivable and theory. But in practise, timeout occurs before we can derive it. So now the question becomes, why would we. One approach over the other? Well we want to think about how the rules are going to be used. The equality saturation applications like Herbie which is this other car is optimizing up locations. Herbie is used for optimizing floating-. Expressions. But other applicants like the verifier on the right here. Are equivalent checking meaning they are trying to determine if two things can said to be equivalent? And if you think about it, one of these is a lot more like the drivability that uses just left-hand side, that is the optimizing side. Because we want to take something, and apply some rewrites to it and then see what we end up with.
                 But in the equivalence checking we have a goal in mind and we want to see if we can get there from the starting point. And so in that case, we might want to initialize that e graph with both sides of the expression. And that is it and we have an abstract here if you like to read it. Thank you. 
>> We have Ernest next. All right, you ready? Okay so welcomed Ernest here, he is going to tell us about Mica. Take it away. 
                >> ERNEST: Hello, I'm excited to talk to you today about Mica which is a tool that automates differential testing for the OCaml module system. In the module system we can have representation independence. This means that I can have two modules that resent M one and M two the interface I but M1 and M2 to implement later I in a complete lead different ways. We sometimes call it IE module signature. Here's a concrete example. Supposed that have an interface a finite set. We have an abstract type alpha T, and the anti-set and I can take the union of two sets and so on. When we can implement the sets is naïvely using less. Here I have to maintain the variant of the underlying list can't contain any duplicates. Another way that I can implement sets is by using binary research trees. Here I have to maintain the canonical BST and variance. Out a natural question arises. Our might to eat implementations equivalent? Morally, does my union operation on list behave equivalently to my union operation of BST's? Specifically, the notion of equivalence we care about is observational equivalence. Which informally says give it equivalent inputs through the functions by two modules produce equivalent outputs. How do we test for observation equivalence? When we, one way of doing so is by doing property based testing. This is a manner testing which we first write a property. An executable specification discovering some desirable behaviour of my programme. And then I write a generator. A function that produces random inputs with the desired type and she. And then I check, do these random inputs satisfy my property?
                 Property based testing was popularized by the celebrated library quick check and Haskell. Now here comes the problem. Writing property based testing code for different modules required to give a get programme or effort. This is because my abstract type is  instantiated differently across my two modules. So mica works, is the user I give it a signature into modules and commending the same signature. Like a parsed type signature and automatically generates property based testing code specialized to the signature without any programme or interference needed. Now I promise that all of the code I'm going to show you now is on Mac they generated by me, the first thing that it gives us his datatype and algebraic data type of symbolic expressions. The declaration in my original module signature would produce constructor for this algebraic data type, and the corresponds of the empty concert or, the add function corresponds to the ad constructor and so on. Mica also gives us these two auxiliary all directives types that represent the types and values of my symbolic expressions. Mica also automatically produces the definition of a quick check generator for random, well types of bulk expressions. This generator, generator, it takes the expression of a symbolic expression. Gen expre T only gives us random well types of all confessions of type T. He has well and example well type is about expression. This as well type because I can take the intersection of two sets. This however is not well type. And engineers promises never to give us that's about expression. It also gives us automatically an interpreter for disability extensions. This interpreter takes in a symbolic expression interprets it over my module implementation. Producing corresponding value. And then Mica gives us an executable for testing observational equivalent. Your we are using Dean streets fantastic court quick check property based testing library to do this. So how do you the pieces fit together? In our first run our generator to generate random symbolic expression. We interpret them over our two modules and then we run our executable to test for observation.
                 
>> [Speaker away from microphone] 
                >> ERNEST: For the stacks and sets respectively, if you remember nothing else from the talk, check this, observational equivalent for two ML modules requires significant programmer effort. But that is the property based testing that can automate this process. Mica is here for you thank you very much and see you at the poster session. 
                >> HOST: Very, very quickly. Here is a tutorial. Please speak into the microphones okay. Not like this. Not like this. Like this, okay? Otherwise we will get feedback and we will get bad quality, okay? Thank you. 
                >> HOST: Okay let's just move on, Michele is the next.--Vishal. 
                >> VISHAL: Is this close enough? 
                >> HOST: Welcome Vishal to the stage and going to tell us about sketch guided synthesis. 
                >> VISHAL: Hello I am Vishal and I'm a graduate of the University of Washington, I know it's been hard with talk so far but we will do one more. I will do a sketch guided synthesis applied to FPGA decompilation. So we are all PL people. Let's talk about a little bit more context on it to know what you're talking about. And his general talk I'm going to give more context on the background problem away during the poster session you know we will have a better time talking about more interesting things. So what does FPGA come PARylation actually look like? Let's get an analogy into software world. And the software come PARylation we start with a high level language and we have everyone's high-level language C, their favorite. And we process this through the software compiler. This is a programme that we want to execute we pass it through our favorite software compiler and we get the GCC or LL VM we get the low-level representation. Which is the architecture specific. In this case and think it is X 86 and X 86 that we have X 86 injections that we can be performed by computer. The Harvard design world we want to start off with a high level design that we want to implement on the FPGA. This harbor design is passed through the hardware compiler this hardware Design is outputting to a low-level and compilers have cooler logos. For one, FPGA primitives have a little bit him different instructions in a few key ways. So FPGA primitives are literally physical modules on FPGA that can pull my design. There is many differ types of them and they all have a little bit of different process when it comes to hardware compiler. Civil first we will start off with simple primitives. You may have heard of lookup tables. Lookup tables are simple hardware primitives that Tivoli taken a few inputs and output anything will bit. They have been around for a little bit and not super hard to think about because there's already automatic tools. Within hardware compilers that can reason about lookup tables effectively and send synthesize designs onto a lookup tables. As Andrew talked about, there are other types of primitives, or complex permit is like digital signal processors. Ignoble signal processors have a lot more different types of configuration and that they can do computation's much more effectively if used correctly. And as Andrew talked about, oftentimes there is none optimal usage with digital signal processors, however one thing that's important to know is that there is to automatic to usage of digital signal processors and hardware compilers. And your hardware compiler can still on Mac, map E high-level design to digital signal processors. Is not effective. This is not true for what I like to call the really wacky primitives. And play some stranger primitives out there and one example of what we're going to talk about today is called reconfigurable. They have outfits dash make outputs that change the behaviour of itself in the lookup table and they can have the outputs that can be changed in the other primitives. It was a pain just to even talk about this. And how to do hardware compilers and look at them. And the answer is there is not any really good automatic compilers and for these are tricky primitives. You might ask yourself, if there is no automatic compilers and in the action exist, how do people excuse them? And the answer is quite unfortunate in that you must Manley instantiate these permit is in your high-level design. If you think about that for a second, that is not so different than having to write in an assembly. Obviously this is a standard that nobody wants to live with. I don't even want to look at Verilog, I want to write a less structural analogue. And I want to be able to pass my high-level design into my compiler and then have it do all high-level work for me effectively. So the work that been trained to do is work with configurable primitives and get better on Mac completion for them. My poster today I will be talking with the ongoing work that I'm going to do to support runtime reconfigurable primitives using sketch guided synthesis. And this is part of a larger project we've been working on called-- cover FPGA compilers in. And going to focus on specific a one really tricky primitive called the configurable Lucky five by a company called Xilinx. And how showing what the initial progress that I may with supporting it so far. And supporting highly complex behaviour that I feel we have been limited by the current scope of hardware compilers with a more advanced techniques of compilers and. I believe that we can actually use these. You can actually suppose much more of these really tricky primitives and that is otherwise unseen. So will you can come to my poster and we can talk more about it. Up to see you there. 
                >> HOST: All right we will wrap it all up with bhakti.
                >> BHAKTI::   Hello everyone!, I'm going to talk about visualizing graphical proofs in coq and the proof assistants are kind of hard and really annoying to reason about proof assistants like inductive definitions, graphs do not inductive definitions. And defining a graph inductively does not end well for anyone I'm sure some people experience this. Also when you are working for the proof assistant you have this costly changing goal state, so the way that I usually work with data structure is that like really hard to look at is I will draw them out if it is a drop graph I will draw it up because it is a graphic but because you have some thing that changing so often it doesn't really make sense to drive it out manually. And why do that when you can, you know have something better? To the question here is how do we work with inductive graphs in the proof assisted and efficiently? And mean I think the solution is a bit obvious. I said graph, you imagine a picture, you don't imagine words. And the semantics are the same. You know, you're talking about the same thing. So just draw the picture, just look at the picture and think about it. And in terms of the picture instead of the words that really complicated. I'm going to backtrack a little here. I did say graphic it is a little too broad. I say graphic so I did not want to give a lot of technical details. But I'm going to narrow the scope to what I actually did which was a specific ethical language. Which is it is called graphical language and is called -- is the X calculus. And it is a graphical language. And it really likes this notion of of connectivity just like the normal graph and the main thing that you're focusing on is what notes are connecting to each other and how they are connecting to each other doesn't really matter. The the X calculus provides a complete set of diagrammatic rewrite rules for that manipulation. These diagrams and what is the diagrammatic rewrite rule? Well it's kind of you know a picture just like every thing else is. And I said this before and saying again, connectivity is the most important thing. And I will say more about this in my poster. So when we come to verifying this language, you're probably guessing, where is this going? You must do it inductively. You have to find it inductively. And you end up putting a lot of information into one singular structure. One term. There's a lot of complicated syntax. It is just horrifying to look at and no one wants to work with that for a standard. Of time especially if you are very-- entire calculus. And what is the solution here? You might guess you might say draw the picture, and this is where I go ah ha! Now, that is not it. Because you actually need to draw the picture that actually helps you. And there is a lot of background here which I will be really happy to talk about at my poster. Basically want to draw a picture that contains the information that you need to make progressing your proof. You know you don't want to look at things that you really don't care about. And that is pretty much all I'm going to tell you here. I'm going to leave you with this really scientific diagram. And if it intrigues you, come stop by the poster. Thank you. 
                >> HOST: Okay thank you that concludes the first round of the SSC, and I will remind you that after the reception where posters will take place. So please do go see the posters and asked the students some questions and engage with them.
