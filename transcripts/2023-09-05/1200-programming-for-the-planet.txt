>> Okay let's get started, can you find your seats please? [Laughter] great.
 Thank you all for coming to ICFP, it is my great honour and privilege to welcome you to the international conference and functional programme, will come to Seattle.
 I want to start by acknowledging that we are on the traditional end of the first people of Seattle, Coast Salish communities, Duwamish, Muckleshoot, Suquamish and others who continue to live here on trying until writing their ancient heritage. That QR code you can learn more about the communities and what they face traditionally and today. And but speaking of televisions, I really think we are here this week at the international celebration, we have one main conference, one symposium, 13 Co. Located workshops, for tutorials and programming contest. A student research competition on Friday evening and anchored you all to go to the performance evening at the farm, workshop and farm performance evening. + we have an online element of the conference. There is a discord instance which anchored you all to join. And all of our events are all streamed live on YouTube. That link at the bottom is our link to the virtual participation page. If you go there, you will find links to the discord and YouTube and so on. So do check that out.
 We are here and you have all you know signed the ECM code of conduct. So please be nice, both in person and online. If you feel mistreated in any way, feel free to come up to any of the conference organizers including myself, but I also want to inform you about SIGPLAN plan cares, they have this group of volunteers who made themselves of able to support people in the community who have felt harassed or disseminated or subject to bullying. Several SIGPLAN cares members are at ICFP this week. Their badges have these SIGPLAN red stickers on the bottom. You will be able to find them. Also any of the organizers in case you feel like you need some support. I will also encourage you about a few other events that encourage you to attend this morning. We had a job market mentoring breakfast. This afternoon we have a LGBTQ lunch, that is in the with the room, one level down. This evening at 6:00 PM we have the reception at the student research posters.
 Tomorrow morning at eight AM is the junior faculty breakfast, also a mentoring breakfast. At lunchtime we have a lunch for underrepresented minorities. And tomorrow evening at a location just across the street from us. Is the women at ICFP dinner. If you are a woman or identify as a woman, please consider going there and you can drop by the registration desk to learn more about it. And on Thursday we have a SIGPLAN‑M mentoring lunch as well. And so just to consider attending all of these events as well. Just a word on health safety, if you have any cold like symptoms, please consider wearing a mask. If you test positive for COVID, please consider not testing in person. Maybe tell the organizers that if you are an you have tested positive so maybe we can move to a more purple posture. Let me just thank our sponsors who are very generously supported the conference. And especially our two sponsorships chairs, Atze and Matthew will donate fantastic job as industrial cochairs. And then you to Arthur yannick and for organising all the workshops and Co located events. All of the colocated events are almost a bigger, collectively bigger than ICFP itself. So a lot of appreciation for the work they've done. Will see a lot of student volunteers this week. They will be wearing ICFP shirts and this would not happen without them especially to the two students volunteer tears Audrey and Ronnie. Thank you a lot. To our amazing video chairs, can we just give them an applause there. Apoorv, Guilherme, John. They do fantastic jobs pulling all nighters, just getting this whole set up working. And I'm really great for to them and then of course in our unflappable and utterly resourceful conference manager and treasurer, Neringa who takes care of all the logistics, thank you. And to all the organizers and the large team of of organizers, thank you to all of them. And finally, I should think Sam Lindley, RPC chair. And he sorted me out when I was writing my dissertation. It seems like every time I have something really mental to do reach out to Sam. And Sam is and Sam has put together an amazing programme for us this week. So with that have a great conference and I will add over to Sam. 
>> [Speaker away from microphone]
 How about now? 
>> [Speaker away from microphone] 
>> How about now? Okay I hear an echo. Excellent. Thank you very much Nick for your very kind words, Nick has done an amazing job putting this whole thing together. So I think we should actually, let's thank him now. But my job here is to introduce the first keynote speaker. He is a former refugee will talk to us about saving the planet. Many of you will know him best for his work on promoting commercial uses of functional programming and in particular OCaml where he led OCaml the lab teams at Cambridge and he ought co‑authored a book on real‑world OCaml.
 But recently his been working on something different, he is been increasingly involved in conservation. And now he is the professor of planetary computing at University of Cambridge. Also the director of the Cambridge centre for Carbon Credits and a member of the environment and energy group at the University of Cambridge. Where he works on large scale computation and sensing for environmental conservation. I am delighted to introduce Anil Madhavapeddy. 
>> ANIL: Thank you very much Sam and particularly hero pronunciation of my last name which was even better than I myself, management it is a pleasure to be here and to see so many old friends and many new friends. It is as Sam mentioned, traditional and the keynote to talk about the past body of research and to look back and contact allies it for the bigger picture. I'm not going to do any of that. A couple of years ago I decided to take a leap of faith and just pivot my research around some of the big crises facing the planet right now. And as many of you will be aware, there is a huge crisis going on at the moment. With the amount of greenhouse gas, being emitted into the atmosphere causing a greenhouse gas effect. And we haven't peaked in the amount of emissions being released. But that's not the only problem. An addition to all the emissions going on, we are also facing a huge amount of biodiversity loss. So while the UN set up a series of protocols in the nineties, based on the conference of parties which might have heard of, COP 26 and 27 just finishing the last couple of years we also have one similar for biodiversity. And in the case of biodiversity there has been a relative average decline in the number of regular conjured species by something like 59% since the 76ers the huge decline in the diversity in the natural world around us. And then we have this problem of does certification and the access to Drupal water is under deep threat. And there's another nation, United Nations and parties to deal with that. We have as a species in the past fix these things. So for example the Montr al protocol is one of the most successful international protocols in human history. And because we have these interlock series of problems, anyone solution tends to have a cascading effect somewhere else. So most recently, a good example is software emissions from shipping. This is response will for a huge amount of pollution, from international shipping. It is a really acidic, bad pollutant. And the EU put legislation in 2020 to curtail it from being allowed. So this really good news for the world in one sense because this crafter shows the megatons of sulfur being emitted by shipping. And there is a drastic decline which is good news for the biodiversity and straw. Acid rain and so on. But then if you measure the global warming of the world, sulfur is actually cooling agent. And  stopping pouring the cell in the atmosphere and as of a couple days ago the average global to picture is recorded, would you can see a really far outside where we should be at this point time. So we need to figure out how we can possibly put all of these different pieces together and try to make where we can take some action and try to have some measurable way of fixing it. So here we are, we are computer scientists, love trees, loves trees a lot. We will really want to make sure that we can play our part in helping you address this particular crisis. The good news is we have more agency than ever before in human history. To make sense of the state of our planet. So satellites have been launched steadily since the space age began. Give us access to global, real‑time and highly multisectoral ETA not just visual range lots of frequencies outside of that. And we have an incredible amount of data that we have been gathering over the years to let us know how we are doing. And is obviously really hard at launching these satellites. You have got to shoot them up and place, you have got to not explode, go to orbit around the world and then they be back to their data. And when they are stumbling is not there, we are simile and the fact that there so much data coming back that we don't know how to process it and then turn into meaningful results to keep track of what's going on. And then give it to policymakers so that we can engage in behavioral change. And this is the weirdest place to be faltering after we've gone to all the effort to put these things in the sky. And this is clearly somewhere where we as computer scientist can actually have impact. And this is, a little where we might actually cut rubric and so what we can do the next 40 minutes or so of the keynote, is give you some real‑world problems and research that I started engaging in. And about how to quantify the health of the worlds rainforest. And then to give you a sense of a kind of problems that we face when trying to make head or tail of satellite data. And then talk about how we actually apply functional programming techniques to dramatically improve how the operation of science happened in this particular space. And along the way it has been a fruitful couple of years in terms of identifying research challenges. And hopefully spark off some conversations for the rest of the conference.
 So the tropical forest, quick show of hands, how many of you have been to a tropical rainforest? How many of you have seen? Maybe 1 of ten of you have been there. It some of the most beautiful land on this planet. A contains about two thirds of the world's biodiversity. Lots of different different species have been recorded there. And it is mainly around the equatorial bell of the planet. But the bad news is because this tropical report doesn't have much value outside hosting a whole bunch of biodiversity from the perception of the anthropogenic human activity. It has been deforested steadily and manically since the 19 seventies. So this set of four plots show us the forest cover in Borneo. And in 1973 there was a heavily for said area and then over the course of the coming decades, it was deforested and degraded until the shell of the tropical forest remains. We can still regrow it but it has been really used up. It has been really used for agricultural purposes. So the land is being converted into the land used from undisturbed forest often into agricultural land for palm oil, coffee, cocoa, cattle or soya.
 So we need to come up with mechanisms to protect the force. One primary mechanism is by finding alternative sustainable livelihoods which give the local people who have been cutting down the forest alternative livelihoods. And making sure that sustainable finance provides an incentive to these alternative livelihoods instead of going for the immediate profit of cutting on the forest for palm oil.
 And to give you an example of the Gola of forest in Sierra Leone, has had a very successful project since the turn‑of‑the‑century. Where they grow sustainable cocoa within the force and use that in market that in order to provide alternative livelihoods to people in the region. And this is sold by RSPB, you could buy this chocolate, I highly recommend it, it provides lots of good benefits for things across‑the‑board for things that we are worried about in respect to the crises I outlined earlier. So the trees are preserved, there is carbon sequestration, they have been around for hundreds of years and they have been vast amounts of carbon in their that the lungs of the planet and it improves local livelihoods because there's the stream of sustainable chocolate that can be sold at premium. It preserves biodiversity because these old‑growth forest are not being cut down. Tree planting as an alternative does not preserve biodiversity the same way because there are into ecosystems that have been ruined. These project sorry when when all in all. But the challenge that we have is that we don't know how to assess whether or not this intervention has been effective or not. So the goal of‑‑Gola the rainforest project has been set up, it is a complex project with thousands of people involved in Sierra Leone. In the classic method by which these are assessed is when the project started, they declare what they think is the rate of deforestation for the party the region. So this is a manual process and then the experts can of dropship in Sierra Leone and they declare that. And then the goal is to create counterfactual to say that how much better this is intervention doing versus the background rate of deforestation. Throughout the rest of the country. And this difference tells a measure of how effective it's been. And this is usually inaccurate over decades because first of all, you are declaring what you think the background rate is. And of course everyone will vary in their estimates. And that makes projects and comparable to each other and we want to find a better way of doing this to require so much ‑‑not so much manual on the ground checks that have been only infrequently. And so this is where it comes in, every tree, canopies more or less visible in space. And we should be able to use this to generate an entirely new global answer, to assess everywhere across the planet, across the equatorial belt, just how effective these forest preservation efforts, with the goal of obviously halting tropical deportation in the next decade. And the way that we do this is by reversing satellites have already been up there in the sky. So since about the 19 seventies, 1972 when NASA launched Landsat, we had a series of Earth observation Silas up there in the sky. And we had roughly around 10,000 known sites, a lot of space satellites we know nothing about. ‑‑ spice headlights we know nothing about. And there about 900 of them and given that they have lots and lots and different instruments and so on all over them. And what they do is these things sweep across the earth. And they are taking measurements. And they form orbits that spiral around the planet. And they gather their specific instruments that of data and they beam them back to earth. Now of course this is not what you see when you look at a map. There is lots of post pricing that happens to remove clouds for example. To take multiple passes and stitched them together into recording it transformed the gives you a similar kind of map based interface. So the things that you see are composites after lots of processing over the satellites. And then we can use that to start reasoning about properties of places on earth. So since about 1990s there's something called the Hampson data set that measures the rate of deforestation across the planet. And this is animation output across the Amazon part of the South America. And you can see that these frontiers of orange sweeping through their. Something around the 20 percent of the Amazon is now secondary forest as a result of deforestation. So already the sellers are starting to give us a snapshot of the skill the politics we identify how bad these things are so we can start to identify solutions to fix them. So this is really urgent that we get get through this. Locally, the most insanely science‑fiction thing I've ever seen, helps us to get more accurate. The Japanese launched onto the international space station, an instrument called Jedi. Which is essentially a lidar, one of those terrestrial leaders that you see in self driving cars for example except they are in space and they are three of them and they measure surface topography. This is very different instrument from the spectral kind of visual or near visual wavelength satellites that were there before. And it let us build another alternative data set. And this data set is the surface of the planet. And start to tell us really accurately how high anything is. And for this in combination with multispectral results start telling us the estimate about the / the world's aboveground biomass. Which is to say how much biological tonnage is there in this part of the forest? So we are putting together a lot of multi model jewels in order to figure out for particular range of the plan exactly what is there.
 What we are trained to do is take the satellite data and looking at the percentage of forced data and our goal is to maximize that and then we have a project that started in a given year. And what we are going to do is come up with some metrics and these metrics will essentially measure what I call the additionality of these I will define shortly, leakage and permanence. And once we have these quantified, or challenges to apply these across the world and is easy way as possible to come up with justification for how effective all the individual alternative budget projects are around the world.
 So the first thing that we want to measure is additionality. There is lots and lots of force in the world. We can directly pay to protect them all which is as much as we would like to. So what we want to do is create counterfactual that says if we didn't do anything this is the direction the force would have gone in. If we do something there is a divergence, intervention, and hopefully less force cut down. And this is the difference of the sum between the force docs during the duration of the project itself. The way that we can measure this using satellites is similar to how we do drug trials. Because we can create some control spaces and use those control spaces to measure areas that should be as similar to the product being intervened but actually being the force to because they are outside of it. So let me show you what this looks like. So this is a system we built for Sierra Leone, for Gola the rainforest and if you just amount you can see that we are often quick of Africa. And the first thing we do is define the project area that is protected part of the rainforest. And this is really interesting part of the world. It between the border of Liberia and the most of the reinforced continues off over there to the bottom right of the map and there's the rest of the Sierra Leone. Which is very built‑up. And what we are doing is going to try to use the satellite data to sample different parts of the project area that is a green. And then we are going to and if I buy doing a huge search across these maps. Where there are some other countries, the same elevation, the same biomass and the same distance from roads. And we will pair them altogether. Then we will say let's find control points that are as similar as possible to the air is being protected. We do this and visually for every part of the satellite map. As you can see here I thought click and the different matches, there is different elevation slopes. Access to the roads is a big deal for the deforestation and aboveground Thomas which is collated from the commission of the Silas and then that lets us up some functions and say, will let's add up all of the forest mass against both the control which is outside the project, and the actual project itself. And this, the X axis here is by year, this is real data from 2012 Through 2020 and then the Y axis is the percentage of force. You can see already that the blue which is the controls are deforested at a higher rate. So there' s clearly some benefit to the forest, run by Gola. Because the areas we match on site all keep changing. And then what we do because we are doing this over satellite data, we can say hey let's just blanket the entirety of the project and get a statistically accurate measure which takes into account the social autocorrelation and cons of cofounding factors and gives us a graph the actual tells us what the traditional ‑‑additional to the project is. In this case, it's 2.2% for the curious, it seems small but that's a huge amount of force being protected and it gives you a sense of how we can start. To do the calculations that we need to do without actually having anyone on the ground going there and measuring individual trees. Which is a state‑of‑the‑art at the moment. And there is lots of variables being used here. Which 50 to 100 variables, all kind of things like soil models, we want to get as accurate as we can. For finding these control points. But because we use that satellite data we can just do that. It's a matter of searching and statistical test across the parts of the world. And then is not enough just to measure positive interventions. You also have to know and measure what is known as leakage. It is thought of as negative externalities that happen as a result of intervention. So if you intervene and for example a coffee plantation, the global man for coffee does not drop. So somewhere else is likely being forced it to satisfy that demand. So we also want to figure out and make sure that there isn't just immediate local shift. In terms of the deforestation that is happening next‑door. And there is other various like global equilibrium models to figure out if it goes to different countries. And this case we apply the same algorithm that we did before. Except instead of searching across the entire landscape. We just search in a buffer zone. That is nearby the project. So we just say got let's just look around the project area and look around this little leakage belt. That is a buffer around that particular part of the project. And this should tell us whether or not the deforestation is increasing locally. Because they are just taking money for one thing and then spending the deforestation somewhere else.
 And you can see here it's like searching for a needle in haystack, so this year, the tiny little things in the middle are the actual forest that we are checking for a particular part of the Sierra Leone. And this this is the five Monitor buffer zone, we had to be really quite accurate in terms of how we assess different forests and trees to make a check for leakage. But that is just a solution to make that possible. And then we also want to make sure that the permanency of these things, because trees don't live forever, after the project finishes this actually has a story of how long the carbon will be sequestered. So unlike putting stuff in the ground. Also want to keep an eye on the project, actually doing what it is doing even after the intervention itself. And the only way that we will win this in our fight is by government stabilizing land use. So there is a promise that COP 26 we would stop deforestation through legal methods by 2030 and well you know it's 2023 now, I think and we're deforestation continues to increase. And once the money. Going there, quite reasonably, they revert back to the things they were doing before and the manufacturers or soil manufactures just because the D four station to drop back to the counterfactual was. And so they come up with algorithms to try to measure the value of holding that you mission back for some years. There is some you and not forcing for the next ten years but not as much as not divorcing for the next 100 years. And we apply lots of factors and economics to do that calculation using the real satellite data of what being deforested as a baseline to give us the accurate projections from. There is a Aber coming out on the major climate change in the next month for those that want to see the details. Up for the purpose of us trying to encode algorithms. You have been taken a whirlwind tour here through what we are going to do in terms of calculating something from this. And we have some functions. These functions are given to us by the scientist and we have additionality, we subtract the leakage and divided by the ratio of excited permanence, and applied lots of satellite data, and we should have something that we can use to help save these tropical rainforest. No have something that we can absolutely dig our functional teeth into and try to say, how do we just make this happen as reliably and quickly as possible? Now the scientist that we talk to, they really, really know what they are talking about when it comes to trees. But then we sat down and actually look at the workflows for how they are actually doing their science. And it was, it was not the most ideal situation. So they would be hacking in R for example on the laptops, using Kroll and W get fetched things. And Google Earth engine which is hosted platform for these things. Then after a while, because there's resource limits on earth engine, they may be create 20 accounts and then eventually download gigabytes of data from NASA and then run it through a C + + forest simulator and compose these things. And run out of memory and before you know it they forgot what they did in the first place because they have been distracted by all of the intertwined problems. And it just took them years to come up with the simplest analysis. It was not a fun experience for them to actually do that export Tory science. And it was hard to reproduce. Data sets kept read changing. And it was just everything was going wrong. There is just also multiple different cloud platforms. And the question is we just it down and design a more functional approach. That could just improve the situation and just make it better. And the answer is obviously yes. Otherwise it would be very short keynote. And the structure of this pipeline should come as no surprise to anyone here. Because we have a bunch of data we want to adjust to, we want to run some data transformations over it. We want to make sure that is traceable as we go through those transformations. And we want to do some analysis of the algorithms that I outlined earlier. And then we want to iterate, we want to make sure that we can, we can recompute based on our changing inputs. And then we should be able to then publish them and actually have something permanent. And a lot of these words should be triggering happy thoughts, and a lot of you right now. Because this is the kind of problem the face that we solve all the time. And the challenges that they give us when we talk to them were simply the ones that you might expect to have to deal with, lots and lots of these intertwined data sets. There is lots of scale involves. And the had to recalculate and run their scripts from scratch all the time. They cannot reuse other people's results because they had forgotten what they share online and what they had not. Some secrets and not. And just generally a very mutable environment. And very hard generally to figure out what exactly is going on. So now let's spend a few minutes figure out what we might be able to do to fix this by applying some functional programming. So the first thing we did was set down with them. Instead of just telling them go learn OCaml, which I might tell some of my undergraduate computer scientist but definitely not a bunch of the colleges. He sat down with them and said how do you structure, restructure this computation into something that might look a little more tractable for applying some good properties. The first thing that you want to do in the situation is separating the data that is coming as inputs. And unwieldy to manage. And then you want to structure the actual ethical competitions that you apply over these things to get to your eventual output. And this seems sensible, it seems fine. Than the voices in my head started speaking to me. And this looks very much like a conventional application of building dataflow graphs. Using monads and applicative's as we do with build systems and other domains. So that is exactly what we did. So we just said let's build their trust and start to write these analyses down. And started building a DSL. That essentially composes transformations over geospatial inputs. And this is called Ocurrent, written by Thomas Herndon who is over there in the audience, he's sitting over there. So talk to him if you like the details about how it works. And a very high level, this is Ocaml code. And the use of it will be fairly similar. We define a term, it's augmented with labels and that we can actually build from it. And then you can parallel application. And this is just the set up when comes to encoding algorithms. And then sometimes you just add extra syntax, so there is nice little binding overrides in Ocaml the to help them look like normal little bindings so write them in the direct style. Then after that you need a little bit of expressive powers. Those initiatives are only enough to express static dataflow graphs. And then sometimes you look at the result of the competition in the pipeline pending on that result. And this is where you need to bind in order to run on the results of your previous commutation. And this is expressed directly in the DSL is over the only real difference in the conventional system is that it defines enough metadata that it can render a user interface for as well. So you are not just describing the analysis. You are also building lots of tooling around it. To visualize exact we what is going on there as well. And then you have to figure out how to get all of these scripts that they have written. We work with them to say well, we are not going to modify your scripts that is your science and all that we ask for is we just work together like all the built‑in downloads and the stuff happening there. And we help them delete code. And found the as sense and the script any pure way and then we wrapped all of that into nodes in the Stata programme and just start to express for example how to run some Python scripts, and very precisely declare its inputs and outputs and this is a sufficient build of isolated container name space they can run that code and reproducible way that if you give it in a certain way, we have wrote hundreds of functions to wrap up the scripts. And if it gets faster that you do more of it because that's just how programming works when you use stuff you've done before. And then we can actually start to them? Other than that we talk well. An algorithm that we talk about is we want to write down that initial ‑ leakage over part of it. And we can use that by using all the terms that we define. Earlier on. We can just run a tropical moist reinforced project. And we can say will pitch me the aboveground biomass from the satellites. By me the matching pairs for the satellite matching they saw earlier. And then give me the Permian's, and then give me back the final result. And so the execution engine around this can interpret this DSL and make sure that just runs on the cluster, runs locally. It just takes care of making sure that it can fetch the data and get you the result that you want. In addition to also just getting the result. Can also synthesize a user interface. So we look at an example user interface in this case here we have the data flow graph where you are reading a project that just executing through, that runs the valuation and it drops into a parallel mode, and you just click and an edge, and they can seasonally with the integrated results were. And they can see example this is a project was running on, what was it? In Indonesia, and Malaysia around the pill. And we can download all the intermediate statistics around it. And you can also say, well, okay that has given us some data about that particular thing. What exactly happened for this run? When you look at this run, this is still a work in progress interface. You can see all of the low‑level pieces that went into it, from cloning the precise hashes of terabyte of data. Fetching lots and lots of different pieces. And making sure that we had deterministically and sold every last thing. So we use a doctor for this but you can also imagine using it next‑door any of the purity function pipeline. And this gives us the ability to make sure that every point of the pipe I knew exactly what is going on. In the because we are running this over hundreds and hundreds of projects worldwide you know it becomes easy to zoom out and look at the different projects and start to build more and more elaborate user interfaces where you can summarize the results of running a pipeline by just building interfaces that can do that in this interface itself is three weeks old. But it was a very quick interface that could build just to unblock the various ecologist who are doing their job there. So this is a nice result. Because just building this DSL. Radically changed the approach they were taking to their scientific method. A sense of structure. Because it tracked all the transformations and generally results. It have to write OCaml the code and the mods and that stuff we will that for them and the results and give it to them and they were waiting for and then could drive that the user interface themselves. And we didn't go evangelize rewrite everything but try to build a more friendly environment using these larger so that would solve their your problem. Of course that's now, we have a ‑‑project to rewrite everything in Ocaml and the key thing about this is that the pipeline works both vocally and globally as well. And provides them an independent resource you choose. They no longer tied to one cloud provider. No longer have to depend on Microsoft planet or Azure or and use local resources and university community clusters McGavin. One of the big blockers was how to manage memory. So at this copy is a bit of surprise because normally you are worrying about multicore parallels him and how to deal with the efficient parallel dispatch and the biggest problem with these scripts are having was when you're dealing with terabytes of data. The Python scripts, would just run out of runner memory. It doesn't matter. How many couriers if they are just allocating huge amount in the GC's. And then finally, from the perspective, they could use this to systematically do sensitivity analysis across millions of parameters. Which they civilly could not do before. This is unlikely the ability to do the new analyses that they civilly get discounted because it was painfully selling the environment before they already been getting going with this. And starting to do lots and lots of good work there. So this is the pipeline started to get some green in it. So we built this pipeline for the purposes of checking the health of tropical moist forest. Then a whole different group in the same building showed up and said hey we want to figure out, how to measure the world biodiversity. And okay, I was like okay that checks off one of the crises so I should help biodiversity also very important. What is biodiversity? Well it is really complicated with carbon your measuring basically the weight of something going in the sky. With biodiversity your measuring, what is the probably of extinction of humanity for example. Anyone? Shout it out. One, right? What is the probably of extension humanity ovary time.? Less than one, then you have time and then you have space because you want bound by that region where you want to measure biodiversity. And stack the difference of probability of extinction and intervention that will hopefully make them go straight less often. And then you just want to suck hundreds of those species. So it's just a really complicated thing. By the observation was that they could reuse lots and lots of data that we'd wrapped for the purposes of the carbon project. And they heard rumors about the system and thought let's have a go. And then this biodiversity was a whole different ballgame and we again started rapping it. And this is cutting edge science, they needed to not only handle this extremely complicated metric but had to explore. And so what they use the pipeline for was to focus on different parts of the planet, to take different subsets of the hundreds of thousands of areas of habitat of species around the world. And the focus on particular ranges of taxonomies. And then try to visualize it in order to look at specifically what ecological dimensions you have their. And we did this over hundreds and hundreds of different variations. So billions of tiles, lots of hexagons and so on and so forth. What is happening here is first of all the yellow represents fewer species. And then the darker portions represent more. So obviously in the Amazon rainforest lots of species. And then exploit these things to see where, where does the different parts of the species happen. It was a weird yellow spot there, it is a city, they can just flip and dynamically select layers of they want. And they can turn the axes around to stack the number of species per topic and just notice everything looks weirdly flat. This means that we are using the wrong skills for management and then we need to tweak the results and switching back to the other access mode and then dropping into a detail part of the hexagon. And then figure out what ecological interventions they want to make their. So this whole thing is work in progress science. Because they are try to just understand exec lay how they can compose all of these different data sets. Enter try to figure out if they are worthy of taking forward future analysis. So the next thing that we discovered was having an extensible pipeline like this, in this DSL, was extraordinarily valuable. Because all of the previous functions that we had written into the DSL got represent the science. And science at different stages. And just applying that function and then adding more functions that just let them extend the set of work that they are doing quite well. And so now where we are is rumors have spread now, exponentially throughout the various parts of the Cambridge conservation Institute and there is lots of people just banging on the door trying to write stuff in this DSL in order to have a good a coating of their self. Because everything that gets added to the library it gets more and more useful for future endeavors. So it actually enables X for Tory signs and lots of the intermediate results are really useful. Just like the intermediate paper drafts the same holds true for the entire pipeline of computations across these bits of satellite images. Even if they don't make it into our nature or science. There is lots of data there. And then the next surprising result was the data versioning. Okay we solved the core of it for the DSL. ‑‑ with the DSL and it's given us a fair amount of data and we can figure out what to do with that. And then turn my attention to these terabytes of satellite data that was just sitting around and being adjusted. Because how hard can it be? Terabytes, it's in my laptop, my problem.
 When you look at these in more detail that you observe a few more interestingly unique things about the way that these satellite image formats work. Firstly, is that they have been going since the seventies. There is lots and lots of different file formats, weird quirks and as I showed use thousands of different instruments that have launched. So there's a whole different ecosystem of how to figure out exactly what the meaning is behind these how formats. And then space, space coming go they burn up the atmospheric after a decade or so and Landsat there is 90 different independent launches since the seventies. So there just different every time. And then the key thing is that we don't have the luxury of eliminating all the previous results just because the satellite is launch. We need to still assign meaning to the his Oracle data. So now we are try to figure out the fact that the quality of data just changing every year. And another weird result just came out of this. So assume that we are doing this forced exploration. And we want to say well, you're going to. A baseline of / of 2022 figure out how well we are doing. For a force protection against that particular year. And there is something called the GRS data set from the EU. It uses lots of satellite synagogue place the force mass that I describe. It goes from 1990 to the present day that is published annually by the team. It's really good data set. Then if you ask what is the force cover in 2020 Mac it very significantly demeaning on when you query it. So far so you hear a little different then we built for this purpose. So where we were in Indonesia, overhear obviously going to show you we are and Indonesia, lots of beautiful rainforest over there. And we can start to plot the actual forest cover in this area. So you can see here, we have got lots and lots of orange which is deforested area. And we can say well what is the amount of force for 2020. As calculated in 2022. You can see here that it's not filled in and we have got lots of forest. And then what are the amount of forest in 2020 and 2023. It should be exactly the same. But in fact, the algorithms change, new satellites were launched and there is a significant months of change for the same year of 2020. So the entire areas that were formerly thought to be forced it and now deforested and vice versa. Because presumably more high‑resolution satellite data can along. Who knows what it is. We look at this worldwide, is really a significant amount of data that is simply different as the years gone. So if you zoom out here for example and move over let's say to the Amazon. You can zoom in and you can see that there is just lots of forest whose interpretation and meaning has changed over time. But for us, we need to know in our pipeline, when exactly did this thing happen? It's not enough to say I'm version controlling the 2020 data set, now you have to. An additional matrix of the Cisco collated in 2018 using this particular algorithm, this type in 2023 using these data sets. And we have to use a transition walk across all of our dependencies across all of our analyses and make sure that they are consistent with the data sets they are using. We have a paper under preparation that just shows lots of the results published across the entire space have significantly varying results similar because they use different epochs of data sets across time. So absolutely controlling and understanding these will make a difference in the future. We are simply saying we need to understand what the difference looks like every time a new data input comes into the new algorithm. So trying to bring everything under this nice structured set of control. So the answer is that actually pretty straightforward, because we are trying to say hey, a snapshot of the data as it comes in. Use fancy files systems and container technologies and so on. And the controls and inputs and outputs. Remember that old mechanism in there with the codes that grips and a DSL that can declare its inputs and then a mechanism that can download them, we write pulling mechanisms and all of the weird and wonderful ways of fetching this data. For example lots of messy data formats to do with. But is all achievable because we bought our fundamental architectural under control hear. S overall the pipeline is known in a pretty happy place. Because the version code and data as it comes in, it is quite immutable afterwards, and then the only problem we have is how do we have a low lengthy feedback for this so that can actually do with the fact that we are trained to do interactive computation. And then in another course of our function toolbox which is another solution emerges which is self adjusting commutation. So Thomas built into oh current the core of incremental engine based around the self adjusting competition. Again you can see the familiar punches over here, you have changeable values that can change over time. And then define capitation's that can depending on the values, they can take a re dependency or they can update them. And then you can propagate the results and interface the external systems. So nothing novel from a functional programme perspectives. But actually useful when you put in the context of one of those pipelines. And we end up with a large, scalable functional and efficient pipeline that has solved many of the problems that we set out to just have a quick go at and when the project started. Across all of these things, it just compose nicely. And just leaves us with the front end which is how do you make these reproducible and publishable? For example in Jupiter notebooks and so on. But is very easily solving that problem we brought the rest of the pipeline under control. But if you are trained to sell vapor reproducible development under your feet it's not a problem you want to go into. So functional programming just gave us a lot of discipline in order to systematically get throughout and solved some of these things. And then the question comes out to whether or not all of this is maintainable. Because one of the very important things that we didn't just throw this code over the wall to the colleagues. ‑‑colleges that we made a deal. And the deal is that we will work with you and order to supply all the benefits and we will keep it maintained and under control for the duration of these projects. And it was really nice running the stuff and normal functional language like OCaml and there is separate libraries, there is links up, and hope with the slides up afterward. And you can find all published in the open repository OCaml's package manager many different pieces that went into building this pipeline. Of course our final product, the thing that is the actual user interface and the executables given to the scientist is something that we build and there's huge diaspora of dependencies that are just naturally the self‑contained pieces of logic that going to this. Our OCamls huge drink this over the years how easy is to cherry. Different subcodes that we use in different projects and just left it out and re factor into the new system   and I've been writing OCamls codes and lots and lots of different things and Zen virtualization days. And the stack stuff working in Doctor where we integrated our OCal into container management and the first thing as training wheels in order to build releases the compiler. And the mission‑critical and the tree saving project were really, really important. So this overall was a really interesting exercise. Because it has led us to understanding new aspects of ecological science. Brilliant empathy with those teams in order to let them advance beyond the state‑of‑the‑art. Let them dream about new things that were possible. All by using reasonably familiar techniques for functional programming. My job is a professor so I do have to occasionally publish. And there were lots and lots of really interesting things that came out of building the first set of iterations for them. And I just wanted to spend some time discussing some of the future directions were we think these could go in terms of advances and what we do from the FP perspective or more generally from the appeal research perspective. The first thing I learned is how sensitive the data sets are in a lot of the NGOs dealing with. So because these are your animal settings collected over decades. We have to be very, very careful about where the data goes. How many people here are into bird spotting, for example? Anyone? Anyone? FC2 hands go up. Now these extremely secretive WhatsApp groups were settings of rare birds are basically sent in can bridge because if they are publish them on social media they will be immediately get hunted down or captured and otherwise abused. And this is that scale very true. The data I talked about the biodiversity metric is hundreds of thousands of species all over the world that are some of the rare species in the world. One leg of this data that is statistically identify will to individual setting and will bring the people looking for value from those down is kind of game over. So kind of the thing we are exploring at the moment is how to start building a more basic your remodel into this DSL. And can example approach and take the information flow control start building that into the DSL. So the label all the data input you can define just decentralized access policies and then organisation should never ask us this result. Then they will be forbidden for and then the pipeline suddenly becomes a lot more manageable in terms of putting in data that comes from different sources. And the great city at work, for example who invented different privacy. Just observed that it is possible to have analyses where you cannot distinguishable of the partnership ‑‑mentorship ‑‑membership of any specific individual or the departure of any divisional. And is it just a lot of data full pipelines or is it just a lot of annotations everywhere? Can we make statistical guarantees about the outputs of these things so that you can declassify data later on in the pipeline and if it is sufficiently broad in general, we can still maintain memorization and caching through those? And then there are tens of thousands of these NGOs working worldwide. Often on a shoestring budgets, and the dream is, well we built this pipeline. So why not just deployed across the entire plan? If everyone and keep Ridge can benefit from this, then once we have a secured model we can just give them installations. And we should be able to figure how to build a cooperative mobile functional pipeline. All the results of these analyses are shared and replicated and made suddenly more durable over the long course of time. So looking in the programme at ICFP, there is lots of fun stuff. I see Lindsay in the front there talking about higher order choreographic programming in a few days. And this is about how to build code stages and deployed into digital distribution system. And very excited, there's obviously cases were we might want to break up our pipeline and kind of approach and we are deploying into a random computer and in Liberia for example. I saw one of the coolest demos of a user interface for a priority language, Hazel, sorry this might be the way that we programme the excellent non CS layer where you want to interactively explore the data suspects we are already with a whole bunch of existing published work in the space that we can apply to this domain. And perhaps add in all the security restrictions on this that we need. So this gives us a good structure for making that happen. And then the projects last for the younger than the Internet has been around. So we just committed to a treeplanting project in Cambridge will last for 120 years and the Internet is not around for that long. And the data formats keep evolving. The governments themselves are engaging in huge political shift at the moment. Their moving from analogue systems that just, they understand how to archive these things. To digital systems. And I don't think they quite internalized how big of a ship that is. It means if you are no longer able to interpret was something meant a decade ago. And is absolutely critical that we would be able to do that. But here I see at ICFP, lots of different pieces that can help us get towards it. So one of the keynotes by Andrea Strasburg Rossberg, about WebAssembly, which is nothing to do with web or with assembly. And is really useful with an intermediate byte code to less express capitation on lots of different linkages. So we can start to think about how to archive these things in the context of a pipeline like I described to keep these things run over a long time. And then ‑‑Fisher has done a lot of work and defining the meaning of what it needs to be a file format such as finding, August, general chair, Nick has built lots of formally specified higher order purses and on par servers, so it should be possible to put all these pieces together to build some of that is more holistic so we can actually bring some structure to the absolute chaos that is all the different file formats for the different organizations around the world. And this feels like not just in exercises and specification. But in exercise that isn't execute will specification. Because we actually use these things in the actual products that we are building and to help keep the more maintainable and understand the impact of changes afterwards. So there is lots and lots of exciting research that we can move into. Once we solve the basic problems in the actual ecological science that is the urgency of the crisis that we have at the moment.
 So I see some people  nodding, and some people going eh eh, I don't know sounds a bit heavy not hurt something want to get involved in. So should you get involved? Was not all roses. When I shifted to subject matter, everything changes. Their all publishing and the nature and science journals and nobody cares about SIGPLAN. We don't really have a home for these pieces of science to fit into both camps. Instantly enough novel in computer science, is a bit to come located and they don't appreciate all the stuff I just described. The only care about scientific outcomes. There is a real opportunity cost in action building things for people that are used. You just slow down. But on that other hand no. This is just noise. There is a genuine crisis. If you care about the outcomes. If you just want to think I want to make a measurable change in something. You might fail and that is okay. But at least the act of trying helps quite a lot. It certainly helped me quite a lot. Because you feel that you want to do some thing about that as well. It is not just a case of trying it is also a case of educating yourself about the actual issues involved in the various approaches that you might take for whatever you decide that you want to work on. It doesn't have to be forced obviously. It can be everything from carbon sequestration to more sustainable livelihood for people. But all of it requires computer science and data processing at a minimum.
 So what we did observe is when universities are basically setting up these interdisciplinary centers, if you need to find a home, then I strongly advise to find one of those. For me I was likely to get in the Cambridge conservation initiative, which is the building in the middle of Cambridge. Which there's lots of different departments. And so you can hang out and have coffee and learn about what is going on. A very informal way of having those coffee conversations. Which we are resuming again after the various lockdowns. And is important to build trust with people. Finding someone in your local institution that you can work with really helps. Because you need to have lots of to and fros. And make sure that you are around whenever your code goes wrong and because there trying to make you change themselves. And so there's so much dig your teeth into. And so there's whole domains that I haven't really discussed from numerical modelling, how to build scalable climate huddles from agent based systems, how to apply machine learning to these things and all of them obviously on the basis require solid programme language. So I'm very pleased to shamelessly promote a new workshop that will be topple, in January. That is programming for the planet. And Dominic Orchard and myself will be starting to begin this conversation of how to bridge some of these topics across different committees. So in addition to this, we get ICFP, I hope to keep going so that we can start building up a little bit of room for people who want to learn experiment and then spent time building up your own local systems and where every want to go. And overall I just wanted to leave you with David, David and Berg, the building I work in is David at a burger building. And there is so much good work going on that you don't know where you want to country to these really complicated crises. And so by the time I got involved, my overwhelming emotion is now excitement. Because they took the time to give us areas of functional programming were in my heaven. But also this is the time to exit do something about this. Where we talked about many, many years and in venues about CFP and sort of about how there's benefits and functional programming. But here we have an audience who needs it right now. And can measurably multiply their effectiveness when they go into the big world and try to draw a change. All we have to do is make the use of computing systems and data processing or efficient, more reliable, more scalable and more reproducible. And we have done our part in a big collective engine to fix humanity's impact on the planet.
 So I am really looking forward to continuing this conversation with all of you. And if you a have any questions or ideas or disagreements of what I said just, wherever I am. And if you are going to carriage, you will learn more about this and you're all very welcome to come down to get a tour of these areas and see how you might start something up with yourself and your respective institutions. So thank you very much for your time. And I look forward to the rest of the conference. Thank you. 
>> Thank you very much for a wonderful talk Anil. Very inspiring. We have plenty of time for questions, able in the audience who want to ask questions please come up to one of the two microphones. We can also take questions online through discord. 
>> [Speaker away from microphone] 
>> Closer to the microphone. 
>> [Speaker away from microphone] you made a really compelling case for usability aspects of FP improving some really important problem spaces. I wonder if you have done any kind of formal usability analysis in the style of HCI work or if you have considered that kind of thing if that something you think would be important? Or anything like that. 
>> ANIL: Is a great question so the question is the first metric is our people giving us the time of day and we are in a building where everyone is just really, really busy and there quite disconnected so people just disparate field trips to look at ‑‑for weeks and people can handle right now and are still the frontier trying to build regional systems that are self‑service that could be used by other people. And this is one area especially where the formal studies and accessibility are really impertinent. In the global South where illiteracy is really high for example. It is really power starved and often network start, the things that we take for granted here recently do not operate the same way. In the way that people use computing devices like phones, is just different culturally and technologically. So it is really important. And we will learn by doing those. So you don't want to pause in order to formal use studies. And it's to make sure that we have ethical approval for obviously any cause no harm and then continuously be refining our techniques. As we go through. Thank you. 
>> Let's take a question from online next. And when tour, asked for detecting data leaks have you tried to use approaches like Canary tokens.org. He says he has no affiliation with the project. 
>> ANIL: Thank you Edward, I have not by will look at that as soon as I get back to my desk. 
>> Fantastic talk, really exciting. You mentioned new workshops is one possibility for this kind of work, like making into academia. Right. I'm curious if you have any? Other kind of thoughts like how can this kind of work which is really important. A bigger presence or become or have a bigger presence in the academic world? 
>> ANIL: Is easier dash make interesting they say how can have a bigger presence in the academic world? There's one book I read this year changed the way that I think about all of this called five times faster by. And he makes the case that we have to move five times faster in terms of it de‑colonizing and how do you make that happen? One way is to give the policymakers for example the right tools that they need to do their jobs. Because they're the ones that have systemic influence over blends of people. In the way that we work as we tend to publish two other scientists and do direct publication but rarely to government for example. So is just not a thing. The way I'm trying to pass this right now is you need to get involved with the heart of the problem and try to influence not just other academics. Their probably the lowest of the priority but how do you influence policymakers in particular? Because governments around the world are just looking for answers. Now had observers sitting at the COP 26, I saw how all the negotiations happened in these climates conversations. And just guessing at the impact of what things were. And imagine if you had real‑time reactive system with graphs showing the fact that if you gave this emissions trading and you give it this line over here. This is the impact in your country. Kind of intelligence if you provided the governments can inform data‑driven decisions for the good of their own populations. And so I think that kind of empowerment is something that is really important. We should worry about how I can mix two, I guess just making it one thing at a time. Just making of able to the wider world in the first place. 
>> I have two questions. First more general and the specific question. The general question is the compiler in this project kind of seems to intersect with a lot of recent efforts that have been going with the reproducible bills and things like that. So have you done any connection with this kind of projects? And any of the tools and reduce any tools that might be used to to them? 
>> ANIL: Every time you submit a package to OCaml, we do have reproducibility checks and that is just happening incrementally step by step as you go through. So it is not 100 % reproducible yet we can find a large part of the ecosystem with the build tools due to reproducibility. And we can use containers and other isolation mechanisms in order to provide the rest of afterwards. 
>> Question. 
>> Jeremy please. 
>> To the back of the queue. 
>> You have been motivate by ecological questions but a lot of the technical work and what your timeout is applicable across all of the science. So astronomy and economics and things just lead to mind. Are you going to consider those? 
>> ANIL: So other science support, focus, focus, focus, so really goal oriented and occupied. So we want to make measurable difference in this decade to tropical deforestation so that has to happen. So everything that takes parity over that. And just generally, a lot of effort to learn the actual science behind this thing. So you have to. Your focal battles. And my hope here is that we are releasing tools. So someone else that you can look at into for mean the diaspora and using these things. That is where these shine. We are really good at using reasonable tools. So if anyone else wants to look at different area, I believe that's of a lot of value to describe these different techniques and incorporate. 
>> Thank you. 
>> It was very interesting to see your snapshot of the DSL. So that they were mostly play more. Functions. So I'm in India, to represent the concept of the domain into the types. And then when I was senior talk, thousands of times came to my mind, how crazy you were into doing that? 
>> ANIL: The good thing is that my co‑author of those who wrote most of the DSL is in this audience.  The DSL is written in fairly normal OCaml style and defined module types for the term language and then you just have functions that are combined over those. So is designed to be kind of relieve invisible and then you just have functions that are cognitive and combined over that. So it's not designed to be kind of really invisible deep DSL that hides all these things and running the code that we are really happy with the core of it. And there is another rewrite coming in which is to try to remove the need for the dynamic data flow and maybe make more stage approach. So most of time you need a part to apply these and then you can know and done a little bit of the discovery and then move on to the rest. So we are going to refine the core of it pretty fast. 
>> Relating to that, I had a question, did you make use of the new features, effective handlers for instance? 
>> ANIL: We built a new library called C I O O, which is a high‑performance drug style that uses all of the features of the OS and OCaml. OCaml itself doesn't specific use it because it's trying to describe a pipeline picks we don't quite see where it can hide that construction. Does mean that every other mode basically disappeared, so LW T for example is basically long gone. A think for example is a danger one that is gone as well. So it means you just have one more N.E.T. and I find one more manageable when is one of them. So that is my functional happy place. 
>> I'm delighted to hear about the rubble workshop. I think that our committee has a lot to offer to the scientist and many domains. And the fact, there is a new workshop this Saturday collated with ICFP called declarative biology and medicine. We are trying to get people from this community to try to figure out how they can apply all of their techniques and tricks with all of your might. To biology and medicine. So that is this Saturday here. Easier Mac I did not know that, thank you. That's wonderful. 
>> Hello, very nice talk. I want to ask if there is any other competing approaches to function programming in order to analogize‑‑ analyse and harness this special data? 
>> ANIL: So there is OCaml and Coburn 80s and so on and that cloud computing does right now. An approach is basically saying that you specify data flow graphs as a custom file format. They fuse llama or JSON and so on. Anytime you look at them you to go what are the incentives of people pushing the technology? They are pushing to use the cloud. Right? Serving has to look like a resistance problem and you to spend lots of resources and so on. So we specifically moved away from that. Because the entirely of a global satellite pipeline for semi laptop. So is two TB. I have one machine that has 226‑‑ 256 cores and has two TB of RAM and can run run everything as a sale kind of shared memory parallel system or process pay system. So you don't need all of convexity and overheads that these systems ringing. And you can just deploy them locally. So we are really over underwhelmed the state‑of‑the‑art and cloud APIs for service and so on. The functional approach, it just wasn't that much work. A lot of the pieces that we need are already in place. We just know what we didn't want to do which is use existing lock‑in cloud provider APIs. 
>> We are running into the. A bit now. I think let's just have one more question over here. 
>> Hello, thank you for a fantastic talk. I am an undergrad here and there are many others here part of PLMW and the other understood programmes here. And is your slide showed found that we are often some of the loudest voices in these conversation's of conservation. How can we who are inspired by this, immediately get involved in walking the walk? 
>> ANIL: Cambridge has a great graduate programme it's only three years, not five years like the U.S.. We have all these other people that come over, it's a lovely place. But specifically, climate justice and youth justice is one of the most important things that we are doing that we are going to fill the pain. And there's lots of youth justice that is most important that we are doing for generations. And the really hungry for technological inputs so there's actually an internship programme called outreach been used to get underrepresented minorities from all the world to apply for tech programmes. So anyone who wants to apply in a teaching programme, anywhere in the world connection come in and do that. The other part is to give all with open source as well. When you're looking for ten of side projects to learn from. Just try to apply these things to a local problem that you have. So often the way I got into settlers is just trying to analyse various, there's a controversy and keepers about some highways and trees and self. We tried to figure out what that was. And they get into the problem more and more. Start by solving local problems and getting involved with your peers who are competing for actual be puerile change. Augment their actual efforts. And then learn and learn how to use the multiplying actors of technology to solve good positive social issues there. Instead of perhaps less positive uses of technology. 
>> Thank you very much. 
>> Thank you well. 
>> I believe Anil, you will be out and around the rest of the week. So you have time to ask more questions. We resume in this room and the other room and just 17 minutes.
