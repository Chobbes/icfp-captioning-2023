Ready?
Fantastic. 
So I'm excited to welcome you all to our next session of ICFP, this session is about fixpoints, and first talk is by Sven Keidel, presenting work on big step abstract interpreter and is using fixpoint there. 
>>  Thank you very much for having me, this is joint work together with Sebastian Erdweg and Tobias Hombucher, In this work, we described an approach to modularize the implementation of fixpoint algorithms of big-step abstract interpreters, and let's break this down, and start with fixed point algorithms. 
Specifically what are fixpoint algorithms and why do static analysis require them?


When you analyze a program such at this one here, then the result of the analysis can be abstractly described as solution to optimization problem, like this, over a number of inequations derived from the program, and because of language features such as loops and recursion. These inequations can have cyclic dependencies. 
And the task of a fixed point algorithms is to resolve these cycles and find a solution to the optimization problem. Preferably, the most precise solution within the analysis letters. 
And now let's come to the modulariation part, specifically, why is there even a lead to modularize, the implementation of fixpoint algorithms, this comes down to the observation that there is no single fixed point algorithm which performs best in all situation, Specifically, which delivers satisfactory performance and precision for all analysis frameworks or for all analysis within a single framework, or even for all programs within an analyzed language.  And because of this analysis developers often create specialized fixed point algorithms for different use cases. For example the Soot analysis framework, implements 7 different fixed point algorithms for different types of analysis . Two algorithms for distributive analysis, and two algorithms best interest B directional distributive analysis. Two algorithms for field sensitive direction and analysis And one algorithm for context flow and field sensitive distributive analysis. More generally, if the implementation of fixpoint equity items is not modular, then it takes a lot of effort to develop and maintain many different versions of fixpoint algorithms. 
And it's hard to reuse parts of fixpoint algorithms. Furthermore, analysis developers often feel the need to change existing fixpoint algorithms for example to fine tune their precision and performance. 
To see this. Look at the changes, the monthly changes in the number of lines of code for different fixpoint algorithms in the frameworks over the span of multiple years, and you see the fix point algorithms require constant changes to deliver satisfactory performance and precision. 
Now the problem with changing existing fixed point al fixed-point algorithms if the implementation is not modular is that changing the algorithm can introduce performance and precision regressions, for some type of analysis.  Furthermore, changing fixed point algorithms can introduce soundness bugs which question the validity of the analysis results . And they can be found with the rigorous soundness proof, yet, proving soundness of fixed point algorithms is really complicated in maintaining a soundness proof over long periods of time over many changes takes a lot of effort . And often infeasible in practice. And these problems can be addressed and mitigated by modularizing the implementation of fixed point algorithms. 
And so in this work, we describe an approach that modularizes fix point algorithms if big step abstract interpreters. 
So what are these. Big-step abstract interpreters are alternate big step language semantics. 
In their simplest form, they evaluate an expression under an environment of bound variables to a value. But because this is an abstract semantics, we use abstractions for environments in venues such as intervals for numeric values for example. Such an abstract interpreter can be implemented in language like Haskell with a regular recursive function like this example here, now I don't expect you to read or understand all this code, but this interpreter does what you expect. It pattern matches on the expression you want to analyze, and then it calls itself recursively to evaluate subexpressions to abstract values and then afterwards recombines the abstract values. 
And now the problem with the big-step abstract interpreter it does not terminate where in the last case try to evaluate the body of a function. In case of a function call. 
While, nontermation is expected in the case of concrete interpreters. Nontermation is really undesirable in the case of abstract interpreters. Specifically we would like the abstract interpreter to always finish and yield an analysis result deliberately trading off some precision of the result. 
In case of big step abstract interpreters, termation is enforced by the fixpoint algorithm. 
So let's dive deeper into the reasons why big step abstract interpreters diverge to inform us how we have to design the fixpoint algorithm that enforces termation. 
Now in our work, we identified 3 distinct problems that cause big step an extract it is interpreters to diverge, that fixpoint algorithm have to address. The first problem are recurrent recursive calls. 
For example, if you're trying to analyze the recursive factorial function, then you obtain analysis trace like this. This contains a recurrent recursive call, specifically the abstraction interpreter calls itself tries to analyze same expression under the same environment it seen further up the stack, and therefore the trace become infinite and the abstract interpreter diverges, and even if a fixed point algorithm would address this problem, there is second orthogonar problem to this. Infinite recursive call chains may not even have a recurrent call, e, if we analyze the factorial function for negative numbers of minus infinity to minus one. Then we obtain analysis trace, like this, where the argument with a factorial function shrinks and shrinks and shrinks. Those there's an infinite causal chain of recursive calls, but they never repeat with fixpoint algorithm has to address the second problem as well. And yet a third problem, that is not at rest yet, which we have to look at the trace of different function. And for this function, the fixed point algorithm has to solve the recursive equation here, and does this with standard process called fixed point iteration, and applies the equation, and producer intermediate results for example, intervals which grow and grow and grow, and never stop growing, and fixpoint iteration here does not terminate. Because of these 3 reasons, this big-step abstract interpreter in the example does not terminate. So let's try to fix this. Specifically we now use fixpoint algorithm, which solves these three termination problems. To do this we refactor the big-step interpreter from before, and introduce a function fix-that implements the fixed-point.
And I will explain this function how it works in a little bit. Furthermore, we have to replace recursive calls of the abstract interpreter to itself by calls to the function,... , which is the argument to the function which we pass to the function fixed by the fixpoint algorithm. Allows the fixed point algorithm to take control of recursive calls. Lastly we have to change the type of abstract interpreter to allow, the fixed point algorithm to pass along auxiliary data that it needs alongside the inputs and Outputs of the interpreter . So now let's dive deeper into the functionfix file fixpoint algorithm, and see how it works. Fix-F, a regular simple function, which takes us input, the interpreter function which we have seen before in the example, the fixed point combinator encapsulates the core logic of the fixpoint algorithm, and it calls the fix point combinator and interpreter function, alattorneynately, to allow the fix point combinator to take control of recursive calls, and allow it to enforce termination. 


So these fix point combinator are the core contribution of this work, and allow to control different aspects of the fixpoint algorithm, for example the order which analysis results I iterated on. 
If for example, there are multiple nested recursive calls, then the inner most combinator here,  iterates on the inner most calls first until nothing changes anymore, and then moves on to the outer calls. This combinator also solve the first termation problem I talked about before, by cutting offeree current calls and enforce termation. Further more it solves the third termation problem by applying widening operator to the analysis result, to force fix point iteration to terminate. Further more other combinators control how deep the abstract interpreter is allowed to recurse. For example, the context sensitivity combinator here, introduces artificial recurrent calls by joining calls to the same function, if they occur in the same calling context, truncated to length 1. 
Further more, other combinators control what expressions are iterated on.  So before you've seen that the abstract interpreter in this example diverged for expressions which are the body of functions Therefore this filter combinator applies the combinators only if the expression is the body of a function, which is a performance optimization. So now let's circle back to the motivation and let's see how these fixed point combinators allow us to create specialized fixed point algorithms. And here we take a look at bit larger example, of analysis, which applies language with functions and loops. 
And here we can create a combinator expression which applies, different iterations strategies for loops and functions and interleave them seamlessly, and so look at lower expression which applies to the body of the loop. 
So here the unrolling combintator increases the precision of the algorithm by unrolling the first ten iterations of the loop and analyzes them accept rattedly. Further more if they are multiple nested loops, then the outer most combintator f iterates on the outermost loop first. Now let's see how the combintators allow us to make changes to existing fixed point algorithms. Specifically, we can make changes by adding or removing or rearranging combinators, for example, we can increase the precision of the algorithm from before by adding this unfold combinator which always unfolds the 1st 3 recursive calls before calling. The combinator which increases decreases precision to Now we implemented twelve of these fixed point combinators as Now we implemented 12 of these fixed point combinators as part of the sturdy analysis framework. Many of them you have seen we've implemented three combinators which implement the different iterations strategies which describe the order analysis results are iterated on. Two combinators which allow to mix different iterations strategies based on a predicate. And four combinators which affect the recursive depth of the analysis. And first 2 decrease the recursive depth to enforce termation and the latter two regain some recursive depth to increase precision. 
Lastly, we implemented 3 tracing combinators that allow to record important analysis data like, interprocedural control flow graph. Further more we used the fixpoint combinators to implement fixed point algorithms for three different analysis for three different languages. Specifically we've implemented analysis for Stratego program transformation language web assembly a low level language for the browser and Scheme, a function language with imperative features. We use these case studies to show that the combinators are in fact analysis, and language independent and useable across many different analysis use cases. 
Further more the case studies show the combinators are precise and performant enough to yield useful analysis results for real world programs.  So I won't go into all of these case studies, but I want to highlight the web assembly case study. Now we describe this case study in a lot more detail in its own paper which recently got accepted, and invite you to look at it.  In this case study we implemented a constant propagation and that code analysis for web assembly, which can be used to eliminate instructions of the WebAssembly binaries to save space. If the paper we evaluate the performance and precision of the analysis by running the analysis on 1458 WebAssembly binaries of development bench benchmark suite. The left graph shows the running time of the analysis where we set a timeout for each individual analysis for each individual program to 60 seconds to keep the running time of the entire experiment, within a reasonable timeframe. And for this we obtained an average running time of five seconds, right graph shows the precision of the analysis, namely, the percentage of instructions and can be eliminated by the WebAssembly binaries, and for this our analysis was able to eliminate 20% of instructions on average, which is more than the industry-leading tool binary, that was only able to eliminate 9% of the instructions. 
And so this case study shows the combinators are precise and performant enough to yield useful results for real world programs. 
And lastly we developed a soundness theory for the combinators, specifically we showed each of the fixpoint combinators  be proved in sound once and for all. And this means that analysis developers don't have to worry about soundness of the fixpoint algorithm as long as they reuse sound combinators. Furthermore, we used and applied this theory to prove the 12 combinator sound that we have implemented as part of this framework, and with this, let me conclude the talk by reiterating the contribution of this work.  We implement we describing approach to modularize the implementation of fixed point algorithms for big step abstract   implementation of fixed point algorithms for big step abstract interpreters by the means of fixed point combinators. We've implemented a library of 12 different fixed point combinators and implemented them as part of the sturdy analysis framework. Algorithms for analysis for three different languages to show that are resusable and practical .  Lastly we developed a theory for soundness of these fixed point  combinators which allows us to prove combinator sound, once and for all. 
With this thank you very much for your attention, and ready to take your questions. 
>>  Thank you very much, we have plenty of time for questions. And we have microphone over here, and also possible to ask questions on Discord and I'll be checking that. 
Do you have a question?


>>  Yes [off mic]. 




>>  So you should have said the approach works by applying the function and operator alternately, and apply the function once, and then apply the combinator, and does this introduce overhead, and easier way to eliminate the overhead. 
>>  So every time you modularize something there is potential to introduce some overhead, and the way we mitigated this, is essentially by inlining everything, we inline the implementation of combinators, g. Everything, so we inline the implementation ofz and we inline lot of the modularization that is part of the abstract interpreter, and then we inspected the low  level code that is generated for the interpreter and this is very  close to and written interpreter that you could have written without all of the modularization on top of ... I'm Peyton Jones, | work for Epic Games. So in a, in a compiler for Haskell, we have a demand analyzer that works a very similar way that you described. But one of the problems that finally fixed points we have those if you've got a function and in the body of that function or a recursive function, and in the body of that function is an is a nested recursive function. Then, for each iteration of the outer fixpoint you tend to have to see look at the inner function, we really going to fixpoint all of that.  T. So, and even if you do, then maybe for the next iteration of the outer function, you might start from the point you got to on the inner one, rather than starting from scratch. We found that ... that's quite a complicated thing to express, can your combinators express all that interaction too. 
>>  Yes. 
>>  Wow, great. 
>>  The combinator does something like this. We seen in the the iterates on the innermost calls and then on outermost calls. And the Fibonacci example that we give in the paper, that it actually first iterates on the innermost calls and then on outermost calls. And ... I'm not talking about inmost calls, and outermost calls, function fthat has a nested recursive function 9, right, when I'm going to fixpoint en F but in each fixed point iteration, I've got to look and find fix point, G, and having two calls, and do you see the difference, the combinator handled this part, they alternate between fixing the combinator of the inner function and outer function. 
>>  Okay. 
>>  It depends on which combinator you use.  E innermost combinator does that the outermost combinator skips the iterations of the inner calls. 
>>  Its great to hear you can do that, very impressive .
>>  , it really depends what kind of performance you get from that. So in our experiments we have seen that some of these integration strategies like the innermost iteration strategy works good for some programs, and worse for others, and reinforces the problem we're trying to solving, there is no one single best fixed point algorithm to rule them all. 


>>  I guess you heard about the fixed point algorithm that is very efficient, it tracks dependencies between the different calls, and functional F, and so avoids unnecessary calls to F, and saves a lot of time, and practice to run the analysis, and so that's one of the fixpoint combinators that also tracts the dependencies to avoid unnecessary computations. 
>>  What dependencies. 
>>  Dependencies between different calls, of functional F, that is best to the fixed point solver. 
>>  Yes, so some combinaors track dependencies between analysis, result, specifically the inner most and outer most combinators, and they track recurrent calls, and depend on other recurrent calls, and these allows combinator to decide how much it needs to recurse and before cutting offerecursion, and where to iterate, so this combinator does the dependancy management you mentioned. 
>>  Great. 
>>  We have not considered narrowing, yet, we only considered widening, and so, to give some context, widening, computes an upper bound of the analysis result with the previous result, and ensures that an ascending chain of the results terminates. 
Narrowing is the opposite concept, where you start from a call, and you create smaller and smaller calls to improve precision, but we have not considered this case yet. 
Over there. 


>>  So it's not of use for your examples. If evaluator is written, in CPS style, and branch is matching and calling back to something that looks like a fixed point, and most of the time it's not, how would you express that in the fixed point. Most of the calls are not recursive calls. 
>>  Sorry, I think I don't understand your question. What do you mean with "CPS style". So if the interpreter not in the language of style... everything going to next instruction, and continuation of the interpreter itself. 
And so you are going back to the fixed point combinator in that case, I suppose. Is that going to the fix point places and just continue, and not have abstract. So the next step abstract interpreters that we have implemented are not implemented in cps style, but in direct style, where you recursively, call the abstract interpreter, and have not considered other styles, of abstract interpreters and would describe the CPS style, so I have not thought about this, so let's discuss this afterwards. 


>>  Let's thank Sven again. 
