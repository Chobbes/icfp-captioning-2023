>>  All right, next talk, is by Joachim Breitner, and he's going to be presenting Functional Pearl, a talk on more fixpoint! Appropriate for the second talk, thank you. 
>>  I like Haskell. Very bold thing to say in this crowd. You might be wondering why Haskel, and many reasons could have been one of them, and I like Purity.  Everything is nicely positioned now. And you can do this equational reasoning equational reasoning particulars, I think amazing things to to do deal with... compared to... they can't do it.  But what does purity in mean, actually, like, what is the essence of purity was actually hard to find a formal definition that kind of describes purity, but even informally, to boil down to what purity is. And I came up with this. I'm sure you all recognize the various funds, left code fun, and right one, math font, and purity means equations are equalities. You write down equations in the program, and left and right are equal, and that's what unlocks power of equational reasoning and treating this like math. 
And Haskell is great because it's a pure language, and also because it's a lazy language. 
I like laziness, there is many reasons to like laziness. Obviously the thing about unused expression not being evaluated. It's clear, you probably heard point out, laziness, keeps the language honest, nonlazy language, you might be more tempted to give up purity, and because I like purity, and laziness. And more reasons I like laziness, and the one I want to talk about here is, laziness, allows us to have recursive definitions. You can't have recursive definitions in nonlase language. Let me give an example for something mutually recursive, for example, odd and even, and defined in terms of each other, and how it might look like, and we have mutual recursion. 
And I sense a little confusion in the audience, because this doesn't quite look like Haskell even with all the extensions. 
This is C code for those who haven't seen it before. 
And a little bit tongue and cheek. I claim that C is a lazy language, because you can have recursive definitions and write the top definition there, and can make sense oit, even though you haven't defined everything has been composed of, and actually only matters what you put on the right hand side at some point later. Which in a way laziness, or at least nonstrictness, or if you are pedantic but for now, let's call it laziness. Laziness is a spectrum. So you have languages like C, where some things are lazy like the functional definitions pretty hard to imagine a language where functional aren't lazy, and more things are lazy, data structures are lase, and functions are lazy, and more things with recursive equations. And can do things... something using itself, where the whole thing works and makes sense; the prominent example is Fibonacci numbers, as infinite stream, and I want you take you along and look at this concrete, relatively practical application of this knot tieing in Haskell and so I have represented here, interinjures of their success, and define the reflexive—transitive—closure of the graph so for each node. One other nodes can I reach. Soy solve this writing local definition, and map each vertex to the set of vertices and can read from their vertex and I basically write specification of the equation, and so all the things reachable from nodes that are adjacent to it. 
And this is a recursive definition, it can only work because we have laziness, and let's see if it actually works. 
And so I load this code in, and I run the function and simple graph, 3 nodes, and it returns a graph. 
And hard to see if this is really the reflexive-trantive closure. And put on the slide, and this is the kind of program I use to try and impress programmers that haven't experienced the beauty of Haskell yet, I think this is great, declarative programming at it's best, unless the person I'm trying to impress, is clever is asks me to add one little edge to the graph. 
And then, yeah, I have to do this... so go to my code, and add this and already fearing what will happen. It doesn't work. 
So it does something, it prints a little bit. But doesn't calculate the result. 


And, that is disappointing. 
I have a nice declarative specification of the problem. Describes the solution I want, why do we not get it. And let's see where it goes wrong, and so you have the map structure, and using laziness, and knot tieing, to understand the problem, back to Purtiy and equational reasoning, and we start unrolling these things and see what is happening.  So this is the expression that failed, and the function, and I'll do these equations steps l'II fast forward to the interesting point so we inline the function the map function over the data structures, if function of the values, and name the values, The inline the function f, and there's a list comprehension of unknown list, and we're here. And now something interesting happenings. We have not fully evaluated the program yet, but know enough about the map reaches, that you can look up into it. 
We don't know what S1, S2, S3 are going to be. But we know looking up 2 in the map will give set as 2. So we can resolve the lookups at the bottom, and now something magic happens, the recursive definition has disappeared. We tied the knot and pulled on the strings and get straight line code. I like this analogy. So the use of lazy map in knot tieing way was not the problem that made this not work. The problem is still here. It's the recursive definition of these sets. 
And, I said earlier you need laziness, in recursive definitions the problem why this doesn't work is this is nod laziness, and indeed, we look at data structure, of API... and maybe in the dock somewhere. We see these functions are too strict, and the set library in Haskell and they need to know the argument completely before they can do something. Not lazy, and can't use in the recursive equation, and that makes me sad. And wondering could it be this system works, could we write the API in a lazy way, so recursive definition. 
And I believe you can not write normal standard Haskell. But I'm not using normal HASKELL. 
And it it has all these things going on. And unsafePerformIO, you can do anything you want, just be careful not to beak things, and so there is a library, and has pretty much the same API, and loads of function to convert back to the normal sets, and these operations are lazy. 
And you can try. 
So... yeah, one slide more, and this is the code you wrote before, and the API was essentially the same, and so plug in the different types and different functions, and project on the end. 
And try this. 


And so still running, oh my god. 
So let's load this other. The code from the slide you just saw, and of course the A-graph works like before, and I can run the cyclic graph and it works. And always anticlimactic you know something is there. But hey! It worked! 
People were wrinkling their forehead, when I said unsafePerformIO. And dangerous territory, and can I really do that with the Haskell and the question is, is this still pure, I think it is, and I think there is very simple argument, why it's still pure, and boils down to what I said earlier, purity means equation are equalities, with my library, you write an equation like the top one, taken from the code we just saw, it will do something, but at the end it will assign values to the variables, so this equation becomes an equality for the finite sets, and so this means... it's still pure, and this is very short argument, why in this case; it's okay what we're doing, doesn't really matter how we're doing it. And so the effect it has on the language... which is okay. 
Of course, sets are particular nice example for the main really can do this. Because with sets and monotone functions are the least unique solution, we wanted, and the API, and always get one, and always computable, and this works really nicely, and the broader take away, is that, I think there are many examples where this should work. 
And so when you have like in a language like Haskell that likes to be declarative, and pure and lazy. I think whenever you have a domain you have the property, where finite set of equations has unique most desirable solution, and you have way of computing it maybe even, that should work as equations, and to write on the equation, and get them, that's a very, shallow DSL so to say. 
And this means we can look at the spectrum of laziness, and go further than the Haskell we know. So the library, you could think of language extension, everything in unsafePerformIO is a language extension. 
And there is also sets and Booleans you can use with recursive equations and more domains that are quite obvious, and some more tricky, and more interesting, and can explore, we have the nice parser combinators, and they they have this property that you write equations and they describe a solution, but they tend to fall apart if you use left recursion and doesn't have to be that way, at least not from an entitled user point of you video. And I think there is something to explore where you can gain something. And so the first thing to gain of course is we can now solve more problems using the nice declarative pure style we saw the graphs. Previously could only do for a-cyclic graphs, and now we can do for cyclic graphs as well. And this declarative thing is important, if you solve the problem for f reflexive-transitive-closure forum cyclic graphs and non Haskell. If you do it nicely, you're starting to mix declaration of your solution and the solution , and have you to probably some local function, and iterates... and keep track of what you have seen, you want to keep it separate, if you like abstraction, if you don't like abstraction... you wouldn't be using Haskell anyway.  Declare the monadic style and then query the results and that works, but I always feel sad when I have a nice pure function and I have to for some reason I'll make it monetic syntax heavier and you..  n operators that then suddenly evidence led to protests well, it works really great for like a single recursive equation, which is putting fixed that's fine, and gets more complicated. If you have multiple equations you have to pull them occupy, and number of equations and values, and not dedicated known, you have to deal with maps, if you want to handle heterogeneous equations. So set of equations where some equations are unsets and some are in booleans and but they are connected and general fixed point combinator for that seems to be an interesting puzzle to come up with. And I think it really breaks down when you set of equations collecting and pass them on to combinator when this happens not in one local lexicographic space. So imagine you're traversing a syntax tree in a program language, what we do every day, and every node, and new equations and define new variables you want to solve for them, and want to solve the whole thing together. 
Then this mismatch between the structure of the code that creates the equations and the structure of the equations itself... really becomes tedious, maybe one of the best way of expressing this the benefit from the approach, is we do have a fixed point combinator Haskell to define recursive functions. And we're not using to define recursive function, we're using top level bindings, because it's most convenient way to describe set of recursive equations so why only for Haskell functions and why not other domains as well. So why we should look into the style of programming. There is reasons, and caveats I should point out as well. Not always applicable, you really need this property for given set of equations you have, or need more desirable solution, so you preserve the purity, wouldn't want to give up purity for notational convenience. And I didn't say much about the implementation, but relies heavily on sharing, so you need the property that you define finite number of equations, and not intently infinite number of equations you never see the end of when trying to solve them. 
So in this way, working with the library, is just about as mind-bending as working with other knot tieing tricks, like the ones you might use for r memoization or dynamic programming. So it's not completely easy, but it's doable. It's something that people tend to at least enjoy writing books about. I don't know how much you want do in production, because termination now becomes critical for the whole thing to work. We 0 change the equational theory of Haskell. Now, as I stand by my statement that it's still pure, but there are contextual equivalents of Haskell, beyond purity, example, lambda lifting is not observable in Haskell normally. But the library... breaks, ambda lifting is already a somewhat dangerous transformation because it can dramatically, meaning asymptotically changed the performance of your program.if you lose sharing, and assuring to me, because the compile will not just do that willy nilly but it's one of the problems certainly with the approach. 
Right, so this was the talk variant of this thing, and paper is rather different, and let me advertise the paper a little bit. And large use case, and from the previous talk I guess, I wanted to write the program analysis, without having to encode where I do the fix pointing and all these things, and of course how do I do the unsafePerformIO thing. I think there is neat idea, where I'm using... identity implicitly as the identity for notes in my graph.  Itjust becomes this graph on where the values propagate and this works rather nicely. Once you have unface-to-face performance, very lot about reentrancy problems. So even if you have a basically means you don't have necessarily have threats, but you serve all the problems of concurrent programming, and need Todd be addressed and space leaks become an issue, and can't do all the equations, let X equal X, is annoying, because there is no function involved, where I can do magic, using unsafe. And so and notjust put it in a new programming language , like related work like data one which would be the closest inspiration I guess . It looked like semantics for the whole thing, has call has the nice annotations for semantic, and fix point finding on sets, has nice semantics and somehow come we nations is rather tricky, and some noncontinuity happening, and so work in progress, and have a draft I can share, and very happy to share with people who really think it would be easy and tell me how to do it. Of course, and library factors you can play around it, and just put on the main take aways of the talk and looking forward to your questions. 


>>  Thank you very much, we have lots of time for questions, both here and online. 


>>  Yeah, that's what I get for being overtime every time I practice, and speeding up every time, and of course... plenty of time, but looking forward to the discussion. 
>>  Thanks. Can you draw any similarities to Plotkins ... that let's you evaluate two things, so divide the first one, and second one, gives you the answer first, and so... 
>>  Is the question whether we had parallel, or could do without using... 
>>  That doesn't change the digital semantics. 
>>  That's interesting... so I guess the reason why you... hmm not sure. Because the reason you can't do this in plain Haskell naively, you can find a value is in a set, imagine you do the sets like lasy lists, and infinite Val in the list, you can look it up, and find finite time, and as you keep doing the looping, you never know something is not in there. 
And so I don't... I'm not sure the parallel thing... 
>>  You know it's not the only... value telling you it's not there. But if you keep computing something you loop, but keep looping anyway. 
>>  That's the point I can write equation, if try to implement list it wouldn't loop. 
>>  But you end up in the same place, so maybe offline... 
>>  Yeah, might be interesting, thanks. 


>>  Okay, we have a question from online. 


>>  Michael Adams asks about left recursion. Is that something can you do today in your system?
Or is that future work, and how far future?


>>  That is future work. I haven't tried it seriously yet. 
And so, I don't know how far. It shouldn't be hard, but how can I say that when I haven't tried it?
Famous last words. 
>>  Yes, definitely. 
>>  And another question here, and please introduce yourself. 
>>  This is E Hong and have some... for example, if I want to do this with my own framework, do I start from scratch, is there principal way from the framework, to support extension on other lattices. 
>>  The library itself is slightly modular. It takes an imperative propagator network, and imperative library, that can be in IO, and you can declare variables, and declare equations and ask for solutions. 
If you have that, no matter how you implement it. You can use the generic unsafePerformIO that you can wrap up in pure API As long as your library has certain properties, which I haven't spelled out completely, but it's it's roughly what I said and we're gonna set of equations, needs to have a unique solution no matter which order you declare things, and no matter which order you clear things. 
>>  So result of generic framework. 
>>  Yes, library at the moment, supports recursive sets, and recursive Booleans, it has a different implementation for solving them for the Boolean that uses something smart over you. Only propagate when something goes from false to true sets it's rather if could be Could be a smart implementation, and you add your own like solvers into this mix. .
>>  So this thing on the slide here is very attractive right, as a programming story, but somehow there must be a bit more than that. Like, don't you have to have some notion of monofonic thing, and ascending... if I just write, you know, let X eSQLs, X times, X, well, that has maybe one solution, if X is type Nat but probably not going to find it. I could write solution to the polynomila equation, so as programmer how do I know what the boundary, and what is possible, and do I need to be some type class, or what. 
>>  Yes, indeed, what I tried to do the implementation. So I keep separate the idea I have domain theory to solve, and wrap it up purely, so overall statement whenever you have domain with equations and solutions you can solve them, and you can also have pure API, and now ask for natural numbers and addition. So you add something new to the domain, you have to design, and choose the API so the property holds, so for set V, we have only have the functions where they are monotone, and the construction we have solution. 
And so if you define recursively, definable natural numbers, ordered by normal order you have addition, and multiplication, and wouldn't have subtraction, but it will still work. It's true, not going to be the case, everything you can write number of the equation, and have solution. 
You get smaller API that's that guarantee you have a solution. 


>>  So my question is still as a programmer, how do I know whether I can use your technique, I have your library, does your library, provide a fixed set of domains, and that's it. So if I want a different domain, I'm out of luck, I need to aplay to you for a new domain. 
>>  It depends on which program you are on. If you are the end user who wants pure API and want to do these things, yes, you get fixed API, and forget the name already, the person who asked about extending it. Then you can use the components of my library, to extend it and provide more API's, but some safety conditions you have to do to ensure yourself, when it's not completely... yeah, you have to do something to make sure it still has the properties. 
Okay, we have time for maybe one more question. 
Yeah. Can you go back to the slide where you have the... unique solution. Yeah. 
Yeah. Could you weaken a little bit, to have unique least fixpoint, sometimes when I was reading it, I snuck in the term, unique most preferred solution. 
>>  Yeah. 
>>  So of course, if I write something like: Let s =... [writing]. 
And this is one of the recursive equations... I can make it bigger, sorry. 
And of course this equation has many solutions, any set that happens to have 2023 in it is a solution. But usually we don't want these. We usually have notion of what's most preferred, and so lattice, is hopefully the least one that you can solve for it. Said most preferred, because this is not restricted fix point analysis, you can have solver that does something different, like linear programming in convex domains or whatever, as long as your set of equations.c there's a unique solution, you will get no matter how you set up then I think this technique applies . 
>>  And so not quite sure how you exactly implemented it. And kind of least fixed point, and not doing anything beyond that right. 
>>  Booleans, always find it, and API... not showing slides... the API for sets that we have is in a way, there is going to be solutions and going to be represented as finite sets, and you can find it, and open question, we want to allow set.map, it's interesting, it's monotone function, and you will have least solution in terms of sets. 
But not necessarily of finite sets, in other words, if I were to add set.map to the API I provide could write equation, that don't... not productive, they will not be productive always so it's still pure so and that's why I think it might be okay. It has the programs are used to non— termination and. Let it kept it out for now because nobody asked for it. But there's a wiggle room, and where not every equation has a solution, or a solution, and nontermation on top. 
>>  Okay, thank you. 
>>  Thank you. That concludes the session, and we have half an hour break, and then returning in the grandballroom, reports from various contests, thank you all.
