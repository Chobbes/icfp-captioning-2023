>>  All right, we don't see anything on the screen.  
Okay.  Welcome to the next session, and in the first talk here, with we're going to hear about how to make cons free programming not cons free. 
>>  That's right, so welcome Siddharth Bhaskar. 
>>  Thank you very much.  
>>  Thanks everyone, my name is Siddharth. And at James Madison University in Virginia, and talking about some work I did post-doc at University of Copenhagen. With Jakob Grue Simonsen. His work combines sort of programming languages and computational complexity. I am neither a programming languages...So my background is in mathematical logic.
I will be talking about programming languages without knowing anything about them which should be apparent almost immediately, but we'll see how it goes. And read write factorization, what does it mean? So I'm going to define simple first order language over strings.
So let's fix an alphabet sigma by an alphabet, I just mean a finite set of size at least two and mysteries instead of financing the strings, and that's the only type that this language has except for the set of Booleans, which we call two. And you're going to consider the following set of string primitives. Okay, so you have head and tail which decompose a string into its first character in the remainder. Nil, which is a constant naming the empty string and null, and test, whether the string is empty, and cons which basically smushes a character onto the front of the string. And how do we build a language from that? Well, I'm going to form terms from basically recursive calls, and calls to the primitives, the string primitives which are those and conditionals if then else statements, and then a program is just a list of simultaneous recursive equations which we can allow with at least fixed point semantics or whatever we like, and this is the kind of program I mean, I've seen concatenation written on the screens many times today, and so here is another one, and so to concatenate a list, you test whether the first string is empty, and if not. You concatenate the tail of the first, with the list and smash the head onto the front right and I've also seen reverse several times on these screens and here it is again, to reverse. And concatenate on the reverse side with the head, and this is certainly, very plain Turing complete programming language. And I want to remark that running time that is like, the time native to this language is basically identical, I think to time on a deterministic Turing machine which is to say we can compile each model to the other in a way basically preserve time. So now I want to say what I mean by a cons free program, Okay. So amongst these string primitives, head, tail, and null are destructors. Right they use them to decompose the string and nil and cons are constructors. So we omit the constructors to get contrary programs, and obviously this is no longer a Turing complete language and it seems like a very weak one. Right? What can we even do here? And IT'S sort of a by now classic theorem that cons free programs, If we're looking at programs which compute relations so functions of type string to Boolean. 
These compute exactly the class of polynomial-time relations. So in other words, every polynomial-time relation is computable by a cons program, of the appropriate type, and every cons program of the appropriate type computes a polynomial-time relation. And we restrict ourselves to the tail-recursive fragment of this language, every recursive call. In the tail position. Then we capture logarithmic space in exactly the same way.
And, moreover, this was sort of extended by higher types into a correspondence, which captured levels in the deterministic exponential hierarchy. So example, 2X time, you are looking at finite towers of exponentials, and their union is the class of elementary functions, that's if you have unbounded order. So, from this whole program, as you can see it's sort of a useful framework for capturing a number of robust complex deterministic complexity classes of interest. And there's a number of outstanding questions, two of which what is cons free running time. So the running time of the original Turing complete language basically corresponds to Turing machine running time but when we remove cons, we sort of severely lessen its power, in the sense that yes we can compute every P-time thing. On the program, and the program may itself run in exponential time. And then the other thing is how do we capture functional complexity classes so not just relational but functional. So obviously without constructors we can't even capture the simplest of functions that increase the input, these are the two questions, which I basically solved in my post-doc. And the current paper, or the present talk is about the second question: But the first question is a previous paper, and it's really part of one story, so I do want to say this, which is very interesting... cons free running time is the log of Boolean circuit depth, so we remove cons from the program language, and actually recover another classic measure of complexity, I think is very interesting and unexpected.
And so that means, also joined with Jakob Grue Simonsen, and Cynthia Kop, And this means that we have characterizations of complexity classes which are intermediate between. As a time class of concrete programs. OK. So we're gonna come back to this result at the end of the talk. But for now, let's focus on the second question how to accommodate polynomial-time functions in the framework, so obviously we have to do some sort of construction, and how do we do in a limited way, cons free programs, we can think of one type of data besides Booleans, which is read-only strings, so the basic idea is to augment programs with a type of write-only strings, strings you can only construct. You can't have strings you both construct and destruct, but with a typing system we can partition strings into construct only, and write only, and read only, and so once we do that we got the following typing of the string primitives, so head, tail, and null operate over R strings, and nil is a string of type W, and empty string, and cons works on write-only strings type W, so we have a typing discipline, and impose on programs, and string program, is factorizable, recursive function symbols can be consistently typed in this way. So if we look at the previous two examples of concatenation and reverse. We can see that they can in fact be typed this way, right, so for concatenation IT'S important that its first variable is a read-only variable, which we destruct, and the second variable, is a read-only variable we construct. Reverse function is R to W, and if we go through it tactically, you can check the typing is correct. 
And so now here is an example of a function, or program which is not consistently typeable, it basically takes so here one is just a character, it's a numeral, not the number one.  And it takes in a string.  The output is a string of two to the length of input 1's, and so take string of length N and return string 2 to the N1's, and basically just a standard definition of the exponential function by near exponential just hidden a bit, and you can see not consistently typeable, because we feed the same term into the both arguments of concatenation typed differently.  
Okay, so one might expect then that we're done.  RQ factorizable program of the type exactly capture polynomial type functions unfortunately not quite correct, here is interesting program I call leap.   Hard to describe exactly what it does. 
Intuitively, because there's a nested recursion going on here right. The second recursive call is for the second line is....   I would have a board and work some small examples out but if you do so, you'll actually see that this function  computes exactly the same thing as before, but IT'S eminently RW vectorizable right you can absolutely type it and one of the W argument here in red, and that's a typo.  Should be in blue.  
And this is clearly not a polynomial type function, because the output is too large, turns out if we eliminate nests recursive calls, we get exactly what we want, so nonnested recursive, polynomial time functions exactly captureded by nonnested RW factor support programs of this type.  And more over, if we restrict to the tail recursive fragment, we recover talkspace functions in exactly the same way. Here we don't need to say non nested because the tail recursive property already covers that for us. And you might wonder why the original theorem of Neil Jones didn't have the nonnested assumption in because in cons free programs we can actually eliminate nested recursion.  And so I wanted to say something about the proof.  
But I sort of want to say a little bit first, a little bit of aside why I think this is robust idea.  And goes beyond this particular language.  Obviously you can talk about R factorizability for programs over any type of data which can be whose primitives can be partitioned into constructors and and dedestructors, and think about in generally algorithmic paradigm.  I think it's instructive to think how the concept can be used to maybe, give a taxonomy to various algorithms, and so insertion and selection sort are vaguely RW factorizable, and I say vaguely, because they if you think hard enough, they are not.  And insertion isn't, according to the type of discipline, I just talked about.  If you don't think about it too hard, they basically destruct the input, and construct some sorted output, and so I think okay, they should be in some sense.  So maybe my definition isn't quite general enough or something, but they should be.  And now let's thing about hsort.  Is not RW factorizable.  And we have the datum, the happy, and which were both constructing and the striping. 
However, we can express it as a composition of two RW vectorizable procedures again vaguely RW vectorizable procedure, in one, we have a list, and destruct and while construct the happy and next part we dot opposite, have a happy and deconstruct it and reconstruct into a sorted list.  
And so it can be expressed as composition of two things, and now let's think about merge sort.  And now just the merge operation definitely is definitely literally rw factorizable. We are destructing to read only lists, and we construct an output list. But merge sort itself is not.  And not in a very strong sense, because there is no way, here you see the input type and out put type of the sorting function has to agree with -- no the out put type of M, has to agree with the input and out put type of merge, and I think that if you -- I think that sort of construction and deconstruction are hopelessly entanned in merge sort.  And I want to air this out.  Is this a profitable way to think about algorithms?
And so that's one deeper question.  And let's leave that aside for the following time.  
And talk a little bit about the proof.  In the proof of original correspondence, a crucial part of machinery was counting module, so counting module is basically dependent type, initial segment of national numbers, which is bounded by some polynomial and length of input.  So depends on the length of the input.  And this is very useful trying to simulate P time Turing machines because the modules can be used to measure running time or length of work tape or whatever.  And the nice thing, if you extend cons free programs by counting modules it's conservative extension.  I don't know what the correct PL term is by conservative extension, maybe you can compute functions of more types, but, of the functions that you can compute, like, if you are looking at functions of the same type as ground program, you are not extensionably computing more things, you can eliminate them... you can accounting module can be eliminated in favour of fix with tuple read-only strings and idea is very simple, if I have input of length N and want to get numbers up to N cubed, then what do I do.  Just represent as triple of numbers less than or equal to N, 3 didn't number base N and number digital based N could be encoded by suffix of input of string, which you can get by taking tail.  Okay.  
And so, we're going to use these counting modules, kneel in the his originalal paper used counting modules and didn't formulate them as dependentent type explicitl.  And actually formulating as dependent type is explicitly is very useful because we can now extend his original theorem by additional types so basically by one star mean natural numbers, unary strings and function of type sigma star times one star to Booleans and sigma star one star, and other types of involving one star, computable in polynomial time, if and only if they're computable by con free programs with counting modules. And the idea here is that the counting modules handle unary strings in one star, and they're essentially no new ideas here , just modify the proof from before, to handle the additional types with the additional type that we have.  
And so what does that have to do with computing functions from strings to strings?
Right, because that's what we really care about.  And so, if I have a functions from strings to strings, I can sort of decompose it into the two functions the bits function and length function.  The bits function over here, sort of tells me... like, I have my string, and you should think about this as index, and this returns the character at that index.  
The length function takes a string and returns it's length.  From a function from string to strings, I can define this and vice versa, if I have this I can define the other thing, and observation is F computable by polynomial time if the bits and length function are.  Immediately gives us characterizization of function is computable, polynomial time, if concrete plus content module programs if compute it's bits length function respectively, and this is immediate from miner extension of Neil's original result.  
And all this holds respectively for logarithmic space and tail recursion.  Okay, so how do we show capturing by the RW factorizable programs  simply we provide a compilation in both direction between RW vectorizable programs and pairs of hands free programs with counting modules Bit/length computing  -- the bits and length of original function, and here is just one tiny part of one of those program transformation.  If I'm computing the identity function right reproduced over here. So the identity function is a recursively compute the identity of the tail and then I smash the head in front of it.  And then, I can compute the length of the identity function in the following way.  Just 0 if the string is empty, other wise, one plus the length of identity function of tail of X, and you can sort of see how you get the bottom one from the top one via program transformation, and the structure is similar, of course more complicated to do the bits function, and it's, you also have to go in the other direction, and that's just a hint of what is involved.  
By the way, I should remark here that you do have to do something like this to get an RW factorizable identity function, the trivial program idea of X equals X is not capable of type W... ou have to deconstruct the input and construct the output, which shows you information can flow from our types to W types, but you have to do a little bit of work to do so.  And can't go in reverse, and now I want to talk about compositionality, and programs are not naturally compositional, because R and W are not the same type.  So what if you wanted to  sequentially compose them. 
And I think that this is a feature because if we remember heapsort, composition of two RW vectorizable procedures but IT'S not in and of itself RW vectorizable and that captures something about intentionality, intention meaning of happysort, we don't want the whole thing, factorizable.  And bit length programs are however loosely compositional in the sense that we're basically using same information, and taking bits of the input using the head function, for example, and we're defining the bits of the output and using the same type of things for the input and output, and means there is relatively simple program transformation, composing these guys, pairs of bit/length programs and can lift that to something, we can lift that to something composing RW factorizable programs.  
And I want to say, some consequences, and I'll move a little bit more quickly here.  One is... because you have these programs synthesizing functions like we can compose them efficiently, we can actually show the programs satisfy the Kleene second recursion theorem.  It's kind of known that Kleene second recursion theorem follows from the existence of various programs synthesizing things, but don't know it's been studied for subrecursionive program language before, so some nice things, and I want to end this talk by talking about some big questions.  
So first of all, I want to dedicate this talk to Neil Jones who passed away suddenly earlier this year.  Neil grew up basically convince dent with the physical computer, born 1941.  And started programming as under graduate in southern Illinois university in the early 60's.  If not the late 50's.  And always thought complexity and PL's should talk to each other, and if you knew anything about Neil, very allergic to doing anything pigeon holed.  So here's a bunch of blended complexity PL questions which I think are very interesting. In the interest of time I think I'll just present one, not this one, but this one.  How many programming languages are there?
Well, just one in the sense that there is a single indentions of partial recursive functions up to recursive isomorphism.  Compile any Turing complete programming language into each other but how many languages are there which capture p time,   I just presented one but there's a host of other implicit, so called implicit characterizations of polynomial time, are they the same obfuscated a little or are they genuinely different.  And so how many index things are there, up to polynomial time isomorphism. So 34 choices:   Countably many are continuing many, and I have no idea sort of which world we're living in.  It even depends on complexity theoretic hypotheses. So this is something I'm very much interested in. I that blends PLN complexity theoretic characterizations. There's a couple other big questions, and that I'd be happy to talk about offline but maybe I'll stop here. Thanks very much.  
>>  All right, do we have any questions?

>>  Remember to state your name and affiliation.  Felix.  I'm not 100% familiar with the work you show, very district question about the detail you show.  The mention bit about property being logarithmic and length of the Boolean circuit.   I'm trying to figure out is the intuition there that the program is going to be the ideal programming that's ... evaluating and parallel. 
I'm trying to figure out how you look at the length okay so half way down, run parallel.  And so the intuition starts...  is a bigger one circuit that is the lower one okay, what we actually do is instead of using circuits, we compile, we compile them to opsPDAs, and the depth of the stack, or the running time... is the resource which captures cons free time, and that is something known to as log circuit.  I said circuit because I thought that would be more familiar measure. 
>>  Thanks very much. 
>>  Neil P university of Cambridge.  Thanks for a very interesting talk.  
One question I had was the type system you imposed on the programs was quite simple, and I wondered if you had thought about perhaps if you used some restrictions like license linearity if you were able to intertwinningal read and write a little bit more, and mostly due to my ignorance.  And many of the other characterizization, many implicit characterization have to do with license airty, and wonder if somehow encoded here, and so love to learn more.  Thanks. 
>>  All right, let's thank our speaker again.  Thank you very much.
