                >> ARNAUD SPIWACK: Okay I think I will get started pay welcome to the Compilation session of the talks and there are channels for each of the thread sorry for each of the talks. You can look at them. That's not all in order. Just search for the ICFP and then the talk. And you will find them. So the first one we have is Patrick Bahr.
                >>  PATRICK BAHR: We have the programme only was, the syntax is given by the algebraic type like this. Here for a very simple expression language. And the semantics is given by the interpreter for the language. Which in this case is simply evaluating the simple expression language. Right. And so what we want to get out of the situation is a compiler. And we want to get to this compiler by some kind of mechanical process. And only the compiler itself but we also want to have some kind of assurance of the compiler is actually right. So out of this process we also want to have a proof of the compiler specification. That says something like this. So if it is compiled the expression E into code and then we run that code on the virtual machine. The results should be the same if we just evaluate the expression out right. So that's what you want to get. So how does the calculation get us to that situation?
                 So the approach is to simply start by improving this compiler spec over here. Just try to prove the compiler spec even before we have the compiler itself. And then as we do the calculation using the equation reasoning, the definition of the compiler will fall all of that. And that might seem a magical. But I will show you in a bit of how this works. So stay tuned. So will and what we did last year, at ICFP is showing that this also extends to semantics. The semantics of our programme languages is given in terms of the monad that is able to also capture not determine is him and non-termination and ordered for our spec now has to replace you quality for a bisimulation relation down here. So the whole machine resources before. And to define the semantics of the concurrent language and then to get a compiler out of this calculation. We also need to get some form of the nice algebraic properties that allow us this compositional reasoning. That is the goal for this presentation. So to get more into detail about what we need. There is two things. The first thing is that we need monad to express concurrency. So what do we mean by that? Need some kind of parallel composition operator. That takes two computations and composes them in a parallel fashion. We need some kind of observable effects that we can actually observe that something happens concurrently. And we need some form of communication. So that the parallel communication and parallel threats can communicate with each other. And then secondly, we need a nice reasoning principles for our calculation approach. So we nice, comprehensive algebraic properties that we allow this.
                 So what we do in this paper is to construct such a monad. Based on recent work on choice trees. We add two things to the choice trees. First of all, we do a very small refinement of the syntax and the semantics of choice trees that gives us a slightly better algebraic properties. Which we need actually for our calculations. These turned out to be crucial in the end. And secondly, we add code density construction. On top of this, which we need for specifically, expressivity, to be able to actually express the semantics that we want for current language. And secondly, we actually apply this monad it to a language higher order concurrent language. Namely essentially a lambda calculus + fork for parallel capitation's and then channel based communications between Channel 6 we formulate the semantics for the language like this and then use the machinery to get a compiler out of this. Using the calculation techniques. So in this talk I will mainly focus on the first part. How do we get to the monad and then I will talk a little bit about the second part and in detail of how we do this for this particular language. Okay let's get into this. So this is as I said based on this recent work on choice trees, presented this year. So this is the definition of choice. This is very sort of compact definition. So the choice trees allow us to model capitation's with non-determinism. So it choice tree has these two arguments and that the A the argument over there says what's the return type of this computation is. So whenever the computation terminates then it will give us a goal, a value type A, of we have computations and now constructor that expresses these. And then the second argument defines what kind of algebraic effects we can use in our choice tree. Then apart from this we also have not determine is him. In the form of nondeterministic choice operator + here. And then it's also unit 0. And then finally we can also express non-terminating computations in the form of this later constructor. That works very similar to the delay mode. If you are familiar with that. So that is the definition of choice trees. And this is very, expressive. So it forms a monad and we can also express parallel composition operators for this spectrum. And here I just mention one of these parallel composition operators, this right biased parallel composition which she because numeral two competitions this one on this one that only returns a result of the second one. It just throws away the results of the first. Competition-- capitation only executes it for its effect. So this will be sufficient for our purposes you. All right so this was the syntax of choice.
                 Now to the semantics of choice. You can have the semantics simply by giving a labeled condition system for this. There is fairly straightforward and that's all of the rules we need. And then go back and once we have that we define our bisimilarity and we have the monad laws and that we have the nondeterministic choice of forms and item pertinent commodity dash making we have nice all generic properties for composition. So this is working very nicely. So this is also. This is a very simple concurrently which-which just extends the arithmetic language that I showed of the beginning. With the printed primitive that prints the integer to the screen and then also the fork of a competition that evaluates this expression. And down here given the semantics of the language in the terms of the choice trees. So here we have the effect that you can use of the printing of the screen. And don't know the semantics for Val the just returns the value and for Add X just as Y the it has the result and then hear the bottom we have the  print statement simply the expression letter X and then prints the resulting integer. And then the interestingly here the four, the actual parallelism happens and so what happens here is we have the fork test run 0. But in parallel eight evaluates the expression X. Very compactly defining the choice trees. And this is the right semantics and as it turns out it is not.
                 If you look closer and check out a simple example. We add fork XY and see how this work and compare with what we would expect the semantics to be also unfold the definitions of the semantics right here of the semantics here of the fork. And the remainder of the semantics of the ad. And then we can apply our algebraic reasoning principles to simplify this and it turns out that the semantics of this expression is actually this year. So this is the magnetic find operator. First it gives the value X of and then afterwards it develops Y so there's no actual pearl is him going on. So kind of disappointing. The have a fork in their. So we actually would expect that the two are evaluated in parallel, right? But this is not happen. So clearly something is wrong with the semantics that I've shown you. So let's look with the semantics of the fork a little bit more closely.
                 This doesn't work. There is-- in their. If you look at the assurance of how you model the semantics of the four. He would see something like this. So to evaluate the fork inside the evaluation context which what you do actually is you evaluate the remainder of the evaluation context C and replace the fork with  value of 0. And then return it in parallel to that and evaluate X over here and write so this is the what we actually want. But in order to actually use this in our definition, so this is not actually a valid actor in Haskell. We can easily transform this into a form that uses continuation. We can extend Eve Al's as a continuation argument and then we simulate this evaluation context out using this continuation and said. So here, the continuation is used on the right-hand side of the parallel composition. Them the left-hand side we should evaluate other X with the continuation as you will. So this is now the right links, and check it out and we can see that is the one where we need for the four. And this works all nice and well for the semantics. Especially for the simple expression of language. If we actually want to reason about the semantics, you will find out that we can't really use our nice Monodic reasoning principles. We end up having to prove sort of ad hoc-- about how continuations and they interact with the valve function. And so on the sort of ad hoc areas they look very much like the money loss, not quite. So it's a bit of a shame that we have to give up. Just because we have a introduction of these continuation. So the fixes very, very easy anyway, right? To because there is a standard construction of sort of built-in a continuation into your monad is the code density mortgage reconstruction. That's exactly what we do. We extend our choice tree, monad with continuations in the special, right using standard co density construction. So now the co density choice tree look like this works like this so it takes a continuation that produces a choice tree. And then we produce out the other choice tree. And it has built in the intimation. And that upshot really is construction that is twofold. First of all we can lift all the operations on choice trees. Them on attic operations and just domestic choice of that can be lifted. And the co density choice trees construction, right? And a portly all of the algebra clause follow along and we have the same reasoning principles that we can use. So that's nice work and secondly, we can also get an additional law that tells us how the monadic bind interacts with the parallel composition. And that sort of shows that is the right parallel composition for the modelling how to fork works. So if we have to parallel, to parallel computation PMQ followed up with capitation F. And we can move that capitation F inside the power composition to the right. And this will turn out to be exactly the mesh that we would need for. So are fork construction. And we get this by defining the composition and condensed choice trees like this where we okay this credential choice trees, take continuation and the first argument which we are taking here is an argument. And then we pass this on. To only the right and that's where we want to continue the capitation. And that's what we use for the modelling of the semantics of our four. So now we can revise this in many that I've shown you earlier to now use co-density choice trees that have continuations built-in. And it looks very much the same. The only differences that looks we can use the co-density choice trees. And I check that and our example from earlier has the expected semantics now. And if you have the adding four times 21, we can unfold the definition and it looks very much like the definition I would before and the property that tells us how parallel composition and bind interacts. And we can bind here in the power composition like this and we are now we can simply add standard monad loss and simplify this to the especially. So this is exactly what we would expect. Evaluation X is evaluated in parallel to evaluation Y. All right so now we have this in place we have our monad, we have the reasoning Prince was let go with it. We of the semantics of the language.
                . Let's try to take the calculation of the compiler. We are given the syntax and the source thing would like this. We want to drive the compiler. And turns out we get a whole bunch of other stuff. With this methodology. In addition, we get the actual syntax of our source of our target language. So the code is produced by the compiler. Along with the semantics that is given here as the function that defines a virtual machine. A stack machine. So it executes this code. It takes the code as the first argument and then the stack which is simply a list of integers and then runs and gives us the code. And gives us how the stack looks like after we executed the code. And use the same effect structure that density choice trees with this printer fax. That we also have in our semantics of our source language over here. This exactly what we need for the reasoning. So the semantics of the source language and the target language live in the same monad.
                 Okay this is what we want to get out of this, we also want to get out of this the proof of the compiler spec. So we want that if you compile the source expression and run the tilting code, starting with the stack S overhear the should be the same if we just evaluate into value V and then stick this resulting value V on top of the stack. That's what we want. Out to make the calculation go through a bit more smoothly, we generalize this property. That will give us a nicer induction hypothesis. Where the compiler now in addition also takes a continuation argument. And the compiler says different X mission and I produce the code for the 6%. And then follow-up with this code with his additional continuation code afterwards. So we have this additional code continuation in this compiler. So now the compiler spec is also generalized in this fashion. It takes this additional code continuation that also shows up here and left-hand side. So after we have put the value here on the stack. We followed up with executing the continuation also. And I know this is the spec that we want to work with now and we can crank to handle and go through the compiler you should. In the calculation machinery. And we start as we prove the property and those definitions out. The compiler, the code and semantics. So let's play this. What we are going to do here is we are going to prove this by induction on the structure of the source thing was. We start with a simple case when we namely that E is just an integer literals in that case. So it starts out on the left-hand side of our stack. And now our goal is to message to so that we eventually end up in the right hand side of the equation. So the algebraic properties of that. The problem is we don't know quite yet with the right-hand side looks like. Because we don't have the definition of the compiler yet. This part is missing. What is the compiler? Well, that is the, where we did the definition of the compiler we actually fall out of this computation. And what we do now is we simply try to get into a state into a term of this form where our goal is to have some kind of term down here, see prime that is of type code. And then we simply make the connection to the thing we want to prove by field by basically saying okay we simply defined the compiler to be exactly equal to what we have calculated.
                 And now the proof will go through by definition. And the definition of the compiler by reading the last line of the computation. So this is what I mean in the definition of the compiler will simply fall out of the calculation. All right let's try to tell. The very first up is just applying the definition of the balance of the semantics of our language and we get that value in. So it's returning in and that we can apply to the monad law. So left, left unit law, that says okay we can now replace letter N for letter V and here. And now that we have the N on top of the stack. And now that was just one step missing. Our goal is to get the stack down here. And the problem is that we have the stack is down here. An additional argument with an additional element on-topic because we don't have it down here. So that means we socially have an equation to solve. How do we make these two things equal? So this equation here. Essentially.
                 And again the strategy is to solve this by fiat, simply make this into a definition. How do we make this into a definition? We have to make sure that all of the variables on the right-hand side also are bound on the left-hand side. And this turns into a definition. So as occurs on the left-hand side. Good enough. And we also have C and N over here. Don't appear on left-hand side. But we can make them appear there by simply sticking them in the see prime. Which we freely choose. Simply choose see prime to be a new constructor which we call push. Which would be apparent and why we call it push to make with these two arguments and see -- letter C right and now if we stick that in there it's now a definition. For the executive function. And now solving the equation that we need to fill in their. Not only that, we just have student discovered this new instruction for our talking language. And also it semantics which is given by this equation which we have introduced in the order to fill the gap in the last Epic to find this new instruction and it semantics that fills in the gap and we can pose the proof. Not only that, we can also read off the definition of our compiler an integer literally simply through the push instruction. This is the simple case, the value case. Let me show you the four-case and how this works.
                 Now or will run through very quickly. Again we have to proof this equation. And what we do is we start on the left-hand side and we start by applying the definition of you valves, the semantics of work, right here. And typify this by applying the algebraic law that we have and get into the state. And then the goal that we have now is to apply-- for the X year, we should be able to turn this term here into something that matches the left-hand side of our connection hypothesis. Again we do this by fear. The introducing a new instruction in our code. And the equation that defines it semantics. So it is exactly as the rate sheet so that we can apply the induction. That's exam we what we would do now. Apply the induction map of this. Which gives us over here. And now we are almost there. And now we're getting in the state where we have to solve an equation, as we get up to the final state here. Our target. And again we solve it by fear, by introducing a new instruction along with an equation that defines it semantics. And we are done. Okay so now we have discovered a new instruction and along with it semantics now we can read off a definition of the compiler for the fork expression in our sourcing which. So this is how it works. We can also use this for the rest of the source language. And so, the compiler that we get out, it's very simple. And we also get out the semantics of our target language. Wright, which is the stack machine. It is a recursive function. With the parallelism in their. So now we have a calculate it compiler for a very simple concurrent language. All right that brings me to my final slide.
                 So there is this more in the paper. So if you're interested in the details of the differences between the choice trees and how we use them and how they were originally introduced so that they can get better algebraic properties that is also the paper. The differences are subtle but they are very crucial for our completions. You also find the details for the compiler, calculation for this concurrent lambda calculus and which features full construction and channel-based communications. So for the first time we have a derivation and of such a compiler for such a language. And this is all formalized in Agda. In the choice trees, algebraic properties and also the compiler, Galatians themselves. Thank you.
                >>  ARNAUD SPIWACK: A few questions, introduce yourself please.
                >> AUDIENCE:   Hello this is Adam Chapala from MIT. The interesting thing that you are a story about the compilers they were talking about this technique is that they get it to invent their own target languages. In some sense there is a copout case that whenever the going gets tough, you just add a new instruction that does whatever you want. How can this approach be applied to cases with the target languages fixed? How can you apply this approach to situations where the target leg which of the compiler is dictated to you and you choose it?
                >>  PATRICK BAHR: You are only allowed to. From whatever instructions you have. In a way that reduces your search space in your calculations. But might make yours problem more difficult. Because you might have to have a string together several instructions. So that you can solve the equations. So you might have a bit more work to solve the equations that I showed you. But it constrains you a bit more. But I don't see in principle, a problem with that.
                >> AUDIENCE:   Thanks.
                >> AUDIENCE:   I expect you to solve the problem but you introduced continuations for any slightly different way. So I looked at that and I thought okay choice trees are not sufficiently expensive because they cannot express computation's that deliver a result and then continue to perform effects. She could have split the now constructor into two constructors, one for delivery and result and then another for halting. Did you consider that? Does it running two problems along the way?
                >>  PATRICK BAHR: So useful to now into what?
                >> AUDIENCE:   Split the now into deliver results and then into you with an executing code. And a separate instruction to halt. So you deliver mutations that deliver a result but then continue to perform effects afterwards. And that change I think would meant that you didn't need to introduced continuations.
                >>  PATRICK BAHR: Okay so you basically built them into the, I did not catch all of the details. There is a lot of echo in the room. So the idea is to build into the definition of choice trees themselves. So that you can have some continuation if you want to, that you want to perform after you've halted essentially?
                >> AUDIENCE:   Yes exactly, some action that you want to continue performing after you are delivering your results.
                >>  PATRICK BAHR: Right. I suppose that could work. I did not think about this.
                >> AUDIENCE:   Working? No. Okay. [Speaker away from microphone] so in your first choice you first define your transition system in your choice treat and then attending a simulation from the transition system. How does this compare to just defining and algebraic theory on the operations so that you directly get the equivalence consideration from the algebraic equations? Is it more general or more flexible?
                >>  PATRICK BAHR: Right so the proposal is to start with your algebraic theory so that it fits with what you need. I suppose you can do that as well but then you have to prove that it is consistent.
                >> AUDIENCE:   Yes.
                >>  PATRICK BAHR: And here we sort of take a different route. The could be possible approach to that.
                >> AUDIENCE:   Okay thank you.
                >>  PATRICK BAHR: Then you could carve out exactly the kind of laws that you would need for your proofs. All right.
