>>  I think we can start, now we're going to learn how to do bit-stealing,  like the best C programmer, the same time as we keep algebraic data types. So please tell us. .
>>  So I'm going to talk about Bit-Stealing Made Legal: Compilation for Custom Memory Representations of Algebraic Data Types, Compilation for custom memory representations of algebraic data .  And so the link here is link you can go on and play with ribbit our compiler.  And so first what we are targeting, basically any program that needs some sort of a lot of memory usage and use some inductive data structure.  And so inductive data structures used basically everywhere.  And they include various kinds of trees, like red-black trees shown here, and two really need language features to have dealing with them, a will, gebraic types and part matching, and so algebraic types, combine with some product types, and here, red black trees, and each have tags, and inside the note tag, and we also have a product type, aggregates 4 fields together, and so we can easiably, intuitively moderate red black tree, because red black tree is empty or root node, and contains color red or black, and 64 bits integer value, and subtrees.  And especially useful combine them with pattern matching, when we write functions that inspect the shape of data to express control flow.  For instance, here if we count the total number of nodes in red black tree, obviously 0 for empty, and the cardinals for each subtree for the nodes case, and this is concise and also safe, because we get static guarantees that come with... and here, immediate, both nodes and empty, and when we talk about T1 or T2 on the riding hand side of each pattern, they actually exist, and we know they are off type, RBT because of the structured we have there.  And pattern matching interesting for operations that inspect nested structures like this rather complex rebalancing operation, red-black trees, and relatively consistent readable, compared to patternless, and alternative, because we follow the structure of the data to express control flow, so filtering with this data structure, any kind of of tree, most basically want to use a language with these feature of adjacent pattern matching and now established quite useful language tools to have the next question is what about the performance cost of the nice abstractions and answer is mostly depends on the memory layout, used to represent data.  The problem is compilers for languages with pattern matching typically assume, fixed and uniform memory layouts you can't fiddle with. 
And so for example, it may be naive representation, like this one.  And so, here we can see using the graphical language, I'll keep using for the rest of the talk, that's node is hidden behind a layer of interaction behind a pointer, and to a bloc.  And another one, just for the color bit.  
And finally the rest of the data in the remaining fields.  
And so why is this a problem.  It's a problem, because in the real world, inductive data structures also used in performance critical programs, like the Linux kernel that uses red black trees internally, in Linux, red black frees follow a memory layout.  That is optimized by hand for performance and especially for space as from there So if we're compared to the previous night layouts. We can see that, starting more information in less space.  It makes use of an optimization which is known as bit stealing. Because since red black tree pointers are ... significant bit is always going to be zero. And here we're using this free bets at the end of departure to start the car, which is just one bitso it fits in there and prevents us putting word or half word just for... and so the finally tuned layout with requires control of quite a lot of details to write.  
And so it's only available through languages like C, and accordingly in the Linux kernel, data types with C stricts, and the whole data manipulation code by clever tricks, decided in the layout, and so simple parent accessor, requires masking off the lowest bit of pointer and dereferencing it.  And also pointer cast for good measure.  And have so if we look at rebalancing operation, it's even worse, and clumsy and painful to maintain, for the case of pattern matching we saw earlier, need 5-10 lines of code in C. 
And so to add to the problem, we completely use the static guarantees, that come, because whether the data type here, properly models what the user had in mind, in this case, red black trees and whether the data manipulation code is correct, everything is left up to the user, so the main goal in the work is combine arbitrary memory layout, with pattern matching, and so we achieve there providing a specification language, for custom memory layouts, which is the green exploding node here, we add on the traditional cooperation pipeline in black.  And so what what happens is user specify the representation once.  And uses data types and manipulation code, and matching.  And knave representation, and provide formal criteria, and agreements between the memory layouts and ADT's, and we also provide a new pattern matching algorithms capable of code that knows how to deal with data represented in whatever twisted way, specify earlier with memory type, and so this is all implemented in compiler, and in web demo available here.  And so going to look at memory layout, specification part, which consists of specification language, and DLS, as well as formal agreement criteria between nice high level types, and clever custom memory layouts and going to use this example, to represent our DSL.  And basically what licensus style red black trees would look like dealing with immutable data and use single bit for color, 0 for red, and 1 for black, and empty, 64 bit, word, and unspecify padding for the rest of the word, and comes to node, going to be 64 bit pointer, with to 0.  And second lowest to store color, and going to point to a structure, with 3 fields.  And so the 64 bit, integer, left and right subtrees.  And going to see how to express the layout in ribbet in DSL, and first see how specify very simple type which is color.  And so every custom type consist of source type, and standard ADT and here, same as before, and we get the keywords represented as and get the actual layout, specification, called the memory type, and the only information we have to encode for the color is a tag, red or black, and whenever we want to specify different representation for different tags, we're going to use a memory tag construct, we call a split.  
And so splits are kind of similar to types in that they express, the type, or shape of value we're dealing with, depends on some part of it, on discriminate value.  And here we have two split branches, one per tag, and each branch consists of discriminate value, 0 for red and one for black, and going to be concrete value, in memory that identifies the branch, and we get provenance between the link and memory types, and I wanted indicate which tags branch applies to, and the right hand side, and specific memory type, and I one for the single bit, and we see memory type encodes the desired representation, a single bit, set to 0 for red and 1 for black.  And so where is simple type, we haven't gained much compared to C, and so we're going to see what happens when we try it with more complex type, and the actual red-black trees, and so we... feature a tag, with several fields inside.  Which is tag if 4 fields and want to express whether fine aspect, and right here.  And since we're dealing with a type we again start with split.  And so we're going to specify discriminate location, in this case, the bit here, which is part of the memory value, we're going to use to distinguish between the two tags, empty and node.  And so again, we have two split branches one for each style, why one bit is enough to distinguish them, and specify the lowest bit is one for the empty tag, and 0 for node.  With discriminate values and provenance as before, so empty bit the lowest bit is one, we do so what we scale bit content specification, and introduced with keyword, with.  And here, lowest bit, 64 always specific to one.  And using the same location as discriminate location to get lowest here.  And single type to assert fixed value in this case, 1 at this location, so it is redundant with split left hand side, which already says this.  
But, we have it out, and leave ribbit infer, the specification here.  And this sufficient to incur the end tag, and node case is a little more complex, because we also need to fit 4 fields, and so first specify i64 pointer and lowest bit free, and to address alignment, which is okay, because we're going to point to align we see later, and also since we asserted 2 bit free.  We're going to specify the content, and so lowest set to 0.  Used as split discriminate.  And second will contain color, fits into the single bit, and how we describe bit stealing, and so the subterm, which are used to express some parts of source value here, should be represented there, in the location specified according to some memory type.  For instance, here, we say that field C of node tag, should be encoded right here, as color using the memory tag, defined earlier, and together, with split... really the memory types, to source ADT, and so the end of the pointer contains 364 bit write field and is encodes field from the node tag above.  And using subterms, and the field V encoded in the first field of 64 bits, and left and riding subtrees, presented in the second and third field and is see we cover all data from the tag from node.  
And finally the memory types, specifies complete layout for the type.  And what is not immediate from the specification why this type is suitable for encoding tree values, and so have formal criteria agreement between source and memory types, and so formally described in the paper and here look at two mains ones distinguishability in orange, and deals with some types, and ensure all types can be apart inspecting one or more discriminates in our case, okay we can look at lowest bits distinguish between empty and node, and also have coverage in blue, and deals with product types, ensure all data in source type appears in some form between the memory type, and we see each of the four fields appear somewhere in the as a subterm. 
And so far we have focused oen types, and now we're going to look at data manipulation code, more precisely going to look how we use the memory type during pattern matching operation, to generate low level code that deals with data that follows custom memory layout, the syntax is unchanged, which means compile high level code to layout specific machine code.  And so in practice, it means compiling pattern match to this decision tree, and switch nodes and rectangles here, and leaves, the green nodes here.  
And switch inspect part of the memory value, and relief says, during the execution, at this point, the pattern associated with the ID is the one we match.  
And so, we're going to use this pattern matching example, to illustrate the process.  And features 3 patterns with little bit of nested infrastructure, and our goal is to going to be animate the following decision tree.  
And so, so far we have seen the input language on the left, and now going to go through the compilation procession on the right.  Which consist of 3 main steps, which is going to... oh, characteristic of the compilation process is tree based and directed by the memory type, because, memory types are rich enough to get necessary information for pattern matching and use the fact to... refine the decision tree going progressively. 
And so... the first step of computation approach, and build skeleton of decision tree, and structure refrequents memory type, and first see relief is empty, and reporited by patterns in the next step, and empty set means no pattern currently accept values in the given leaves, and so the split memory types maps to the switch node and decision tree that inspects the discriminate location, and another type of node, which parallel nodes that group together the parts of memory type, not depend on each other, like fields of an instruct, and basically the semantics of that, should continuously evaluate each subtree, and as for which pattern we match, we match if and only if, every branch matches.  
And so we just capture structure of memory type of the scaffold of the tree and question how do we pattern information, and so wave each pattern in the tree, and good to populate leaves with pattern ID's, and expand the parts of the tree we need to to inspect nested subterms and proceed following the shape determineded by the memory type, with choices as switch nodes depending on the pattern we're currently waving, and here we start by pattern into the tree, and empty sets lowest bit to one, and only explore the one branch of the root switch node, and which leaf, and means nothing more to inspect and have the pattern to the leaf, to say pattern Aic a Septembers memory values... 
And now we're going to the second pattern, only accepts node values, and so consider the 0 branch of group switch.  And then encounter two parallel nodes that correspond to pointer and strict memory type.  And so going to recursively explore the branch of these nodes and one of the branches are leaf, and so like the previous pattern, the out put ID there.  And then we're going to into an issue, because if we look at leaf highlighted in green, corresponds to $ field and out put ID to the leaf, because this pattern does not accept any kind of colour value.  Only accept black, and so inspect the color value to match this tag.  And since the leaf annotated with memory type color, and so going to grow a new memory free.  And inspect... and so annotated leaves called buzz, because when we need inspect subterms we grow the buds into the new tree.  And so this is result of growing color buds and scaffold new tree for memory type cutter and get a switch on the subpattern, black.  And this is how we get the out put ID, and on the leaf coming from the one switch branch.  And moving on the second feeled of the interval value, because the subpattern is white, no need to grow the bud, and treat it like a leaf, and addB to say, we accept any integer there.  And left subtree.  Pattern, accept any value, and highlighted red black tree buds, and get new reflective tree, and since only interested in empty tree, pattern the leaf coming from the one switch branch here, and moving on the the last field for the right subtree, we have a white, and so we add pattern ID to the leaf.  And the last part we have to weave since we accept any black red tree value, we going to add, to every leaf in the tree, exploring every branch.  
And so, now that every pattern has been woven into the tree, it means all relevant information, types pattern have been encoded and simplify on optimizing decision tree to make it computable, and so first discard typing information turning buds into the leaves and also perform few generic transformation and here going to merge the equivalent branches together,  bunch of nodes and say the same thing and reduce to leaf C, and so this is the result, left with the following decision tree, in which still one parallel node, and the goal of this step is to sequentialize, or determinize parallel nodes, because so far left the evaluation order of the two switches unspecified because both individan, and no reason to priorityize one over the other, and order to produce executive double quotes we go, we have to pick a subtree to come first. And it is kind of choice actually no problem, patsern matching operation, we're going to use classic heuristics to choose which switch will be evaluated before the other. We'll pick the bottom switch here, and finally the decision tree, kept the first pattern IDH leaf, and first matching pattern wince, and so the out puts of the code, and so going to the decision tree here, to LLVM IR.  We see CFG here, and mostly straightforward, and subtleties that are explained in the paper and end of compilation chain, and so after LR, We can actually execute compiled by the matching for custom memory layouts and so we have various type  and layout exampleses that we used to run a few benchmarks so that we could see whether rebates actually reflects the expected shifts in performance memory usage when using optimized layouts. So we're going to look at benchmark for red-black tree rebalancing and recompile for ribbit for ocaml rust Linux layouts and have native versions and rest layouts using their respective compilers for these slides will focus on two metrics. The total space taken up by red black tree.  And also average execution time of compiles of LLVM on compile decision try on the same sample red-black tree and first thing we notice is that we find the exact same memory usage between ribbit and difference of one G pointer and good sign ribbet Kratly model existing representation, and see Linux layout takes much less space, and expected because specifically optimize for space, and so comes to execution times have to be careful computing the numbers, because comparing different run times really interested in the speedup between different layouts and can see with ribbit we do get correlation between layout optimization, and performance improvement.  So this tells us two things, one it is worthwhile to take the time to optimize memory layout, even though, now we don't know to do in C, with units any more.  And also, ribbit as much compilers when it comes to relative performance of different layouts, the full benchmarks are more extensive and available in the paperive. 
So we gone over feature of the input language and basic steps of the computation algorithm what is missing and available in the paper is full formal agreement criteria between source and memory types, and agreement source typing, and just classic typing we're able to prove the correctness of our algorithmic and it's a decision tree, and semanticall equivalent to the source power matching and proof in the paper, and capture various real world memory layouts and bit trees and so on.  And things like boxing, and quite brutal inlining, and strict packing etc and one future direction is integrate memory types with other compilers and like MLIR and rust, and we think not all memory types by hand, and probably more interesting to use them as IR, as intermediate representation, and aim to capture as many existing memory layouts as possible, and already have a few, but, many, many more, and all kinds of memory layouts just weird layouts are wanted and welcome in the great wide memory type zoo.  And please come and tell me about your weird memory types. 
And so I'm going to end the talk with example of yet another wonky memory layout, and arithmetic expressions, which reclaims some padding bits to inline , and so thanks for listening to me and go play with the online demo and come tell me about your great memory layouts.  

>>  Questions?

>>  Hi.  I have a couple of questions.  I'll ask one. 
>>  Name and... 
>>  I'm Milano assistant professor at Princeton university.  There are aspects of traditional function runtime, like Ocaml  garbage collection that are already taking advantage of hacking in extra information into spare space and pointers, or cutting off integers a little bit early. How do your techniques interact with that.  And could I represent Ocaml collection tricks in your technique. 
>>  For the representation part, most definitely because we capture quite wonk things, whether our approach calls interacts or how it interact with garbage collection etc.  I have not thought about it, but mostly thinking about people who want performance, and going to write custom memory layouts in C so probably not going to use garbage collected languages. 
And actually with custom shapes data, I think they might have trouble with that.  And probablingly the best. 
>>  That's what I was cures about, if you know the language was collectible in general, and haven't thought about it.  And... we compile it down to LLVM and no garbage collection as all.  


>>  Hi, I'm JGibbons from Oxford, very nice talk, thank you, earlier today was a talk on place functional programming kind of related about difference of reusing memory without the clever wonky data type squishing.  Have you thought about putting the two together?

>>  So I given it some thought.  
But, so far we kind of swept mutability and stuff like that under the rug, and so allocation, quite interesting topic, that high performance programmers very interested in obviously, and so this is also a future direction, I think to think about how all of this works when we need to actually locate data, whether we need to big chunk of space, and... be done with it.  And whether particular layout will affect how affect data.  Not something we thought too much about so far, because we assume we're just going to allocate once, and have... build up some fixed data and pattern match on it.  
>>  Hi I'm Richard E, Jane street.   I actually have two questions, but one of them has already been asked which is about garbage collection. So thank you for your answer there. The other  was, can this be used for sort of asymptotic games so sometimes it particularly convenient to  essentially work with a unary natural numbers, but of course that's sort of ridiculous at runtime, is this system capable instead of saying having the unary representation, we will use binary representation, can it go that far. 
>>  So using different encoding for numerical values. 
>>  So unary natural write, so storing natural number using a linked list. 
>>  Oh, yeah, actually, future directions more general linear regression of recursive data, in a lot of situations we want to squash it into the oblivion in some sort of flat structure.  And can perform brutal inlining, and... and inlining flatten list, and technically instruct of arrays and out of bounds two.  And probably the very next work... very next thing we will work on is to look at arrays, and how can it flatten data. 
>>  Sure, thank you.  

>>  So I had a question about the... Sam, Indiana university and had a question about code generation, when you showed the LVM IR you were generating you broke down each of those individual switches into the separate LVMIR.  Of course those first two switches are looking at bits that are next to each other.  And almost certainly, if... you were writing this in C code in the Linux kernel, you would look at the pair of bit patterns and combine things in some way. 
Now it might be LLVM and combine knock all that out and does the right thing, but seems like a trust but verify kind of situation. 
So does LLVM generate the kind of code that low level programmer might write instead of the naive compilation or is that a future thing you would have to optimize. 
>>  I think it really depends on what kind of options you have in the LLVM compiler. 
It's able to do some quite brutal optimizations.  
And so this is LLVM without any optimization, and to keep the overall structure of the decision tree. 
And now if you wanted to solve this, the memory-type side it would be completely possible to say the split on the... lowest bits, instead of two separate splits and say one is this, and one is that.  And also completely possible, not smart, a possibility, it doesn't make sense.  
And so there are two. 
>>  I'm curious, what the result of LLVM optimization looks like for this.  And whether it... removes the need for optimization at the higher level.  
>>  So, I don't have it at hand right now.  But, I don't... I don't remember if it merges the two locations, and so it's a good question, and have to look at LLVM does that. 
>>  And the web thing that can you play with it.  Can you get to see the final machine code as well?

>>  Sorry?

>>  This thing you can play with the compiler on the web. 
Can you see the final LVM code, and machine code. 
>>  That's exactly the part that's not on the web demo.  But the source code is also available, so if you determined to see the outputs, you can check out the source code. 
>>  All right, let's thank the speaker again.  
