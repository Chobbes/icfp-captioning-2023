>>  Hello, everyone.   Welcome to this session.  If you are online and you want to ask questions in Discord, the next talk is about explicit refinement.

>>   Hello.  I'm here to talk to you about explicit refinement types.  We will start with the favourite example.  This is a relatively simple function.  We take the first and stick it at the front.  Doing this with the empty yields the identity and on the other hand starting with something we first put in the tails and then put at the beginning.  So far so good.  Nice little programme.  This programme has a lot of properties.  Citation needed.  There is only some of them we care about.  Let's take a simple one.  Say the length is the length of the input.  It depends on the application.  Document that.  Always good to write documentation.  The problem with documentation is it is written by humans who are fallible and it might be wrong.  It might be right today but wrong tomorrow.  I prefer if the machine checked this.  There are two major directed ways to do this.  We have refinement types, which we will demonstrate with liquid Haskell and then refinement.  All we have to do is just change our signature to represent the property we want.  The output is now a set and it writes down the property we want and it is dependent on the values of the input.  The programme stays the same.  I will feed this in.  

To do this now we will have to introduce a new concept.  We could actually just write the set of lists of length that works, but-- okay, carrying on.  Not my fault.  To write this we could write a set in a property, but more natural is the data type.  We will have vector at A, and for every natural number I'm going to give you the list.  We're going to define it.  This is the empty list.  I stick something at the front I get this.  We can now translate our signature.  Notice highlighted in red I have added new indices.  That is to tell the programme what length of list we want to pass in which is useful information.  I throw that in.  It is the same.  We just added the indices.  I see the typo.  Moving on.  

Let's try type checking this.  It works.  

This is sad.  I know it is 0.  What gives?

It using definitional equality.  I don't see any way to use those rules to derive the N and N equal to 0, so I need to teach it something new.  I need to use casts.  I am explicitly saying these are the same, so cast along that.  Not very nice.  Let's do this in refinement types.  That is nicer.  Why would I use dependent?

I love refinement.  All the properties I have shown are simple.  What if I want to talk about montonicity or transitivity.  We have qualifiers that make things difficult.  So if I want to have a multiplication of two integers, that doesn't work.  Otherwise things become hard.  We have a problem here.  Even without that, solveers are massive pieces of miracles.  That means they have bugs, lots of bugs.  Years ago I go to my friendly Z3 instance and learn about quantifiers and ask, I assert to it, if for all integers A, existing B [indistinct] (talking so fast audio is unintelligible, sorry) 

We don't want this and what I have done here, refinement times have high automation, dependent types, low automation.  Refined can be low properties, depend high expressivity.  Refinement times here big trusted code base, big [indistinct] small TCB.  It's great.  The problem is this is essentially all just heuristics, based on experience but there is nothing behind this in the semantics.  If you look at these, you can have rich systems that can all be viewed as refinement type.  There is nothing intrinsic saying it has to be the case.  We want to take the system and let it support cool stuff, quantifiers, or I might want to talk about formulas like this, and to be able to do that we have to start by taking out the automation and we can put it back in a principled way.  We take a step backwards to take a step forwards and work towards the theory that lets us express rich properties.  We might want to represent schemes.  Let's do it.  Starting with the standard example.  This looks very similar to the Agda code.  We have an array here and it is a list with predicate on it.  It is the same as before you see with the empty list we are d structuring that.  We want to show that the length of that is a 0 plus N.  How do we do that?

I know that the length is equal to N.  I don't know if it is equal to N.  Normally that would be true by data reduction.  Here it is a proposition like any other.  There is nothing special.  All our comparisons is up to Alpha and this leads to useful semantic properties and long and annoying proofs.  Let's do the other case.  This is different.  The result is an array, a set.  We need to destructure that and prove the data is correct.  Then we need to have the proof.  I'm not not going to fill them in but they look the same as above.  Do you see the greyed out text?

These are all ghost variables.  They do not exist.  They disappear.  They don't matter.  We have a programme like the original Haskell programme.  Our system is designed in such a way so that no matter what your propositions all go away, ghost variables go away, it is not something like finnicky and erasure procedure.  It is always the same.  

Let's look at the previous example.  The proof obligation has changed but the non-grey text is exactly the same.  If I want to prove this, then it looks the same.  However, now I need to show if N is equal to N plus 0.  I could prove that.  I can just plug that in and it looks exactly the same.  There's no difference.  Go back to the original programme.  I could ignore the proofs and when I'm compiling it I do.  Same thing.  I couldn't tell.  So far everything I've done can be done in a refinement type system.  I could do arc tree proof.  This is really serious stuff.  Multiplication is to 0 by Beta and some Lima and then induction.  Excellent.  We could do that.  Let's recap.  What did we do?

We took fancy types.  We changed them to simple types and we do the same thing for terms.  We have simple properties but we also support complex properties.  There exists N, et cetera, so a little more complicated.  This third grade not really.  We have our simple types which are going to be units, naturals, co-products, products and, of course, function types.  We want to refine these.  The base types refine to themselves.  Co-products and erasure erases the dependency.  We get to the cool bit.  This is the first bit is a ghost, irrelevant.  We have a universal quantifier around the first bit is relevant.  The star of the show we have got sets which just erase the underlying.  We have preconditions which is A is a well formed type.  Now for our proposition, we have true/false, for and, and implication.  The and and implication is a bit weird.  They have this P of type 5.  Sy or A may not be well formed without P.  It could be that that division just doesn't like make any sense.  Now we get our quantifiers.  We can only prove tortologies in this.  That's it.  That's all our types.  

Moving on to judgments.  The standard judgment there, and then refined type.  We will have that standard judgment.  Refine context there, the Gam ma will consist of variables which are erase type, propositions, ghost variables.  We can state one of the major theories which is erasure.  We are going to need a few more judgments, that a proof is valid, a type is valid, that a proposition is valid and whole context is valid.  Moving on, we can now finally give everyone's favourite part, which is starting out what you would expect it to be.  We do add one cool thing.  We add an error stop.  That's going to let us block out things that never happened.  A branch that never happens.  We have semantics for context which is what you would speck.  Moving on, we have our semantics for the standard typing judgment, which is nice and this lets us give semantics for refine types.  It is just a subset of semantics of erase type.  

Looking at an example, what is a dependent function type.  It is set of erased functions such that for all valid inputs, the output is valid, where that E is shunting thing.  That is error stop.  Actually make this into a ghost, a quantification and we erase to the argument but nothing changed.  Nothing changed.  We have got this red X here.  It is a bit confusing because the context should erase to a unit.  We promote ghosts to regular variables.  Consider that type, set of integers, well, you know, if it is 3012, and we need to depend on N.  We can give the sets and that is the set of valid values satisfying the proposition because, of course, propositions are propositions.  True is true, false is false.  Equality is equality.  It's just simple semantic equality.  It is too simple.  It doesn't let you do functional externality.  It's exactly what you think it would be.  It is all very simple.  This lets us define this.  Empty context is valid.  If that context is valid and the context satisfies the proposition and finally a context with a variable is valid for the same, in the same with ghost variable.  The context is valid and the value assigned to that variable is valid.  It is a bit weird but exactly what you would expect.  Now we can state the most important theorem in the paper, which is semantic regularity.  

We say if A is a well typed term of A, refined contexts, then for all gamma in the updated erasure, if it is a valid context, then the erased value for the down grade of gamma.  I will get the ghosts back to units.  Is valid.  That's it.  That's semantic regularity.  

Because false means false, that means that we get the refinement system is consistent.  We can't prove fake things.  That's nice.  

Recap.  We introduce language and we take a simple typed effectual lambda calculus.  We have rich logic of properties.  Our language is higher order and quantifiers is first order.  We also support explicit proofs of all properties and we give everything  denotational semantics and prove it sound.  

That's it.  Thank you for listening.  There is my contact information.  I have some fun plans to extend this which will be entertaining so feel free to ask.

>>   Thank you for your nice talk.  We have questions.  So say your name first.

>>   He even took my mic.  It spreads it out a bit, right.

>>   Hello.  Adam from M IT.  The system you showed us seems very related to the way we are used to using type theory with Sigma types with different cool erasure issues.

>>   That's the point, yeah.

>>   That reminds me of the calculus of constructions which tackled the same thing, it is safe to erase variables.  What are the important differences?

>>  The first one is simplicity.  We have a really simple semantics.  Everything works like you expect it to work.  That is important because sticking effects is an open problem and sticking effects on something like this is hopefully much easier.  That's what we want to do.  Therefore we need to make sure to have the simplest possible foundation so we can bolt it on to something complicated without dying from quadratics.  The next is squash types.  They behave in a technically slightly different way that I'm not comfortable giving an on-the-spot rendition but it is on the back of the paper.

>>   Thank you.

>>   You presented in your fancy types that you had dependent pairs and also an existential quantifier.  My question is what is the difference and I'm guessing the answer has to do with this first order property.

>>   Yeah.  The thing is that we don't allow quantifying over either types or propositions and in the future we would like to generalise the system so that we can generalise over propositions but we don't want to generalise over types, but without typing universes.  That is one of the core differences.  If you look at the formalisation you will find products and stuff like that.  Those can be done by kind.  If you think after than existential it is a ghost and non-ghost, but it made automation easier and also for the exposition we kept them separated.  There are nicer versions of in that could be done slightly more complicated but more uniform.  We want to keep typing universes out of this because of simple semantics.

>>   Thank you very much.

>>   Ryan Scott.  One thing about refinement types, you can prove one thing and change your programme slightly and all of a sudden the solver is no longer to prove it.  Have you thought of what it would take to somehow refinement type based proof to be translated into an explicit proof and allow programmers to manipulate that to get it into a form proven manually?

>>  There is not a story of actual research but there are ideas.  There are all kind of searches and tactics.  We showed a tactic in the presentation; namely, the [indistinct] all of those are actually tactics in a way.  You could do more.  You could have one that could rely on proof output because it is first order logic not complicated.  You could do all kind of things to integrate automation to have explicit proof.  The key engineering property is a refinement type contains the information in a refinement type.  You could export this opaquely in a module and that can only be used in the type or other constant proofs that you export.  Whatever your solve your or other things create, they can only use the experts of that module.

>>   Thank you.

>>   Some day an engineer will use the system and I will be very happy.  I await.

>>   Stephanie Warrick.  Adam essentially asked my question so I will ask the same question again with my words.  On Tuesday we saw a talk about an extension with Agda, that had been ... that included erasure.  I really want to know what is the connection between this and that.  In particular, you have your context operation.  It looks very similar to this resurrection operation that shows up in treatments.  I want to understand that connection.  The one difference is you're not doing the [indistinct] you're separating your proof system from your programming language.  That's a trade-off I don't understand very well and I can see there are advantages and disadvantages for it.

>>   I think you hit the trade-off right there.  The first thing is we're aiming for simplicity because we would like the programming language to be something that is not... friendly.  The larger context of this project is ... for that you might have some weird exotic logics which might not be friendly to standard techniques and you might also have weird exotic effects that might not be friendly to Curry Howard.  So the goal is to have a system flexible in its automation ... engineers are almost certainly not going to use this but perhaps the probability is increased.  There is a lot of erasure but this is a refinement and ... why is this not dependent type...
 >>   The depend entity type theories, they give you - they work very well as meta logics.  All in a setting where you have an approach to mix in code from different systems.  You have this common thing.

>>   Here is a mono system type, yeah.

>>   Max Nu.  Something I noticed when you defined the erasure of the types it seems like there was some arbitrary choices involved.  For instance, when you have the Sigma type in logical structure, you have A on the left and B on the right but there's no - that's erasure but that's an arbitrary choice.  You could swap the order and it would work just as well.  For the purposes of verifying a programme, it seems like your data layout is arbitrarily tied to the logical structure.

>>   It's interesting you mention that.  The reason is to reduce the amount of cases.  Let's take your case where the second part is a ghost and first is not.  If I want to have the second part being a ghost I could have a set type and then the second part is a proposition and then that proposition could be an existential true and that will give me a ghost.  I can't have in the first component.  I could with the assumption type but it is a bit finnicky.  There is a lot of formers could you add.  It wouldn't add anything to the power of the system.  It would just be harder to prove things because inductions are painful.  We try to get rid of all the units but you need push value and that is nice, but it is also complicated and the name of the game in explicit refinement types land is implicit for now and weep didn't do that.

>>   Thank you very much for your talk and answers to questions.
