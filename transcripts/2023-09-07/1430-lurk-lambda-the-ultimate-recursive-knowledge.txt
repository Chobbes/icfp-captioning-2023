>> A long time ago I did my PHD thesis in the project that led to the formalization of the platform phantom. Of this is a conservation by a user that I don't know, that describe the experience of actually verifying the system theorem proved. Which amounts to downloaded programme and then running the type checker for two hours. And 12 is a bit long, it is a bit painful. Went it be nice to have the ability to create a proof of the artifact that is simple  A symptomatically faster to verify then redoing the type checking? Turns out that there is an even the description for, for any computation. And then even a sharing okay, which is what if you want to be fermat, and the proof exists and is worth looking for centuries. And so this talk is about how we did lean type checker. But I won't have enough time to go into the expressive details on how we wrote the lien type checker and the 0 knowledge proof system. You have to read the paper for that. But I will talk about it is that we did at the cheap cost of writing that type check Colonel list. Actually it was at least just as an artifact of us having an engineered something, but have been a new general-purpose functional programming language. And interpreters that define those functional programming language have a role to play in the cryptographic context. This year and especially because we made them. The cryptographers have made the relevant and that's how we built one and how it had to be that interpreter had to be shifted from the cryptographic context and where you would like to help from the community about the everything that we are doing wrong. And why use a theorem proving example, this is not a talk about fear improving. This is a talk about 0 knowledge proves. In the creek to graphic sense. So which I'm about to detail. Okay so 0 proves and our protocol that involves approval and that a verifier is receiving the proofs and the verifier reads those bites and that has a way to design whether the proof is valid or not. Traditionally have present as a proof of computational integrity. So you have a function of letter F that you like to compute over some inputs X and W. And it is producing an output of Y. And it is proof of satisfied ability in order to get the 0 knowledge part of this general agent both. And the actual protocols for creating these proofs is constructive and that is very difficult for the prover to create the proof without having the actual witness so the prover tells you I have a witness double usage that affects and F of ask and W is equal to Y I'm not to you about W. You have affects and letter Y and Y and that's all you can verify work with. The port of call it sounding complete. Sounds means you can't cheat the verifier. Complete means you have to convince the verifier in the end. But there is the one cost that all of those trip to graphic, diggers, cryptographic protocols, which is that you need to express F as an aromatic fundamental reason. And being used that lead the probably normal testing. And once you have done this expression of your computation all arithmetic secret, you get all this other cool stuff. Some cool stuff includedsuccintness and without having to rerun the type checking which is very verify the run type checking. And the other cool thing is that you get your knowledge in your witness is actually valid and something that you are interested in. And the classical names start and noninteractive arguments of knowledge that you will find. Okay, arithmetic circuits, heavy quality constraints in the end but basically the only primitive times that you have already final field elements and the only operations that you have our addition and multiplication. Now the good thing is that we have good calculability results that you can encode the traceability of any capitation here. And the one thing I said too many for the during completeness we all know and love I just of the trace of any capitation. Does that mean?
 There is already DSL's that compile, relatively simple programming language into seconds. There is five of them on the slide and there is more. And the general approach of how you go about doing that is to say hey, well we are going to break down the components of your source language. And we are going to step-by-step translate every instruction into adult communicators that translate whatever semantics on integer strings that you want to model into ad hoc company years. Except that they completely flattening of everything and you have no control over jumps or anything that even remotely like that. That means we can't go into the detail, but just to give you a flavour of this. A means unrolling all of your loops and it means that you have interesting problems when you want to branch. And it means that at a deeper level that is analytic segments are not informed model of commutation that includes traces of commutation rather than computation itself. And so example of this is that it leaks the expression of the function that computes the Syracuse recurrence. And is the subject of collatz the where the recurrence dominates for the input. Which means you cannot unroll loops for this function, expressed in the open form. And once you have the close form that is for a certain number of argument yes you can unroll. At least the conjecture is right but if you give me one of the inputs for which is usable to enrol then yes experimentally you can do it for any input it is impossible. Another thing is that it's difficult to combine to check which is an even simpler fraction. You want to branch, cool? The whole point of this proof is to verify constraints. If I don't have any alternation, I cannot verify constraints on just one branch. Of my programme. So if I have the division by 0 and one branch of the programme, that proof is not valid. Which means there is serious messaging needed to bypass to direct or generate a computation of this. Okay. So direct computation to the circuit, not really fantastic. Traces are not a great thing to compile. Is there something better? And the answer is, very recently there has been something. But there is a general framework, which is basically the way cryptographer told about something that some familiar to most everyone in this room. Which is a letter Y. They have been talking about this in the terms of IVC. Incrementally verifiable computation. It says that you have the function you can break down the and a individual but identical steps you can verify piecemeal using the incremental proves. You take the verification protocol and that verification protocol is itself a computation. It can itself become a function. So you express that verification the circuit. And then the capitation you choose to verify it and every step, is that the verification of the prior step which will be true. And / and the new increment that is the new step. Of applied to the output of the prior step and some secret witness. Regions and output to the next that. The problem with that is expressing verifier in the form of a circuit makes this computationally extremely expensive. So I think we are all clear and why this is allowing you to do a regression. The thing is we didn't know how to engineer it a practically. So this is what it looks like. And so repeated that over and over again and I greeted the individual proof or each step which is why apply the function. Now since 2022 we have an extremely high decrease in the cost of applying the Y combinator in those cryptographic protocols. The computation has become extremely cheap. And the trick here is that you do not verify the prior step of computation. In fact you just manipulate proof statements. You take proof of statements or two steps and you create third proof statement that which the validity implies the validity of each of the prior proof statements.
 That is called folding. It is also  called speed disseminators in the literature. In the case of the natural recursive form of the computation. It means that we cannot do this practically in cryptographic protocols. Cool. This means now that language the Phoenicians can look at it a bit smarter. We have a combinator is a part of the cryptographic protocol. Which means that the compile elation to a circuit just need to be the trace of what? If you are thinking in terms of the functional programming language. The step function of the combinator. And the functional programming languages with a definitional interpreter. And we gave the semantics with a small, small steps semantics the tell us exactly, the form of a small step abstract machine. How you take a source in the language and how you get to an Evaluation? Now the abject machines or not, the universal model spatial having whether long but what you should do-- he came by and showed us that there's no actual correspondence between many of the ways of defining the semantics of those functional programming linkages that correspond to a small set abstract machine. So now the only thing they need to express is the step function of an abstract machine. And that is the only thing that we need to compile into a circuit. So this is what LURK has done and is we have taken the CEK machine which is the definitional interpreter for this. We have included all of those rewrite rules in the form of separate. Now that results into saying that LURK is variance that is relatively fancy integrated CEK abstract machine separate. We took the step function rotated into a second and there is still a couple of problems with this.
 That we are going to go into in terms of obstacles. And we managed to have the constraints abstract machine and 12,000 constraints. And they are one CS. To be specific about it. 12,000 constraints to give you a sense about the size of what this represents is very small. The shadow 56 hash function expressed as the secret takes 44,000 constraints. A lot of those countries are padding, if you remove all of the padding and just focus on the compression function that is at the core of this hash function. You are really looking at 27,000 constraints. Specifying the CEK abstract machine was writing essentially manually of course all the programme making tools that are at our disposal.
 And what does writing and interbreeding the cryptographic context mean? Whether is domain specific knowledge and how to write seconds. Translating the moralizing of the primitive types of your language as well as the control vectors. And a lot of the tweaks that we don't have time to go over but please refine the paper as well as its preference for what that represents. But the larger aspect is of the abstract machine. So that CE another iteration into another. A control term and environment and continuation. And the idea here is that all of the state management is adverse. That is the provers should not be able to fool you as to which programme they are proving anything about. And the other idea is that in the sea keep machine, the rules on this screen is even if you don't need to write one of them, is that they are a lot of variable and holds. All of this in the science of writing an abstract machine, is about refocusing which sets out the term that we now want to compute on. Now that better matching for us requires forcing the prover to review only part of the programme and not the entire programme. While directing the prover through the graphic protocol about which part of the term they should actually reveal to us to follow the correct step of the abstract machine. How do we do that?
 The answer is, we have everything. Whether in passing large inputs both for passing the state of the abstract machine in general and bounding them to what has been put in them. All of those are authentic data structures. So the control word got the environment and the continuation are authenticated data structures found to buy a hash. Every time they tell us about those things they need to tell us that they are the correct thing and that is the hash is to the commitment.
 And more interestingly, we have those reflected in the linkage itself. So the language can now speak about the cryptographic protocols and convince themselves. You can when you are the prover, evaluating the ripple, create commitments as well as open those commitments. And then passing a programme itself. The term is done through hashing. True cryptographic hashing. Using the well-known technique of hashing. Using the well-known-- we not only hash everything but we hash every structure, sub-usually this is done in compilers to get cheap, equality between values. The way that we do this is reason we use this technique in order to focus some part of the analysis by forcing the prover to give us the pre-image of a particular hash. Gets us the evaluation of which will this interpreter fires and that is a syntax directed. And everything that is a cause, everything in the C mission, everything that we paddle match against is the head that can't, and that approval is for will gives us the hash of the tale of the list and that value for the actual input. Managed to fire a rule and validate the correct step of that prover should be doing at that particular iteration of the step mission of the CE K. Additionally LURK is allegedly is. It's deterministic this. We haven't put all the bells and whistles in their. We can tell you how many iterations of the interpreter we need for the produce results as well as the produced proofs of those results. But can you do -- what can you do with LURK? You can write functional commitments that you can now make proofs about within LURK without knowing anything about circuits or caring about what they are. What that concretely means is that if you have a super secret function that you do not want to reveal. For those that are in the American context, or at least the North American context, that would be a credit score function. Your bank, your giving a credit score, you are an agency in your giving a credit score to a bunch of people but you don't want to tell them how it is computed.
 You can make proofs about how the credit score you delivered to one particular user is fair in the sense that it was computed with the same algorithm that everybody already had. We can even do fancy things such as higher order functional commitments. That in due time I will leave you to read about in the paper. And we have, built this is a complete system that has repository preprints and specification that has already been noted in the editorial blog this is something you're interested in making proofs about.
 I hope that this talk has given you a flavour of how, why we really like this, the generality of the methods we are employing, mean that we could possibly have chosen another abstract --abstract machine. If you things stood out. Maybe you are interested in building a proof system for another abstract machine because you care about another functional programming language. Maybe you care about OCaml, maybe you care about hassle, and, facial integrity for those linkages. So there are a few things that we are still figuring out. In cryptographic context, bookkeeping steps are a lot more expensive than abstract machines issue. Some of the history of the zinc machine for OCaml, for example, has a list that if you look at the least abstract machines that looking at keeping through steps related to passing arguments of function through purification and then it is think the vacation of a short-lived closure is then passed the argument and that's a little bit pedantic and that was interesting to remove. And we need to prove accountable through hash and that is costly. So all bookkeeping steps are terrible. So the other thing is 12,000 constraints is tiny. We already have a specification of what this does. This is a -- any method will get you formally verified way of expressing proofs as seconds as comparing -- compiling seconds one by one. And then finally abstract machines that we know and love are very similar and they are designed all function to pattern matching. And there's probably an abstracted general discipline. Including them to seconds. In the end we have a multidisciplinary team with a lot of folks from a variety of backgrounds we are probably being-- in a number of aspects, if you have some feedback to share it, we would be really considering a gift, it says in so many terms. That we are really being stupid about a couple of things. Thank you very much. 
                >>PETER THIEMANN: We have plenty of time for questions, right? 
>>  Yes Greg Morrissette, Cornell, very, very cool work. If you relax this option about $0. If I don't care whether I'm releasing information about the proof. Can you take advantage of that in the translation? 
                >> NADA AMIN: Yes, in general yes we are paying much cheaper cost not quite at the abstract level machine that is. The abstract achievable the design looks roughly the same. Your proof still needs a basic notion of correctness and that is driving the cost about that level and that is very true about the cryptographic level. And if you relax the gene requirements, in general cryptographers know that. So when it comes to engineering they are offering in moments of two forms of their system, the costing one that requires the genre age that used blinding for only normal minutes for example. And the cheaper one as well if you're interested in the system in detail, please come talk to me I'm happy to help. 
>> --Chapman outputs. You mention the circuit size and could you comment on the performance in terms of how long it would take to generate the proof and how long to check it? 
>> So this gets relatively complex. So we made proof of the function at 1000 iterations-- in around 13 seconds. Which sounds absolutely horrible. There are layers to this performance. But Taylor the base system that we are using to get this chip recursion and now, the reason being that it relies on the district log assumption for those finite fields. We do model this using and particulars. And all of the model related curves you need to have big fields. I cannot go into all of the details of this, the cryptographic cost, but we already have results at ASICs and see and Justin Taylor, Georgetown University, etc. This year. That we can get ten ask out of this. Which means it performance is I think the answer you want is performance for the knowledge proof system has been for a long time of the domain of is too soon really. That it doesn't work. Proving it's very great. But compared to proving but proving is horrendously-- we are getting out of the market now. The proof speed improvements over the last 3 to five years thanks to the massive investments in the space, have already knocked down ten ask and they are about to do a second one in the implementations. For which we already have benchmarks. That certainly not widely distribute it. So our system is still a little bit slow. It is built on other systems that we know will improve very shortly.
                >> AUDIENCE:   Thank you.
                >> AUDIENCE:   I had a similar question. If you compare the size of the circuit of the interpreter to the Shaw 256 function, the difference there is that the Shaw function is all ready for and so you know the size of the programme directly correlates to the size of the circuit whereas the interpreter is being run recursively? Can you hear me or no? Because the interpreter is being run recursively got it seems weird to compare the size of the generated circuit to the size of the Shaw function because you don't know how many in advance of the iterations of the interpreter that you run to the complete the programme? Is that right so far? So I guess I'm wondering is that the size for the prove complexity for generating the proof is known for example because of the Shaw 256 because you know the size of the circuit, where as you don't know that advance of the proof complexity can structure because you need to know how many times it runs. My question is like you will you could just run the interpreter into many times it runs without worrying about the proof. I'm wondering if you can just directly compute the size of the construction or complexity of the construction advance by some simple formula for how many times the interpreter will run? 
>> Right, okay so again we are somewhere, first you are complete right. This is the computation in itself. But there is a couple slight details in this. For example what is the size of the input that you are giving to Shaw 256? And what I was giving is a step function of an abstract machine that might run for a large number of steps that can predict it otherwise I would have solved the stopping problem. Completely right. The point is that I was saying about the relatively small circuit, is first and foremost, that from a correctness point of view. Verifying those 12,000 constraints is looking at a circuit that is much easier to formalize. Because basically it fits on the metric ticket. That is not a point to effective. The reason why I said it would make these things efficient. Is that we are pretty sure that the secretary is minimal and is going to be difficult to make it, and so far that's what is got to make it smaller. The next improvement are in the stuff that the functional programmers and compiler writers are very familiar with. Let's reduce the number of steps that we run through, perhaps the modification of the sector itself. You are correct that this does not guarantee horribly efficient benchmarks.
 To give an insight, we ran both evaluation and proving at the moment, proving is 30 times slower than evaluation. So evaluation has nothing to do with circuits. Your just running a least temperature on the programme. The ballpark is for really to be sensible common task. It takes 30 times the time of the evaluation to prove the competition. Which is terrible if all you want to do is prove it. But two things, first, we now know that with the technical lookup of arguments. We can make that proving faster by your ten ask. Specifically in the conditions which you are modelling computation over small objects. And where you want to pay the cost of manipulate no small objects rather than the one that's not, so far has been forcing us to use which maneuvering big financials, the financials are tuna 56 bits which you want to model and VM is computation of the doubly integrated integers, 64 bits, not 256. The eight X blowup is horrible.
 So next year we are hoping for three times, so proving is three times slower than average. And when proving is slower than he will evaluation you can get into a range were some applications get to become really interesting.
 And that is that I may need to run my tablet shaker three times slower but I only run it once because the verifiers will get such a discount on running the prover. And the exponentially discount and it will be interesting for them not to have to redo their work. And while you are developing your prover, the theorem you are reading the type yourself. And once you're done you spent six hours generating the proof. Ship it to whoever wants to verify. Then they verify in ten minutes instead of two hours.
                >> AUDIENCE:   Thank you for going into detail. 
                >>PETER THIEMANN: Thank you again. I guess lunch time?
