>>  All right, welcome everybody, welcome back from lunch I hope you enjoyed.  Our post-lunch speaker will be Lukas telling us about how to evaluate blame for gradual types take it away.  
>>  All right, this talk will be about this research question: How does gradual typing help with debugging mistake? And by that I mean language that lets programmers mix typed and untyped code. And so in this work, we're interested in investigating this question empirically and interested how implementation of gradual typing behave in real programs. And so the way we come to do that. Investigate completely different angle. And a little sneak preview to whet your appetite, and for now want to slow down and talk about the context, and for now, talk about TypeScript, a well-known success story of gradual typing, and had a keynote about it yesterday, in case you missed it. TypeScript is a language that adds optional type annotations on top of JavaScript. And widely adopted and a whole lot of new code written in TypeScript these days. And natural developers writing that code want to reuse the code from the extensive JavaScript ecosystem. But making that connection from the JavaScript world to the TypeScript world presents a problem. Because in order to maximize the usefulness of TypeScript type-checker, those libraries or at least APIs ought to come with type annotations. And the TypeScript community has come together to try and bridge this gap creating this thing called definitely typed. And what is that? Essentially, definitely typed is a crowdsourced repository of type annotations and called type declarations or I like to call type interfaces because they essentially describe the interface of a library, so for example, a sorting library interface might look something like that saying it provides one function called sort that accepts two arguments, first one an array with anything inside, and the second one, The second one is itself a function of two arguments that returns a Boolean. The comparison function, and result function in array. And an interesting thing, the interface is crowdsourced by 1,000s of contributors to the definitely typed repository, meaning it probably wasn't written by the original author the library is referring to in the JavaScript world. In addition to that, it lives in a completely different place here on the definitely typed GitHub repository. So what I'm getting at is it's quite easy for the process to result in mistakes in type annotations. 
The transient semantics on the other hand takes a different approach entirely, and goes and rewrite type annotated code to insert the inline type assertions that check the type values in the type code as it runs, and transient provides it's own flavour of blame, and this one identifying potentially many boundaries in the program as being possibly involved in the fault.  
Now, of course, these alternative approaches comes with performance overheads they do all this extra checking and error tracking, but maybe it's worthwhile if the extra error information they provide, provides debugging benefits.  
So now we have the context to state more refined research question we're really interested in here.  Which given the staple statics which semantics if gradual typing provides better error information for systemically located type interface mistakes?

And now to understand this question, and how we investigate it, it's important to understand concretely how the different semantics compare in terms of debugging information they offer in the face of type interface mistakes.  
And so let's look at a smaller example that illustrates the difference in imaginary python-ish type language, in this example we have untyped library called JSON unpack, and have an interface for it declares, function of the same name, and accept JavaScript object and returns list of hash table on the client side untyped main component that imports the function and calls it and passes the result off to a helper function to process the result.  
And that helper function is itself defined as separate client module which is typed, and all it does is takes the data apart, and iterating over it using appropriate list iteration forms and hash table lookup operations all of this under the assumption has the data that has the shape described by the type on the interface.  Unfortunately that type is wrong.  JSON unpack doesn't actually return list of hash tables but actually list of association lists. 
And so now let's see what happens when we run the program with each of the 3 semantics starting with the erasure.  With this example, the type annotations here don't help the type-checker identify the problem, so when we run the program with the erasure semantics program might crash, at best, perhaps the Boyd of the helper function, and  when trying to perhaps to use a hash lookup operation on one of these entries which isn't actually a hash table. When And in that case, all the developer gets is a stack trace, identifying the main and help or components here. Under natural semantics with exact same program and exact same type system would have a different result, the type annotations with the type interface would become a contract, ensuring the result of JSON unpack is list of hash table when that would fail, and natural would provide special blame information.  Which would identify the boundary, between the JSON unpack interface, and untyped library itself, clearly mismatch between the type on the interface and actual value returned by the function.  
And finally under the transient semantics the same program with the same type system i Transient goes and rewrites the types helper function here, and it inserts inline type assertions, one right at the start of the function body, which asserts that data is a list, and then we're in one right at the start of the Boyd of the loop, and asserts each entry, is a hash table, and assertion fails and transient provides, own kind of blame, identifies the boundary between the main helper components this simple example already makes clear there is significant difference in the information that each of the semantic provides in the face of type-interface mistakes. 
So a research question is, how do they compare if one's trying to locate the mistake in the program.  And to understand this question, we really need to do is systematic investigation and comparison how each of the different information each of the semantic offers relates to the type-interface mistakes.    ICFP and before that, lays out a framework for this kind of investigation called the rational-programmer. And the ICFP paper in particular, describes how the rational-programmer framework we use to create an experimental method to evaluate and compare the value of error information, locating bugs and really type programs. Now that paper focuses on mistakes in code only.  So in work, investigating flip side of the coin, mistakes in type annotations and hence the title of the talk.  But the main point here is this experimental method consists of four pieces. First, we formulate a hypothesis how the information systemically relates to the location of bugs, and then design automated procedures that rarefy the hypothesis into the action and trying to automatically locate bugs in the program, and with those procedures in hand, we perform large scale experiment, testing how successfully able to locate bugs in real programs, and result of that experiment is data evidence that either supports or refutiles the hypothesis and can use the data as basis for comparing the different semantics with respect to the original hypothesis, and now through the rest of the talk going through each of the pieces in turn and talk about how they look in setting with investigating type-interface mistakes.
Starting with the hypothesis, In one sentence, The hypothesis we investigate that error information can be translated into the location of type-interface mistakes by using the type system itself. In other words, we can locate the cause of the problem by adding more type-annotation So the program, guided by the information available to us, to develop intuition on the hypothesis, and why it's reasonable, let's return to the example we're looking at.  The natural semantics here and had the blame and identifying the boundary between the type interface and untyped library.  And we got in blame because as mismatch between the type in the interface, and actual result of the JSON unpack.  And the idea here of the end of this hypothesis, so that we can use the type system itself. To get more information by annotating the untyped part in the.  Identifying JSO.  Unpack to type component, and then run the program again, and if we do we find the type-checker doesn't allow us to.  And raises an error, and directly identifies the mismatch between the actual type of JSON unpack, and typen othe interface.  And that's the idea of the hypothesis concretely, and at least for natural this example seems to support our hypothesis.  And let's see the case with transient.  We can apply the same reasoning here, using transient blame, instead of naturals, and so we start with the blame transient provided between the helper and main components and again looked at the untyped here and annotate main, and turning into the type component, and try to run the resulting program, and in this case, the new type annotations don't help the type check tore decide the problem, and so we can run it and get new error message from transient with new blame, and now identifying this boundary we had before from natural.  
And so we know if we repeat the logic here, we again then get to type error, and so we can say for transient again this example seems to support hypothesis.  With an erasure not so lucky, and we do have the stack trace identifying the maybe and help components here, and can select the untyped component here, equipped with types and try to run the resulting program, unfortunately, adding the new types doesn't affect how the program runs, that's the whole point of erasure, and so we get the exact same error message, and now, the error message gives us no pointers about how to proceed according to the hypothesis, and here we can say at least for the example of're our hypothesis does not seem to be supported.  And so now we have concrete sense of the hypothesis and why it's reasonable, and the next step is to reify it in automated procedure, and what the procedure is going to do is essentially what we just did in the example, going to run the program and get an error messages and identify the untyped part in the error message and type annotations to component, and third step might fail if for example, there is no untyped component identified by the error message we just saw in the example, or some similar situation.  And in that case, we say the procedure overall fails to locate the cause of the problem with the program.  
And also a small asterisk here, if you are really thinking hard about this, you might be wondering how are you going to automatically type an arbitrary component, and hold that thought, I will get to that in a minute.  So in the case we can, the component, it goes ahead and type checks the program and if it type checks we can go back to step 1 and repeat the process from there.  If it doesn't type check, we have type error, and in that case, say the procedure succeeds because the translated dynamic error in the static one should identify the problem. 
Now of course we saw from the example, really we can make versions of this procedure that use each of the different semantics we're interested in testing not just the natural semantics and just adjust slightly corresponding with the error information they provide, and really we got family of similar procedures that essentially do the same thing, and one problem here is that it natural and transient they actually do two things differently than erasure.  On the one hand do all this dynamic checks to ensure runtime values match up with the typeannotations. And if those checks detects a problem. They also provide the special blame information on top. And really two differences we would like to separate to isolate the effect of having dynamic checks on the language with the effect of having blame on top of them, and to do that with what we can do is create two more versions of this procedure that use the natural and Transient's semantics. So they have all the corresponding dynamic checks, but they ignore blame information, simply use the more basic stack trace, that also comes alongside with these check failures. 
It's important to note that these stacktraces is going to be different from the ones that will you'll get with the erasure, because they come with failure checks that erasure doesn't have.  And finally small piece of terminology we call each of the version of procedure mode and is relative success and failure of these modes in the face of real programs with bugs, will give us quantitative understanding of the comparative value of the information that each mode is using to try and locate bugs at least with respect to hypothesis.  So now that we have the procedures we're going to design an experiment to test them out on real programs. And of course the challenge here is to obtain suitable programs with type-interface mistakes. For this experiment that we start with the  GTP benchmark suite. Originally used by Ben Greenman, and collaborators to evaluate the performance of gradual typing Racket. 
And the suite has a couple of features that make it a   particularly good starting point for this experiment.  First, it's a diverse corpus of programs, written by various  authors originally for various practical purposes, and in  various different styles and so on. And secondly, it has this property, where every program in the suite has interchangeable typed and untyped versions of every component in the program, such that you can select either the type of the untyped version of each component get concrete configuration of the program to run.  And this actually has an important effect of allowing us a trivially automate that step of our procedure. If the type  and arbitrary component that might get blamed.  Finally, the suites also reject packet which thanks again the  agreement is the only platform that has implementations of all three semantics, under the same language, the exact same type system. Okay, so from the starting point. We then use mutation to inject synthetic faults into the programs, transforming the suite in a large corpus of possibly buggy version is called mutant's it for the single known potential bug.  Now of course, setting, we're interested in bugs and type interfaces and traditional mutation expressly targets code and tries really hard to avoid introducing type-level mistakes to we need to design and evaluate new set of mutation operators that target the new types.  And actually a lot to be said how we design and evaluate the mutators, help go from these mutant's Then, concrete scenarios that are suitable to test in the experiment. But for the sake of time, I have to skip over those details. Please go to the paper if you're interested talk to me after wards and skip ahead, and result of the running the speed limit, and the data, can be visualized in this way.  The plot is a summery of comparative success of each mode, and against all the modes we tested actually quite a lot, packed here, and going to just spend a few seconds talking about how to read it.  So at the top is the name of a mode we tested. And this mode is being compared against all of the other modes along the x axis. For each such comparison there is two bars, if we Zoom in one one opthem, for example, here is how you read it.  The green bar above the 0 line,  the percentage of scenarios where the mode at  he top succeeds, but the mode at the bottom fails.  Partner We can interpret this as a percentage of scenarios where the   information that the mode at the top uses was more useful for our  procedure than the information at the bottom mode uses.  Now the red bar below depicts the inverse percentage. And so  you have these kinds of comparisons for all of the modes we tested.  And create kind of plot, and mode and full data in the paper, if want to highlight few key observations.  So quick glance can see the natural blame significant debugging benefits in the space.  And translated procedure location of the bug in 20 to 40% more of the scenarios of the other modes we tested.   The other modes that we tested Transient's blame on the other hand, has more mixed success.  And in particular, it only improves a bit over transient without blame or erasure.  IfsAnd in particular I want to highlight that they provide either significantly worse information than a ratio for a procedure  and on parawith erasure, and so all these results suggest a few take aways or really questions that bear further investigation:  First of all, considering the overwhelmingly poor vue of error information provided by the natural transient semantics that do dynamic checks and don't provide blame, and question arises whether these kind of semantics makes sense practically speaking and second, natural strong success, demonstrates very concrete connection between the blame information it provides and locating type interface bugs.  
And it's also well known natural has often prohibited performance overheads, and so the trade-off between the debugging information one gets, and the performance over heads is unclear.  But it's worth noting this coensized with the result of previous investigation that found the same thing for code mistakes.  
So, still the question seems unclear.  And to that point, I want to share one last insight.  
Which is that we asked one more question of our data.  What if we separate debugging from bug detection?
In other words:  How often does each semantics raise an error -- any kind of error, to indicate problem with the program, separately considered from whether you can use that information to locate the bug.  And we found the bugs and the scenario tests able to dedetection vast in a jurorty of bugs, and on bar with the semantic about 5% lower.  That's just one last takeaway, which is that perhaps eraser suffices for production use.   If there does end up being a type  an error and then perhaps natural with blame has a role to play as an offline debugging mode, where developers can try to reproduce the problem and debug,   extra information natural offers without necessarily having to worry  about the performance overheads as much. So I'll leave it at that and if you interest your attention, and happy now to take questions.  
>>  May I ask a question.  Hello.  Hi, Will Creigton from Brown.  I'm a big fan of lightweight usability methodologies that don't rely on humans, but I also think that it's really important that we have, you know, in psychology they call validation on an instrument like that, where you have evidence that these non-human methods correlate with some kind of psychometrics that we care about.  So I'm curious what would be your argument as to why this is a valid proxy for real human debugging behaviour, as opposed to alternative ways people might interpret the information given by the CLI that wouldn't correspond to the process predescribed by rational programmer. 
>>  Absolutely, and great question.  And I think at one level my answer is a bit of a cop out, because it's that, I wouldn't claim this corresponds in any way what real developers do in the face of error messages and whether this information is helpful for real developers to look at it and think about what to do and eventually find the bug. 
Rather, I would say this investigation is about are there systemic relationships between the information in the error message and the location of bugs.  
And this is kind of being separated in the experiment from the also extremely important question of:  Is that connection useful?
Right?
Like, if you show a programmer this error message, and this connection holds, and useful combination for them to find the bug, and I think that demands separate study, I would say very important it needs to be done.  
>>  Thanks.  
>>  Max.  University Michigan.  I have a question about generallyizability of this to different kind of languages specifically with erasure semantics Racket is a great language, in that if you do... you use most of the built in libraries and you have a type error, it will actually result in a run-time error, right, and where as in a language like JavaScript, it's very likely there is logical type error, and go on for considerably longer, and may not be any runtime error at all, and so wonder if you have thoughts evaluating this for JavaScript, and if you think specifically the erasure catching most of the bugs would generallyize to most of the semantics. 
>>  Yeah, great question.  And absolutely, I think, like that's an important thing to point out, that, type track, and racket underlying right, have all the safety checks everywhere.  In JavaScript, it's more of attitude is like, that went wrong, let me put undefined or something like that.  And then it can kind of propagate.  
And so I think really, that demands a study with JavaScript, I think. 
And so the question of:  Whether this natural debugging mode makes sense in the context of JavaScript demands another study, I think, interesting thing about TypeScript type tracker type systems pretty similar, and interesting advance features they share, and in that respect good for generalalizability, but I think separate study needed. 
>>  We're running short of time, and maybe have one more question, while the next speaker is setting up.  
>>  Sure. 
>>  I'll give it to Sam.  
>>  Thank you.  
>>  And are we set to go for the next speaker?
Okay.  So, quick question.  
>>  So thank you for the great talk.  I think one thing I wanted to comment:  It seems like your results give us insight into why natural is expensive and in particular, the results of the natural exceptions being useless, tells us that what natural is doing is... it's connecting information across a long distance, because otherwise, the blame and stack trace would have closer connection.  And so the nicely demonstrates, there is something fundamental about that distance in what blame is doing in the natural system, which is why sort of ineradicable form of performance over head, and really appreciate clarifying the that aspect of the landscape.  
>>  I agree, and think that's a good insight.  Thank you.  
