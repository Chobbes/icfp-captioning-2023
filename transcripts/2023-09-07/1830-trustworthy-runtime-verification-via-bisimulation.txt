                >>  ARNAUD SPIWACK: Okay well we are setting up our next speaker is Ryan Scott. You were sticking around. Ryan? 
                >> RYAN SCOTT: Okay thank you for sticking around and today I want to tell you a story about compiler verification. Specifically, we have a compiler that takes a high-level piece of stream code and then compiles it to low-level C code. And we have a verification tool that proves that the two are equivalent. If you ICF long enough this is a story that probably sounds familiar. Compiler verification is really not a new thing. Your have been many verified compilers before. That there is some constraints that we worked under before that makes our story little bit unique.
                 So one thing that is interesting about our project is that both the source language which is called copilot and the compiler for it which is called copilot see 99. Both of these existed well before we ever started the project. And moreover we were not at liberty to make extensive pages to these and pursuit of verifying them. So we wrote this verification tool, which we call copilot verify. We need to make this work with --because of tech. And other constraint that we operated under is that we did not have a lot of time or money overall compared to some of these other verification efforts which took many years to complete. Contrast and only had about one year to complete this project on this project and pretty small team of engineers. So despite the pretty heavy constraints. We were able to get this project done. I attribute the fact that we were very judicious with our novelty budget. We identify the parts that we wanted to focus on and spent a lot of effort getting right. But for the most part we built on top of existing formal methods tools. So all of the parts of this talk will look very familiar to people in the audience. And I think it is a testament to the fact that you can take these for all methods libraries. Answer of it just plug them into your system and it works. And you can use this to save yourself a lot of time in the long run. So this is sort of a success story about how formal methods have come a long way. So what is new in this talk mac to give you a preview of what is to come, copilot verifier proves a programme equivalence and does so by using a bisimulation based technique. When I say the bisimulation it means that we take the stream programme as well as the C programme and it compiles into and we represent the states of each programme is a state machine. And we prove that they are and bisimulation with each other. There is a more precise definition of what that means in the paper. But for the purposes of this talk, this just means that the observable behaviour of both programmes coincide with each other.
                 And what is nice about our verification tool is that it is almost completely automatic. It discharges a lot of the heavy lifting to the SMT solvers under the hood. Which means that you get most of this for free.
                 So how do we do? Well, we managed to complete a fully working version copilot verify it in just under one year with a team of two engineers working part-time on the project. And what is really exciting is that this is actually going to have a real-world impact. You know, it just kind of a escape the research phase and they are plans to use this on missions at NASA and the future. Which is really exciting to me. And as you can probably imagine there is a pretty high-powered for safety if you want to use some piece of code in a NASA mission but naturally there is some redtape we need to clear first. In the meantime we have been trying to copilot verifier on some examples that might look like things we might find on a NASA mission. And through this process we have already discovered ten bugs in the copilot compiler, and you know this is a good sign that the programme is checking for interesting properties. Some of these bugs include things like potential memory and safety and this kind of bug trigger and a mission that there is a lot of money that lies in the line that could potentially be disastrous. So that is a good cashed that these things are early. And we also ran a copilot verifier on the corpus of programmes in the copilot test suite and it was able to verify all of them. And it was a encouraging sign that we can handle these things that you would find in the real world. This include some examples of algorithms that you would find like the well clear violation algorithm that unmanned aircraft and that you use on a mission might employ. So that sort of the background of the talk and I want to spend the rest of the time going into the details of the with the copilot verify a tool is doing. The word copilot is a limit overloaded this day and CS, when I say copilot I simply mean a framework and language writing for monitors using a technique called runtime verification. So the copilot, that allows you to write programmes and a stream based domain specifically which that is embedded in Haskell. So is pretty high-level as far as these things go. And it is designed for writing monitors, so you would link against some larger application. The ideas that the manager will check the application misbehaving the runtime. And if it does they will issue some kind of warning Tony to take corrective action. And then this is the idea behind running verification. This is a pretty old and which, it was originally developed I can 2010 in collaboration with Galois  and the National Institute of Aerospace so it was already over a decade old by this point. And that give you an idea of what the copilot programme looks like to give you an idea what we're doing. So the basic unit of competition of a copilot programme is a stream. So here we define Fibonacci numbers stream. And be have moments of time. And starting at the time Step 1, you get the Fibonacci number one and then as you advance time you get larger number Fibonacci number. Next app you have one Up you have numeral two, then next app 3, 5 and eight and so on and so you can keep advancing as many steps as you want. Once you have the streams you can write the operation stats about the sort of perform analysis on the streams and do interesting things with them.
                 So for instance you can write a stream as different and that is an argument and then produces a stream as a result. So this check in the stream are even are not. If you have programmed in Haskell before, this code will probably look familiar and that is because you sort of take a lot of common operations in the Haskell prelude and left them up to the stream level such as arithmetic, modular division and equality and so on and so forth. So with the, where the rubber hits the road and a copilot programme is writing a specification. And that is done with this spec type. Spec consist of one or more triggers. So in this example we have a trigger called even.
                 A trigger it takes a stream of Boolean's, anytime you sample a true value in a stream of Boolean's of the trigger gets fired. That is in indication and the applicant and that's being monitored that should take some kind of corrective action. You can imagine a more sophisticated example that does something like monitor the engine in a thermos date and at the engine gets too high you might to issue a trigger for instance.
                 So once you have a spec you can feed this into the copilot C Compiler which is called copilot C 99. It will generate some efficiency code that you can run on something like an embedded system. Here is an example of what this generated C code might look like. Don't worry about reading this line by line. The thing that I want to highlight here is sort of the difference in operation between the two types of programmes. So the stream programme on the left, uses infinite streams. Which is all well and good. But the see programme on the right, it has to run in the constant space. So naturally it can't have infinite data. So what it does is uses a fixed size array to store a finite subsection of the stream and memory at any given time. And then as you advance the state of the programme and as you advance time, you compute a new Fibonacci number and store in the array and repeat the process. As it turns out you only need the two most recent of a not she numbers to compute the next one. So this array only has to be your size to. So let's save on quite a bit of space.
                 Now it's my claim but these two programmes have observably equivalent behaviour. Have to sort of squint at this code to convince yourself that is the case. Now this is kind of a small example there is not that much go here. But if you build a copilot programme of any reasonable size this gets quite unwieldy. And few make a mistake when writing the copilot compiler, then you might potentially overlook something here. Like you will notice there is a lot of tricky business here with the modular division. And the compiler writer forgot to include one of those modular divisions than that could potentially be disastrous, especially since that is how the array gets and that. If you index the array out of bounds then something terrible could happen. So when we pitch this idea to NASA they said okay we need something more rigorous rather than just having a human stare at this a long time and giving us a thumbs-up or thumbs down. So that is the question we had to answer. How do we know in general that the copilot generator C code is trustworthy?
                 So we thought about this question in we came up with three possible approaches for how to go about this. So one approach is, well, as I mentioned we could have a human whose job it is to audit the code by hand and be the arbiter of whether it's allowed to be used on a mission or not. And you know, this is one way that you could do it. But hopefully I don't have to convince ICFP audience that this is not a perfect solution. Humans are certainly not without full and you can see it is leaked footage of a frazzled NASA auditor if you stare at auto generated C code for a while you start to lose your mind. So we did something a bit more rigorous with this. And another option is we could formally verify the entire copilot compiler. And this is something that has been done before in the likes of K canal and -- and of the source. This is a labor-intensive process. The sorts of efforts to really take a lot of years with multiple people working on them. Did not have that at our disposal. Needed something more lightweight than this. I said this is a compiler verification project. So what we do if we didn't verify the entire compiler? We ultimately ended up doing a lighter approach that it was called translation validation. And this is an idea dating back to the nineties. And the idea that the contrast to a fully verified compiler where you know that for any given source programme that the target programme a produces has the same semantics.
                 With the translation validation we sort of run your prover alongside the compiler when it is invoked on one specific source programme. And then that will tell you that you know for that specific target programme. You have equivalent semantics. So this is the weaker results. Then the full compiler verification. You know it doesn't have this for all sorts and programme target at the beginning of your theorem that you proved. But despite the fact that it is weaker it is nice for us because it means that we can sort of attach this to an existing compiler without having to substantially rewrite parts of it. To make it work. And this is great for us because we were able to get this done in much less time. This is ultimately we why we chose the transition validation as approach to this problem.
                 Let me go more into about what the copilot verifier is doing when it proves something. To start off with got you need to have a copilot programme which is written as a Haskell file. This is going to run alongside the copilot sequence C Compiler and you have two first turned into a C programme and then you feed that through the C Compiler. That produces all VM bit code and now you have the stream programme at the top and LLVM code on the bottom. And once you have this guide the next up is you extract the semantics of each programme. So we wrote a library at gala called what for? Which is good for intermediate language that is good for proving programme equal lens and also allows you to communicate with an SMT silver which I will get into a bit. And that is relatively straightforward since copilot and what for our relatively both written and a kind of functional style that translates relatively straight forward. Translating the semantics of LLVM bit coat and the other hand is more involved process that's an imperative language. So we go about this with a symbolic execution engine called crucible, to simulate the LVM code and the process that generates what for semantics for? And then once you have the semantics, the two type of programmes then you feed them into the prover part of the copilot verifier at which checks the actual equivalence and this prove process generates verification conditions and feed them into an external S and T solver. The ideas that it calls the successfully verified and equivalence of the two programmes or excludes a lot going on the side and in a vacuum it looks like it would take a long time to fit all the pieces of the slide together. But surprisingly that was not the case. And the reason is that because most of the things on the side insisted before we started the project. In particular every thing that is not great out here is some that we already had when we started. And we had the C Compiler and SMT solvers that have both been around for a long time. And we had developed these at gala but we develop them in the context of other project. We used crucible and what for on many other large-scale verification efforts and verify imperative programming linkages like C, LVM, and so on. Of which I don't have time to talk and go in depth about this and how crucible and what for work. But you want more details, we wrote an ICF paper in 2019 where we talk about them and much more detail so I'd refer you to that paper if you want to learn more. The only two things on the slide that we really had to do, substantial amount of effort on here to complete the project are these two things. So we had to extend the copilot theorem tool to be able to handle the tools and we needed to for the sake of the prover part of the copilot verifier. And we needed to write the prover itself. So I think the provers is probably the most interesting part. And so I mentioned that copilot verifier does programme equivalence checking via simulation. So it bit more specific there got we discharge things to the S and P the solvers and first-order theories are by some elation is not a property that easily fits into the theories that ass and T solvers are good at resigning about. So what we do instead is a little trick where we instead prove a little slightly stronger property. Which is extensional quality. And then extensional equality implies a simulation as the worldly. You take a copilot stream programme and its corresponding generated C code. At E given step you should be able to see the same set of trigger functions are called the same, both programmes and a set of argument. The other thing have to prove is that the copilot has partial operations. For instance it is of course division. So you could potentially divide by 0. If the stream programme divide by 0 then you should also be able to show that if the see programme divide by 0 and vice versa. Separately from this you can also prove that it never divide by 0 ever. For the sake of showing extensional equality you'll enter so that they are crash equivalent so to speak.
                 So you take the semantics of the each programme and what for and you turn them into a label transition system where the nose of the system are the states that are given at any moment time. And then you sort of match up the states and each label transit system and generate verification conditions for various things that are required for the extensional equality prove. For instance, returning to the earlier example, we need to check that the even trigger fires in both programmes and the same arguments that will turn into a really huge chunk of auto generated SMT queries. Where the contents of the SMT queries are particularly important but that is what is happening under the hood.
                 And then you give all the verification conditions to resolve or like Z3. If you can prove all the goals then you know that the programmes are equivalent. This is a really interesting piece of applied formal methods. As with the case with applied formal methods. You typically have to do some clever engineering to make it work right. One thing that is interesting about this project is how we had to handle floating-. Support. There is operations in the copilot such as the sign, cosign, tangent, exponentiation and so on. Which a lot of SMT solvers don't have an ability to reason about particular the deep level. So we had to sort of limit the copilot verifier could do about the programmes involving floating-. Support in order to make it work if you want more details about that I would refer you to the paper we talk about this in more depth.
                 Another thing we had to figure out is you know, it's all well and good if copilot verifier can prove something. You know as functional programming enthusiasts usually that is good enough for us. But is typically not good enough for an auditing process that you would use for NASA. They have a pretty high bar for prove. So we have to sort of explain the reasoning that the pilot verifier does convincing auditors that actually checked all the cases that need to. So this is actually human driven process. So we take the evidence that copilot verifier generates and we have to sort of explain it in a legible way. So in the paper we describe how we do this. Where we sort of take the high-level proof goals that the tool is trying to prove and link them up with the crucible, what for and SMT goals. And this is an ongoing process. So it will be interesting to see what this looks like when it is finished. So that is ongoing. But where we are looking at in the immediate future is that we want to use copilot your fire as part of the missions that require a higher degree of safety that one is currently right to do. So NASA has its classification system for things that various trust through the this requirement. So one level, others class letter D. That level copilot itself is currently released that and then what we want to do is have copilot monitors be combined with copilot verifier design class C missions. Which are the level above the class D. And this is an ongoing process and will be a while before we have any tangible results there. But in the meantime, the source code for both copilot verifier and copilot are publicly mobile on GitHub so if you want to set this up yourself I encourage you to check them out. That is every thing I want to say. So if there's any questions I will be happy to answer.
>> 
                >> AUDIENCE:   John Hughes from selvage. So when it verification fails, what does it do for a counterexample or does it say there's no proof?
                >>  RYAN SCOTT: A goal that is too sophisticated for SMT solver to prove, it might be that you had some defiant behaviour and see you it wasn't able to reason about. It will try to produce some explanation. We haven't spent a lot of engineering effort to make those kind of error messages more readable. Because most of the time was,  we want successful revocation first to go through auditing instead of failed ones. This is a problem we have thought about and it is tricky. I would be interested in hearing ideas about how to present failed verification efforts anymore legible way as well and that is important for the user extremes of this tool.
                >> AUDIENCE:   -- Dominquez from-- I wanted to ask if the designs that you consider before using SMT server?
                >>   RYAN SCOTT: I didn't hear the first part are you asking if we consider any alternatives?
                >> AUDIENCE:   Yes [ mic drop out ] because of the technical reasons?
                >>  RYAN SCOTT: I'm not sure that I fully heard everything here. But I will try to answer what I think might be your question which is the reason that we chose SMT solvers is because we want to this process to be as pushbutton as possible. The idea is you should be able to have a copilot programme and have a do most of the work and that is permanently because we want to put this in the hands of the people who are not formal method experts. Having like a high degree of interactivity required to complete the prove I think would be a nonstarter.
                >> AUDIENCE:   That's good. Thank you.
                >> AUDIENCE:   I just wanted to verify you said during the talk that you had found particular programmes where verification had failed. I suppose that was cases where the compiler would miss compile those pitching the programmes. Not where those programmes were not behaving according to some other specification. About that behaviour, intended behaviour?
                >>  RYAN SCOTT: In the case of the bugs copilot verifier found and like the copilot test way and so forth, and it generates and SMT query involving the operation that it doesn't know how to reason about bringing in which case it gives up. I would not classify those as a compiler bugs as much as the limitations in the tool itself.
                >> AUDIENCE:   Is there any you plan to some point provide external specific Asians regarding intended behaviour of the copilot programme and verify against those behaviors as well?
                >>  RYAN SCOTT: That would be an interesting line of future direction. Yes I mentioned that there are some operations that support for reasoning about them is limited. And just to give you an exam will here, we treat all floating-. Operations as on interpreted at the moment. So it can't do much reasoning but floating-. Division like in the side. You have to be very careful about how you assemble things. So you could add some more intelligent reasoning about floating-. Division separately to sort of allow this kind of programme to be verified successfully but we have not done that yet.
                >> AUDIENCE:   Presumably there is safety classes B + B and A.
                >>   RYAN SCOTT: I don't know the answer but my co-authored us.
>>    Very briefly cleaned letter C is what you need to fly in a drone, letter B is what you need to go to space. A is critical in space. And are publicly documented by the way. Not aiming for those. But you need to get classified first and then you get to B.
                >> AUDIENCE:   Be [Speaker away from microphone]
                >>   RYAN SCOTT: I'm sorry I heard the word matching that was it.
                >> AUDIENCE:   The control flow grass, do they necessarily match and what for? Or do you have to use some graphic transformations? I'm not sure if the LLVM has a stack or?
                >>   RYAN SCOTT: Once you get to the level of what for, the matchup pretty naturally. The crucible will sort of turn your imperative programme into a functional one. At that point it becomes more to more easy to do programme analysis on it.
                >> AUDIENCE:   And crucible that is not just reverse compile your programme that. There is still some work left for the serial improvement. It's not for question. So I wonder how much crucible just knows about the semantics and we just both compilers.
                >>  RYAN SCOTT: Yes crucible is definitely where of the semantics of LLVM. If that is your question. We have tested crucible out on LLVM based projects and sort of we actually run this on an extensive suite of the C test cases and that we were curated and so we're reasonably confident it could handle most of these things that you find in LLVM. But you see these programmes.
                >> AUDIENCE:   So in order to find the bug in a copilot compiler. You still need a copilot programme that provoked that bug. So you told us that you found ten such bugs. If it were me, I would now be writing a random generator of copilot programmes. It is interesting to do random testing with your verifiers, the test Oracle right have you consider that remove you already have a random generator for the previous compiler testing of the copilot compiler there.
                >>  RYAN SCOTT: Yes I completely agree it would be more useful to have property generated test for this. At the moment the copilot test consist of handwritten programmes. But I think we should and more, precisely because this is how you find bugs.
                >>  ARNAUD SPIWACK: We have a little time, is a question from Jack.
                >> AUDIENCE:   So in general bisimulation I think is undecidable for return compiler linkages. Is three way to get around it? Is copilot like not fully turn complete and you can do all the semantics there or is it there are certain programmes you can prove?
                >>  RYAN SCOTT: There are certain programmes that we cannot prove equivalent. No we don't claim to skirt around undecidable Italy. Like this is one example of something that you can improve. Maybe not quite the reasons that you are thinking. And also keep in mind that we are not sort of directly proving bisimulation we are proving a separate thing that implies the bisimulation. The story is a little bit different but yes certainly there is limitations.
                >>  ARNAUD SPIWACK: Thank you everybody. Thank you to the speaker again. All right. 30 minutes in the fireside chat.
